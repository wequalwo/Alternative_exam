{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b43db559",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\wequalwo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\wequalwo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "\n",
    "import re #регулярные выражения\n",
    "import math\n",
    "from collections import Counter\n",
    "import requests\n",
    "import time\n",
    "\n",
    "alphabet = 'абвгдеёжзийклмнопрстуфхцчшщъыьэюя'\n",
    "\n",
    "nltk.download('stopwords') # to use stopwords\n",
    "nltk.download('punkt') # to use word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "73fee5d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# служебные функция для расчета памяти, заниаемой dataframe и csr\n",
    "BYTES_TO_MB_DIV = 0.000001\n",
    "def print_memory_usage_of_data_frame(df):\n",
    "    mem = round(df.memory_usage().sum() * BYTES_TO_MB_DIV, 3) \n",
    "    print(\"Memory usage is \" + str(mem) + \" MB\")\n",
    "    \n",
    "def get_csr_memory_usage(matrix):\n",
    "    mem = (matrix.data.nbytes + matrix.indptr.nbytes + matrix.indices.nbytes) * BYTES_TO_MB_DIV\n",
    "    print(\"Memory usage is \" + str(mem) + \" MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "15c9ebc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Это', 'простое', 'предложение', 'показывает', 'фильтрацию', 'на', 'стоп-слова']\n",
      "['Это', 'простое', 'предложение', 'показывает', 'фильтрацию', 'стоп-слова']\n",
      "Стоп-слова русского языка:\n",
      "['и', 'в', 'во', 'не', 'что', 'он', 'на', 'я', 'с', 'со', 'как', 'а', 'то', 'все', 'она', 'так', 'его', 'но', 'да', 'ты', 'к', 'у', 'же', 'вы', 'за', 'бы', 'по', 'только', 'ее', 'мне', 'было', 'вот', 'от', 'меня', 'еще', 'нет', 'о', 'из', 'ему', 'теперь', 'когда', 'даже', 'ну', 'вдруг', 'ли', 'если', 'уже', 'или', 'ни', 'быть', 'был', 'него', 'до', 'вас', 'нибудь', 'опять', 'уж', 'вам', 'ведь', 'там', 'потом', 'себя', 'ничего', 'ей', 'может', 'они', 'тут', 'где', 'есть', 'надо', 'ней', 'для', 'мы', 'тебя', 'их', 'чем', 'была', 'сам', 'чтоб', 'без', 'будто', 'чего', 'раз', 'тоже', 'себе', 'под', 'будет', 'ж', 'тогда', 'кто', 'этот', 'того', 'потому', 'этого', 'какой', 'совсем', 'ним', 'здесь', 'этом', 'один', 'почти', 'мой', 'тем', 'чтобы', 'нее', 'сейчас', 'были', 'куда', 'зачем', 'всех', 'никогда', 'можно', 'при', 'наконец', 'два', 'об', 'другой', 'хоть', 'после', 'над', 'больше', 'тот', 'через', 'эти', 'нас', 'про', 'всего', 'них', 'какая', 'много', 'разве', 'три', 'эту', 'моя', 'впрочем', 'хорошо', 'свою', 'этой', 'перед', 'иногда', 'лучше', 'чуть', 'том', 'нельзя', 'такой', 'им', 'более', 'всегда', 'конечно', 'всю', 'между']\n"
     ]
    }
   ],
   "source": [
    "# десмонтсранция идеи токенизации и стоп-слов\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords as stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "example_sent = \"\"\"Это простое предложение показывает фильтрацию на стоп-слова\"\"\"\n",
    " \n",
    "stop_words = set(stopwords.words('russian'))\n",
    "word_tokens = word_tokenize(example_sent)\n",
    "filtered_sentence = [w for w in word_tokens if not w.lower() in stop_words]\n",
    "filtered_sentence = []\n",
    " \n",
    "for w in word_tokens:\n",
    "    if w not in stop_words:\n",
    "        filtered_sentence.append(w)\n",
    "\n",
    "print(word_tokens)\n",
    "print(filtered_sentence)\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "print(\"Стоп-слова русского языка:\")\n",
    "print(stopwords.words('russian'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbeb98bc",
   "metadata": {},
   "source": [
    "## Создадим функции, позволяющие генерировать следующие ошибки в словах:\n",
    "\n",
    "1.   **Пропуск буквы:** ${резонанс - рзонанс}$\n",
    "2.   **Дублирование буквы:** ${резонанс - реезонанс}$\n",
    "3.   **Перестановка букв:** ${резонанс - рзеонанс}$\n",
    "4.   **Опечатка:** ${резонанс - ркзонанс}$\n",
    "5.   **Некоторые орфографические ошибки:**  ${резонанс - ризонанс}$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "92be3754",
   "metadata": {},
   "outputs": [],
   "source": [
    "# все ошибки на расстоянии 2 (расстояние Левинштейна)\n",
    "def distance2(word):\n",
    "    return {e2 for e1 in distance1(word) if e1 for e2 in distance1(e1)}\n",
    "\n",
    "# все ошибки на расстоянии 1 (расстояние Левинштейна)\n",
    "def distance1(word):\n",
    "    pairs      = splits(word)\n",
    "    #transposes = [a+b[1]+b[0]+b[2:]  for (a,b) in pairs if len(b)>1]                           # перестановки\n",
    "    replaces   = [a+c+b[1:]          for (a,b) in pairs if b for c in replaces_set[b[0]] if b] # замены: \n",
    "                                                                                                        # опечатки, \n",
    "                                                                                                        # пропуски и дублирования букв, \n",
    "                                                                                                        # орфографические ошибки\n",
    "\n",
    "    last_replaces = [word[0:-1] + c for c in replaces_set[word[-1]]]                           # замены в конце слова\n",
    "    #return set(transposes + replaces + last_replaces)\n",
    "    return set(replaces + last_replaces)\n",
    "\n",
    "def splits(word):\n",
    "    return [(word[:i], word[i:])\n",
    "            for i in range (len(word)+1)]\n",
    "\n",
    "# список возможных замен в слове\n",
    "replaces_set = pd.Series([          \n",
    "               ['с', 'в', 'у', 'к', 'е', 'п', 'м', 'аа', ''],\n",
    "               ['ь', 'о', 'л', 'д', 'ю', 'бб', ''],\n",
    "               ['ч', 'ы', 'ц', 'у', 'к', 'а', 'с', 'вв', ''],\n",
    "               ['р', 'н', 'ш', 'л', 'о', 'гг', ''],\n",
    "               ['б', 'л', 'з', 'ж', 'ю', 'дд', ''],\n",
    "               ['а', 'к', 'н', 'р', 'п', 'ее', 'ё', 'и', ''],\n",
    "               ['ёё', 'й', 'е', ''],\n",
    "               ['ю', 'д', 'щ', 'з', 'х', 'э', 'жж', ''],\n",
    "               ['д', 'щ', 'х', 'э', 'ж', 'зз', 'с', ''],\n",
    "               ['м', 'а', 'п', 'р', 'т', 'ии', ''],\n",
    "               ['ц', 'ы', 'ф', 'йй',''],\n",
    "               ['а', 'в', 'у', 'е', 'п', 'а', 'кк', ''],\n",
    "               ['ь', 'о', 'г', 'ш', 'щ', 'д', 'б', 'лл', ''],\n",
    "               ['с', 'а', 'п', 'и', 'мм', ''],\n",
    "               ['р', 'п', 'е', 'г', 'о', 'нн', ''],\n",
    "               ['т', 'р', 'н', 'г', 'ш', 'л', 'ь', 'оо', ''],\n",
    "               ['м', 'а', 'к', 'е', 'н', 'р', 'и', 'пп', ''],\n",
    "               ['и', 'п', 'е', 'н', 'г', 'о', 'т', 'рр', 'р', ''],\n",
    "               ['ч', 'в', 'а', 'м', 'сс', ''],\n",
    "               ['и', 'п', 'р', 'о', 'ь', 'тт', ''],\n",
    "               ['в', 'ы', 'ц', 'к', 'а', 'уу', ''],\n",
    "               ['я', 'ч', 'ы', 'ц', 'й', 'фф', ''],\n",
    "               ['ж', 'з', 'ъ', 'э', 'хх', ''],\n",
    "               ['ы', 'ф', 'й', 'у', 'в', 'ы', 'цц', ''],\n",
    "               ['я', 'ф', 'ы', 'в', 'с', 'чч', ''],\n",
    "               ['л', 'о', 'г', 'щ', 'д', 'шш', 'шь', ''],\n",
    "               ['л', 'ш', 'з', 'ж', 'д', 'щщ', 'шь', 'ж', ''],\n",
    "               ['э', 'х', 'ъъ', 'ь', ''],\n",
    "               ['ч', 'я', 'ф', 'й', 'ц', 'у', 'в', 'ыы', ''],\n",
    "               ['т', 'о', 'л', 'б', 'ьь', ''],\n",
    "               ['ж', 'з', 'х', 'ъ', ''],\n",
    "               ['б', 'д', 'ж', 'юю', ''],\n",
    "               ['ф', 'ы', 'ч', 'яя', ''],\n",
    "               ], \n",
    "               index = ['а','б','в','г', 'д', 'е', 'ё', \n",
    "                            'ж', 'з', 'и', 'й', 'к', \n",
    "                            'л', 'м', 'н', 'о', 'п', \n",
    "                            'р', 'с', 'т', 'у', 'ф', \n",
    "                            'х', 'ц', 'ч', 'ш', 'щ', \n",
    "                            'ъ', 'ы', 'ь', 'э', 'ю', 'я'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39843dcf",
   "metadata": {},
   "source": [
    "**Импортируем текст**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2a202440",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['что', 'тот', 'быть', 'весь', 'это', 'как', 'она', 'они', 'так', 'сказать', 'этот', 'который', 'может', 'человек', 'один', 'еще', 'такой', 'только', 'себя', 'свое']\n",
      "8714\n"
     ]
    }
   ],
   "source": [
    "import codecs\n",
    "with codecs.open('top10000.txt', 'r', encoding = 'utf-8') as file:\n",
    "# with codecs.open('russian.txt', 'r', encoding = 'windows 1251') as file:\n",
    "    TEXT = file.read().replace('\\n', ' ') # для текста, в котором слова разделены '\\n'\n",
    "    # TEXT = file.read().replace(' ', ' ') # для текста, в котором слова разделены ' '\n",
    "def tokens(text):\n",
    "    return re.findall(r'[а-ё]+', text.lower())\n",
    "list_of_words = tokens(TEXT)\n",
    "\n",
    "# удалим все слова, короче 3 символов (потому что слова из 2х символов неинформативны):\n",
    "tmp = [w for w in list_of_words if len(w) > 2]\n",
    "list_of_words = tmp\n",
    "\n",
    "# пока ограничимся 200 словами\n",
    "# del list_of_words[60000:]\n",
    "\n",
    "# выведем первые 20 слов\n",
    "print(list_of_words[:20])\n",
    "print(len(list_of_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bd35fe6",
   "metadata": {},
   "source": [
    "**Для каждого слова рассчитаем возможыне ошибки на расстоянии 1 или 2. Затем произведем токенизацию**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "94ac577d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Set generation: 24.02 secs\n",
      "    Total size: 11121024\n"
     ]
    }
   ],
   "source": [
    "# words = []\n",
    "corpus = [] # все сгенерированные слова\n",
    "labels = [] # метки для классификации в порядке ошибочных слов\n",
    "\n",
    "\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "for item in list_of_words:\n",
    "    errors = distance1(item)#.union(distance2(item))# ??? как оптимизировать - неясно\n",
    "    errors = errors.union(item)\n",
    "    # words.append([item, errors])\n",
    "    #tmp = [w for w in errors if len(w) > 2]\n",
    "    #errors = tmp\n",
    "    corpus += list(errors)\n",
    "    labels += [item]*len(errors)\n",
    "\n",
    "end = time.time()\n",
    "duration = round(end - start, 2)\n",
    "print(\"Set generation: \" + str(duration) + \" secs\")\n",
    "print(\"    Total size:\", len(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6f47deca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['р0', 'е1', 'з2', 'о3', 'н4', 'а5', 'н6', 'с7']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# токенизация слова на буквы\n",
    "'''\n",
    "def _tokenize(word):\n",
    "    return [a + a for a in word]\n",
    "'''\n",
    "'''\n",
    "def _tokenize(word):\n",
    "    return re.findall(r'[а-ё]', word.lower())\n",
    "\n",
    "'''\n",
    "def _tokenize(word):\n",
    "    out = []\n",
    "    for i in range(len(word)):\n",
    "        out.append(word[i] + str(i))\n",
    "    return out\n",
    "\n",
    "\n",
    "\n",
    "_tokenize('резонанс')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f8312db",
   "metadata": {},
   "source": [
    "**Векторизация текста посредством *TfidfVectorizer (TF-IDF)***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1d5ab93f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectorization: 4.06 secs\n",
      "Найденные токены:\n",
      "['а0' 'а1' 'а10' 'а11' 'а2' 'а3' 'а4' 'а5' 'а6' 'а7' 'а8' 'а9' 'б0' 'б1'\n",
      " 'б10' 'б11' 'б12' 'б2' 'б3' 'б4' 'б5' 'б6' 'б7' 'б8' 'б9' 'в0' 'в1' 'в10'\n",
      " 'в11' 'в12' 'в13' 'в2' 'в3' 'в4' 'в5' 'в6' 'в7' 'в8' 'в9' 'г0' 'г1' 'г10'\n",
      " 'г11' 'г12' 'г2' 'г3' 'г4' 'г5' 'г6' 'г7' 'г8' 'г9' 'д0' 'д1' 'д2' 'д3'\n",
      " 'д4' 'д5' 'д6' 'д7' 'д8' 'д9' 'е0' 'е1' 'е10' 'е11' 'е12' 'е2' 'е3' 'е4'\n",
      " 'е5' 'е6' 'е7' 'е8' 'е9' 'ж0' 'ж1' 'ж2' 'ж3' 'ж4' 'ж5' 'ж6' 'ж7' 'ж8'\n",
      " 'з0' 'з1' 'з2' 'з3' 'з4' 'з5' 'з6' 'з7' 'и0' 'и1' 'и10' 'и11' 'и2' 'и3'\n",
      " 'и4' 'и5' 'и6' 'и7' 'и8' 'и9' 'й0' 'й1' 'й10' 'й11' 'й12' 'й13' 'й14'\n",
      " 'й15' 'й2' 'й3' 'й4' 'й5' 'й6' 'й7' 'й8' 'й9' 'к0' 'к1' 'к10' 'к11' 'к2'\n",
      " 'к3' 'к4' 'к5' 'к6' 'к7' 'к8' 'к9' 'л0' 'л1' 'л10' 'л11' 'л12' 'л2' 'л3'\n",
      " 'л4' 'л5' 'л6' 'л7' 'л8' 'л9' 'м0' 'м1' 'м10' 'м2' 'м3' 'м4' 'м5' 'м6'\n",
      " 'м7' 'м8' 'м9' 'н0' 'н1' 'н10' 'н11' 'н12' 'н13' 'н2' 'н3' 'н4' 'н5' 'н6'\n",
      " 'н7' 'н8' 'н9' 'о0' 'о1' 'о10' 'о11' 'о12' 'о13' 'о2' 'о3' 'о4' 'о5' 'о6'\n",
      " 'о7' 'о8' 'о9' 'п0' 'п1' 'п10' 'п11' 'п12' 'п2' 'п3' 'п4' 'п5' 'п6' 'п7'\n",
      " 'п8' 'п9' 'р0' 'р1' 'р10' 'р11' 'р12' 'р2' 'р3' 'р4' 'р5' 'р6' 'р7' 'р8'\n",
      " 'р9' 'с0' 'с1' 'с10' 'с11' 'с2' 'с3' 'с4' 'с5' 'с6' 'с7' 'с8' 'с9' 'т0'\n",
      " 'т1' 'т10' 'т11' 'т12' 'т2' 'т3' 'т4' 'т5' 'т6' 'т7' 'т8' 'т9' 'у0' 'у1'\n",
      " 'у10' 'у11' 'у13' 'у2' 'у3' 'у4' 'у5' 'у6' 'у7' 'у8' 'у9' 'ф0' 'ф1' 'ф10'\n",
      " 'ф11' 'ф13' 'ф14' 'ф2' 'ф3' 'ф4' 'ф5' 'ф6' 'ф7' 'ф8' 'ф9' 'х0' 'х1' 'х2'\n",
      " 'х3' 'х4' 'х5' 'х6' 'х7' 'ц0' 'ц1' 'ц10' 'ц11' 'ц13' 'ц14' 'ц2' 'ц3' 'ц4'\n",
      " 'ц5' 'ц6' 'ц7' 'ц8' 'ц9' 'ч0' 'ч1' 'ч10' 'ч11' 'ч13' 'ч2' 'ч3' 'ч4' 'ч5'\n",
      " 'ч6' 'ч7' 'ч8' 'ч9' 'ш0' 'ш1' 'ш10' 'ш11' 'ш12' 'ш2' 'ш3' 'ш4' 'ш5' 'ш6'\n",
      " 'ш7' 'ш8' 'ш9' 'щ0' 'щ1' 'щ2' 'щ3' 'щ4' 'щ5' 'щ6' 'щ7' 'щ8' 'щ9' 'ъ0'\n",
      " 'ъ1' 'ъ2' 'ъ3' 'ъ4' 'ъ5' 'ы0' 'ы1' 'ы10' 'ы11' 'ы12' 'ы13' 'ы14' 'ы2'\n",
      " 'ы3' 'ы4' 'ы5' 'ы6' 'ы7' 'ы8' 'ы9' 'ь0' 'ь1' 'ь10' 'ь11' 'ь12' 'ь13' 'ь2'\n",
      " 'ь3' 'ь4' 'ь5' 'ь6' 'ь7' 'ь8' 'ь9' 'э0' 'э1' 'э2' 'э3' 'э4' 'э5' 'э6'\n",
      " 'э7' 'ю0' 'ю1' 'ю2' 'ю3' 'ю4' 'ю5' 'ю6' 'ю7' 'я0' 'я1' 'я10' 'я11' 'я12'\n",
      " 'я13' 'я2' 'я3' 'я4' 'я5' 'я6' 'я7' 'я8' 'я9' 'ё0' 'ё1' 'ё10' 'ё11' 'ё2'\n",
      " 'ё3' 'ё4' 'ё5' 'ё6' 'ё7' 'ё8' 'ё9']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer(tokenizer = lambda x: _tokenize(x))\n",
    "\n",
    "start = time.time()\n",
    "V = vectorizer.fit_transform(corpus)\n",
    "end = time.time()\n",
    "duration = round(end - start, 2)\n",
    "print(\"Vectorization: \" + str(duration) + \" secs\")\n",
    "\n",
    "print(\"Найденные токены:\")\n",
    "print(vectorizer.get_feature_names_out())\n",
    "\n",
    "#3.3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48245b83",
   "metadata": {},
   "source": [
    "***bool* - векторизация:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "800e1d26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Найденные токены:\n",
      "['а0' 'а1' 'а10' 'а11' 'а2' 'а3' 'а4' 'а5' 'а6' 'а7' 'а8' 'а9' 'б0' 'б1'\n",
      " 'б10' 'б11' 'б12' 'б2' 'б3' 'б4' 'б5' 'б6' 'б7' 'б8' 'б9' 'в0' 'в1' 'в10'\n",
      " 'в11' 'в12' 'в13' 'в2' 'в3' 'в4' 'в5' 'в6' 'в7' 'в8' 'в9' 'г0' 'г1' 'г10'\n",
      " 'г11' 'г12' 'г2' 'г3' 'г4' 'г5' 'г6' 'г7' 'г8' 'г9' 'д0' 'д1' 'д2' 'д3'\n",
      " 'д4' 'д5' 'д6' 'д7' 'д8' 'д9' 'е0' 'е1' 'е10' 'е11' 'е12' 'е2' 'е3' 'е4'\n",
      " 'е5' 'е6' 'е7' 'е8' 'е9' 'ж0' 'ж1' 'ж2' 'ж3' 'ж4' 'ж5' 'ж6' 'ж7' 'ж8'\n",
      " 'з0' 'з1' 'з2' 'з3' 'з4' 'з5' 'з6' 'з7' 'и0' 'и1' 'и10' 'и11' 'и2' 'и3'\n",
      " 'и4' 'и5' 'и6' 'и7' 'и8' 'и9' 'й0' 'й1' 'й10' 'й11' 'й12' 'й13' 'й14'\n",
      " 'й15' 'й2' 'й3' 'й4' 'й5' 'й6' 'й7' 'й8' 'й9' 'к0' 'к1' 'к10' 'к11' 'к2'\n",
      " 'к3' 'к4' 'к5' 'к6' 'к7' 'к8' 'к9' 'л0' 'л1' 'л10' 'л11' 'л12' 'л2' 'л3'\n",
      " 'л4' 'л5' 'л6' 'л7' 'л8' 'л9' 'м0' 'м1' 'м10' 'м2' 'м3' 'м4' 'м5' 'м6'\n",
      " 'м7' 'м8' 'м9' 'н0' 'н1' 'н10' 'н11' 'н12' 'н13' 'н2' 'н3' 'н4' 'н5' 'н6'\n",
      " 'н7' 'н8' 'н9' 'о0' 'о1' 'о10' 'о11' 'о12' 'о13' 'о2' 'о3' 'о4' 'о5' 'о6'\n",
      " 'о7' 'о8' 'о9' 'п0' 'п1' 'п10' 'п11' 'п12' 'п2' 'п3' 'п4' 'п5' 'п6' 'п7'\n",
      " 'п8' 'п9' 'р0' 'р1' 'р10' 'р11' 'р12' 'р2' 'р3' 'р4' 'р5' 'р6' 'р7' 'р8'\n",
      " 'р9' 'с0' 'с1' 'с10' 'с11' 'с2' 'с3' 'с4' 'с5' 'с6' 'с7' 'с8' 'с9' 'т0'\n",
      " 'т1' 'т10' 'т11' 'т12' 'т2' 'т3' 'т4' 'т5' 'т6' 'т7' 'т8' 'т9' 'у0' 'у1'\n",
      " 'у10' 'у11' 'у13' 'у2' 'у3' 'у4' 'у5' 'у6' 'у7' 'у8' 'у9' 'ф0' 'ф1' 'ф10'\n",
      " 'ф11' 'ф13' 'ф14' 'ф2' 'ф3' 'ф4' 'ф5' 'ф6' 'ф7' 'ф8' 'ф9' 'х0' 'х1' 'х2'\n",
      " 'х3' 'х4' 'х5' 'х6' 'х7' 'ц0' 'ц1' 'ц10' 'ц11' 'ц13' 'ц14' 'ц2' 'ц3' 'ц4'\n",
      " 'ц5' 'ц6' 'ц7' 'ц8' 'ц9' 'ч0' 'ч1' 'ч10' 'ч11' 'ч13' 'ч2' 'ч3' 'ч4' 'ч5'\n",
      " 'ч6' 'ч7' 'ч8' 'ч9' 'ш0' 'ш1' 'ш10' 'ш11' 'ш12' 'ш2' 'ш3' 'ш4' 'ш5' 'ш6'\n",
      " 'ш7' 'ш8' 'ш9' 'щ0' 'щ1' 'щ2' 'щ3' 'щ4' 'щ5' 'щ6' 'щ7' 'щ8' 'щ9' 'ъ0'\n",
      " 'ъ1' 'ъ2' 'ъ3' 'ъ4' 'ъ5' 'ы0' 'ы1' 'ы10' 'ы11' 'ы12' 'ы13' 'ы14' 'ы2'\n",
      " 'ы3' 'ы4' 'ы5' 'ы6' 'ы7' 'ы8' 'ы9' 'ь0' 'ь1' 'ь10' 'ь11' 'ь12' 'ь13' 'ь2'\n",
      " 'ь3' 'ь4' 'ь5' 'ь6' 'ь7' 'ь8' 'ь9' 'э0' 'э1' 'э2' 'э3' 'э4' 'э5' 'э6'\n",
      " 'э7' 'ю0' 'ю1' 'ю2' 'ю3' 'ю4' 'ю5' 'ю6' 'ю7' 'я0' 'я1' 'я10' 'я11' 'я12'\n",
      " 'я13' 'я2' 'я3' 'я4' 'я5' 'я6' 'я7' 'я8' 'я9' 'ё0' 'ё1' 'ё10' 'ё11' 'ё2'\n",
      " 'ё3' 'ё4' 'ё5' 'ё6' 'ё7' 'ё8' 'ё9']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer_bin = CountVectorizer(binary = True, tokenizer = lambda x: _tokenize(x))\n",
    "V_bin = vectorizer_bin.fit_transform(corpus)\n",
    "print(\"Найденные токены:\")\n",
    "print(vectorizer_bin.get_feature_names_out())\n",
    "#print(V_bin.shape())\n",
    "#print(V_bin.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c4f1a5b",
   "metadata": {},
   "source": [
    "***hash* - векторизация:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9a84d249",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\wequalwo\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:516: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape:\n",
      "(471144, 33)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "vectorizer_hash = HashingVectorizer(n_features = 33, tokenizer = lambda x: _tokenize(x))\n",
    "V_hash = vectorizer_hash.fit_transform(corpus)\n",
    "print(\"Shape:\")\n",
    "print(V_hash.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "27721f52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# для наглядности создадим dataframe с данными на основе векторизации TF-IDF\n",
    "df = pd.DataFrame(V.toarray(), columns = vectorizer.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "29badeb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data size: 471144 471144\n",
      "['то', 'ято', 'чтн', 'вто', 'чт', 'чтш', 'ч', 'чоо', 'чтт', 'о']\n"
     ]
    }
   ],
   "source": [
    "print(\"Data size:\", len(corpus), len(labels))\n",
    "#print(13573258/len(corpus))\n",
    "print(corpus[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "28b1ee9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>а0</th>\n",
       "      <th>а1</th>\n",
       "      <th>а10</th>\n",
       "      <th>а11</th>\n",
       "      <th>а2</th>\n",
       "      <th>а3</th>\n",
       "      <th>а4</th>\n",
       "      <th>а5</th>\n",
       "      <th>а6</th>\n",
       "      <th>а7</th>\n",
       "      <th>...</th>\n",
       "      <th>ё11</th>\n",
       "      <th>ё2</th>\n",
       "      <th>ё3</th>\n",
       "      <th>ё4</th>\n",
       "      <th>ё5</th>\n",
       "      <th>ё6</th>\n",
       "      <th>ё7</th>\n",
       "      <th>ё8</th>\n",
       "      <th>ё9</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>что</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>что</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>что</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>что</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>что</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>471139</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.333669</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>яблоко</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>471140</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>яблоко</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>471141</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>яблоко</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>471142</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>яблоко</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>471143</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>яблоко</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>471144 rows × 398 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         а0   а1  а10  а11   а2   а3        а4   а5   а6   а7  ...  ё11   ё2  \\\n",
       "0       0.0  0.0  0.0  0.0  0.0  0.0  0.000000  0.0  0.0  0.0  ...  0.0  0.0   \n",
       "1       0.0  0.0  0.0  0.0  0.0  0.0  0.000000  0.0  0.0  0.0  ...  0.0  0.0   \n",
       "2       0.0  0.0  0.0  0.0  0.0  0.0  0.000000  0.0  0.0  0.0  ...  0.0  0.0   \n",
       "3       0.0  0.0  0.0  0.0  0.0  0.0  0.000000  0.0  0.0  0.0  ...  0.0  0.0   \n",
       "4       0.0  0.0  0.0  0.0  0.0  0.0  0.000000  0.0  0.0  0.0  ...  0.0  0.0   \n",
       "...     ...  ...  ...  ...  ...  ...       ...  ...  ...  ...  ...  ...  ...   \n",
       "471139  0.0  0.0  0.0  0.0  0.0  0.0  0.333669  0.0  0.0  0.0  ...  0.0  0.0   \n",
       "471140  0.0  0.0  0.0  0.0  0.0  0.0  0.000000  0.0  0.0  0.0  ...  0.0  0.0   \n",
       "471141  0.0  0.0  0.0  0.0  0.0  0.0  0.000000  0.0  0.0  0.0  ...  0.0  0.0   \n",
       "471142  0.0  0.0  0.0  0.0  0.0  0.0  0.000000  0.0  0.0  0.0  ...  0.0  0.0   \n",
       "471143  0.0  0.0  0.0  0.0  0.0  0.0  0.000000  0.0  0.0  0.0  ...  0.0  0.0   \n",
       "\n",
       "         ё3   ё4   ё5   ё6   ё7   ё8   ё9  labels  \n",
       "0       0.0  0.0  0.0  0.0  0.0  0.0  0.0     что  \n",
       "1       0.0  0.0  0.0  0.0  0.0  0.0  0.0     что  \n",
       "2       0.0  0.0  0.0  0.0  0.0  0.0  0.0     что  \n",
       "3       0.0  0.0  0.0  0.0  0.0  0.0  0.0     что  \n",
       "4       0.0  0.0  0.0  0.0  0.0  0.0  0.0     что  \n",
       "...     ...  ...  ...  ...  ...  ...  ...     ...  \n",
       "471139  0.0  0.0  0.0  0.0  0.0  0.0  0.0  яблоко  \n",
       "471140  0.0  0.0  0.0  0.0  0.0  0.0  0.0  яблоко  \n",
       "471141  0.0  0.0  0.0  0.0  0.0  0.0  0.0  яблоко  \n",
       "471142  0.0  0.0  0.0  0.0  0.0  0.0  0.0  яблоко  \n",
       "471143  0.0  0.0  0.0  0.0  0.0  0.0  0.0  яблоко  \n",
       "\n",
       "[471144 rows x 398 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['labels'] = labels\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "4430111a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage is 1500.123 MB\n",
      "Memory usage is 37.705923999999996 MB\n"
     ]
    }
   ],
   "source": [
    "print_memory_usage_of_data_frame(df)\n",
    "get_csr_memory_usage(V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6844b43e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics  import f1_score, accuracy_score, precision_score, recall_score, jaccard_score\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.datasets import make_classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da8ee923",
   "metadata": {},
   "source": [
    "## Перейдем к обучению модели"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52f97459",
   "metadata": {},
   "source": [
    "**Создадим pipeline для более простого тестирования в дальнейшем**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a0803520",
   "metadata": {},
   "outputs": [],
   "source": [
    "# варианты моделей:\n",
    "# DecisionTreeClassifier()\n",
    "\n",
    "# MultinomialNB()\n",
    "# KNeighborsClassifier()\n",
    "# LogisticRegression()\n",
    "\n",
    "# требует доп настройки\n",
    "# from sklearn.ensemble import RandomForestClassifier\n",
    "# from sklearn.datasets import make_classification\n",
    "\n",
    "## pipline, основанный на TfidfVectorizer и DecisionTreeClassifier()\n",
    "from sklearn.pipeline import Pipeline\n",
    "model_pipeline = Pipeline([\n",
    "    (\"vecorizer\", TfidfVectorizer(tokenizer = lambda x: _tokenize(x))),\n",
    "    (\"model\",     MultinomialNB())\n",
    "]\n",
    ")\n",
    "X = corpus\n",
    "y = labels\n",
    "\n",
    "#LogisticRegression(solver = 'sag', random_state = 42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2832e96",
   "metadata": {},
   "source": [
    "**обучим модель**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "34cc7091",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train-test split: 0.36 secs\n",
      "Training: 19.26 secs\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 42)\n",
    "end = time.time()\n",
    "duration = round(end - start, 2)\n",
    "print(\"Train-test split: \" + str(duration) + \" secs\")\n",
    "start = time.time()\n",
    "model_pipeline.fit(X_train, y_train)\n",
    "end = time.time()\n",
    "duration = round(end - start, 2)\n",
    "print(\"Training: \" + str(duration) + \" secs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "941968a8",
   "metadata": {},
   "source": [
    "**Финт ушами: тестируем модель на данных, в которых допустимо расстояние 2**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b50e795b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(corpus, labels, test_size = 0.1, random_state = 42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17a9d8dd",
   "metadata": {},
   "source": [
    "**Теперь протестируем обученную модель**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "75fb980c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing: 162.83 secs\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "pred = model_pipeline.predict(X_test)\n",
    "end = time.time()\n",
    "duration = round(end - start, 2)\n",
    "print(\"Testing: \" + str(duration) + \" secs\")\n",
    "# print('    mean:', mean_absolute_error(y_test, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4f91a1c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " accuracy: 0.8210813207050066\n",
      "precision: 0.7819743002229926\n",
      "   recall: 0.7245866024181618\n",
      "  jaccard: 0.6122483351702683\n",
      "       f1: 0.7329004465535275\n"
     ]
    }
   ],
   "source": [
    "print(' accuracy:', accuracy_score(y_test, pred))\n",
    "print('precision:', precision_score(y_test, pred, average = 'macro'))\n",
    "print('   recall:', recall_score(y_test, pred, average = 'macro'))\n",
    "print('  jaccard:', jaccard_score(y_test, pred, average = 'macro'))\n",
    "print('       f1:', f1_score(y_test, pred, average = 'macro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3b955a7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e13bc7cf",
   "metadata": {},
   "source": [
    "## Проведем \"живой\" тест на модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "0ded9cb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "pins = input()\n",
    "pins = re.findall(r'[а-ё]+', pins.lower())\n",
    "\n",
    "\n",
    "for pin in pins:\n",
    "    if len(pin) > 2:\n",
    "        print(model_pipeline.predict([pin])[0], end = ' ')\n",
    "    else:\n",
    "        print(pin, end = ' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dc2d5649",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'labels' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\wequalwo\\source\\python\\s3.ipynb Cell 31'\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/wequalwo/source/python/s3.ipynb#ch0000030?line=0'>1</a>\u001b[0m \u001b[39m#ккуратно акомпoнировала озартно ортикулировала\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/wequalwo/source/python/s3.ipynb#ch0000030?line=1'>2</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mхочет\u001b[39m\u001b[39m'\u001b[39m \u001b[39min\u001b[39;00m labels)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'labels' is not defined"
     ]
    }
   ],
   "source": [
    "#ккуратно акомпoнировала озартно ортикулировала\n",
    "print('хочет' in labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da7d48ff",
   "metadata": {},
   "source": [
    "## #попытка обучить большие объемы данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a4e09e6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c1abb983",
   "metadata": {},
   "source": [
    "## bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa5a4ade",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "id": "24d8cb4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage is 182.116 MB\n"
     ]
    }
   ],
   "source": [
    "def convert_to_sparse_pandas(df, exclude_columns = []):\n",
    "    df = df.copy()\n",
    "    exclude_columns = set(exclude_columns)\n",
    "\n",
    "    for (columnName, columnData) in df.iteritems():\n",
    "        if columnName in exclude_columns:\n",
    "            continue\n",
    "        df[columnName] = pd.arrays.SparseArray(columnData.values, dtype = np.float16)\n",
    "\n",
    "    return df\n",
    "\n",
    "df_sparse = convert_to_sparse_pandas(df, exclude_columns = [\"label\"])\n",
    "# display(df_sparse.dtypes)\n",
    "print_memory_usage_of_data_frame(df_sparse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "bffcb3cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6571622078283824\n",
      "0.5426987035755111\n",
      "53.354150671547565\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "print(accuracy_score(y_test, pred))\n",
    "print(f1_score(y_test, pred, average = 'macro'))\n",
    "\n",
    "print(mean_absolute_error(y_test, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "id": "344750e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X:\n",
      "Memory usage is 233.391 MB\n",
      "X_sparse:\n",
      "Memory usage is 175.044 MB\n",
      "X_csr:\n",
      "Memory usage is 68.581552 MB\n",
      "X_binary:\n",
      "Memory usage is 68.581552 MB\n"
     ]
    }
   ],
   "source": [
    "y = df['label']\n",
    "X = df[df.columns.difference(['label'])]\n",
    "\n",
    "y_sparse = df_sparse['label']\n",
    "X_sparse = df_sparse[df_sparse.columns.difference(['label'])]\n",
    "\n",
    "y_csr = df['label']\n",
    "X_csr = V\n",
    "\n",
    "y_binary = df['label']\n",
    "X_binary = V_bin\n",
    "\n",
    "print('X:')\n",
    "print_memory_usage_of_data_frame(X)\n",
    "print('X_sparse:')\n",
    "print_memory_usage_of_data_frame(X_sparse)\n",
    "print('X_csr:')\n",
    "get_csr_memory_usage(X_csr)\n",
    "print('X_binary:')\n",
    "get_csr_memory_usage(X_binary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "id": "fed72e5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scipy sparse matrix\n",
      "Testing: 0.32 secs\n",
      "accuracy: 0.011158744881569125\n",
      "      f1: 0.014289106223557658\n",
      "    mean: 188.53618532678777\n",
      "\n",
      "\n",
      "Scipy sparse matrix binary\n",
      "Testing: 0.49 secs\n",
      "accuracy: 0.708187227111282\n",
      "      f1: 0.5981014691555474\n",
      "    mean: 45.30782978530869\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(models)):\n",
    "    print(list_of_names[i])\n",
    "    test(models[i], X_tests_arr[i], Y_tests_arr[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "id": "a331e81e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(mod, Xtest, Ytest):\n",
    "    start = time.time()\n",
    "    pred = mod.predict(Xtest)\n",
    "    end = time.time()\n",
    "    duration = round(end - start, 2)\n",
    "    print(\"Testing: \" + str(duration) + \" secs\")\n",
    "    print('accuracy:', accuracy_score(Ytest, pred))\n",
    "    print('      f1:', f1_score(Ytest, pred, average = 'macro'))\n",
    "    print('    mean:', mean_absolute_error(Ytest, pred))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "id": "e1e2ab54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scipy sparse matrix\n",
      "Train-test split: 0.26 secs\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [313]\u001b[0m, in \u001b[0;36m<cell line: 23>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTrain-test split: \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(duration) \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m secs\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     37\u001b[0m start \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m---> 38\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     40\u001b[0m end \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m     41\u001b[0m duration \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mround\u001b[39m(end \u001b[38;5;241m-\u001b[39m start, \u001b[38;5;241m2\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\tree\\_classes.py:937\u001b[0m, in \u001b[0;36mDecisionTreeClassifier.fit\u001b[1;34m(self, X, y, sample_weight, check_input, X_idx_sorted)\u001b[0m\n\u001b[0;32m    899\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfit\u001b[39m(\n\u001b[0;32m    900\u001b[0m     \u001b[38;5;28mself\u001b[39m, X, y, sample_weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, check_input\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, X_idx_sorted\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdeprecated\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    901\u001b[0m ):\n\u001b[0;32m    902\u001b[0m     \u001b[38;5;124;03m\"\"\"Build a decision tree classifier from the training set (X, y).\u001b[39;00m\n\u001b[0;32m    903\u001b[0m \n\u001b[0;32m    904\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    934\u001b[0m \u001b[38;5;124;03m        Fitted estimator.\u001b[39;00m\n\u001b[0;32m    935\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 937\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    938\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    939\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    940\u001b[0m \u001b[43m        \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    941\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcheck_input\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcheck_input\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    942\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX_idx_sorted\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX_idx_sorted\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    943\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    944\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\tree\\_classes.py:420\u001b[0m, in \u001b[0;36mBaseDecisionTree.fit\u001b[1;34m(self, X, y, sample_weight, check_input, X_idx_sorted)\u001b[0m\n\u001b[0;32m    409\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    410\u001b[0m     builder \u001b[38;5;241m=\u001b[39m BestFirstTreeBuilder(\n\u001b[0;32m    411\u001b[0m         splitter,\n\u001b[0;32m    412\u001b[0m         min_samples_split,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    417\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmin_impurity_decrease,\n\u001b[0;32m    418\u001b[0m     )\n\u001b[1;32m--> 420\u001b[0m \u001b[43mbuilder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuild\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtree_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    422\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_outputs_ \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m is_classifier(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    423\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_classes_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_classes_[\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "models = []\n",
    "list_of_names = []\n",
    "X_tests_arr = []\n",
    "Y_tests_arr = []\n",
    "\n",
    "model = DecisionTreeClassifier()\n",
    "# model = BernoulliNB()\n",
    "\n",
    "# model = GaussianNB()\n",
    "# model = MultinomialNB()\n",
    "# msodel = GaussianNB()\n",
    "# model = MultinomialNB()\n",
    "# model = KNeighborsClassifier()\n",
    "# model = LogisticRegression()\n",
    "\n",
    "\n",
    "#     'Pandas dataframe': [X, y],\n",
    "vector_dict = {\n",
    "     'Scipy sparse matrix': [X_csr, y_csr],\n",
    "     'Scipy sparse matrix binary': [X_binary, y_binary]\n",
    "    }\n",
    "\n",
    "for key, item in vector_dict.items():\n",
    "    print(key)\n",
    "    list_of_names.append(key)\n",
    "    start = time.time()\n",
    "    if (key != 'Pandas dataframe'):\n",
    "        XX = item[0].toarray()\n",
    "    else:\n",
    "        XX = X\n",
    "        \n",
    "    X_train, X_test, y_train, y_test = train_test_split(XX, y, test_size = 0.2, random_state = 42)\n",
    "    end = time.time()\n",
    "    duration = round(end - start, 2)\n",
    "    print(\"Train-test split: \" + str(duration) + \" secs\")\n",
    "\n",
    "    start = time.time()\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    end = time.time()\n",
    "    duration = round(end - start, 2)\n",
    "    \n",
    "    models.append(model)\n",
    "    X_tests_arr.append(X_test)\n",
    "    Y_tests_arr.append(y_test)\n",
    "    print(\"Training: \" + str(duration) + \" secs\")\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06e21790",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokets_list = vectorizer_bin.get_feature_names_out()\n",
    "\n",
    "def _tokenize2(pin):\n",
    "    return [a + a for a in pin]\n",
    "\n",
    "pins = input()\n",
    "pins = re.findall(r'[а-я]+', pins.lower())\n",
    "\n",
    "for pin in pins:\n",
    "    if(len(pin) < 3):\n",
    "        print(pin, end = ' ')\n",
    "        continue\n",
    "    pin_vec = [0]*33\n",
    "    pin = _tokenize2(pin)\n",
    "    for i in range(33):\n",
    "        if tokets_list[i] in pin:\n",
    "            pin_vec[i] = 1\n",
    "\n",
    "    predict = models[0].predict([pin_vec])\n",
    "    print(list_of_words[predict[0]], end = ' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "a6252d62",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "\n",
    "class OnlinePipeline(Pipeline):\n",
    "    def partial_fit(self, X, y = None):\n",
    "        for i, step in enumerate(self.steps):\n",
    "            name, est = step\n",
    "            est.partial_fit(X, y)\n",
    "            if i < len(self.steps) - 1:\n",
    "                X = est.transform(X)\n",
    "        return self\n",
    "\n",
    "model_pipeline = OnlinePipeline([\n",
    "    (\"vecorizer\", TfidfVectorizer(tokenizer = lambda x: _tokenize(x))),\n",
    "    (\"model\", DecisionTreeClassifier())\n",
    "]\n",
    ")\n",
    "\n",
    "X = corpus\n",
    "y = labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "b70ce696",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train-test split: 0.1 secs\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "OnlinePipeline(steps=[('vecorizer',\n",
       "                       TfidfVectorizer(tokenizer=<function <lambda> at 0x000001C00208BF40>)),\n",
       "                      ('model', DecisionTreeClassifier())])"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start = time.time()\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42)\n",
    "end = time.time()\n",
    "duration = round(end - start, 2)\n",
    "print(\"Train-test split: \" + str(duration) + \" secs\")\n",
    "\n",
    "a = int(len(X_train)/3)\n",
    "sum_t = 0\n",
    "\n",
    "\n",
    "div = 100\n",
    "part = int(len(X_train)/div)\n",
    "\n",
    "'''for i in range(1, div + 1):\n",
    "    start = time.time()\n",
    "    print((i*(part) - part), \":\", i*(part))\n",
    "    model_pipeline.fit(X_train[(i*(part) - part): i*(part)], y_train[(i*(part) - part): i*(part)])\n",
    "    end = time.time()\n",
    "    duration = round(end - start, 2)\n",
    "    sum_t += duration\n",
    "    if(i % 10 == 1):\n",
    "        print(\"Training round \" + str(i) + \": \" + str(sum_t) + \" secs\")'''\n",
    "    \n",
    "\n",
    "#print(\"        Training: \" + str(sum_t) + \" secs\")\n",
    "t = int(len(X_train)/2)\n",
    "model_pipeline.fit(X_train[:t], y_train[:t])\n",
    "model_pipeline.fit(X_train[t:], y_train[t:])"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "87852089012c4b81b9af9dc676d2b889d7d3b7bd761d748065844c07d5d6aa6d"
  },
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
