{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b1dd9f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pandas\n",
    "!pip install nltk\n",
    "!pip install sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b43db559",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\wequalwo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\wequalwo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "\n",
    "import re #регулярные выражения\n",
    "import math\n",
    "from collections import Counter\n",
    "import requests\n",
    "import time\n",
    "\n",
    "alphabet = 'абвгдеёжзийклмнопрстуфхцчшщъыьэюя'\n",
    "\n",
    "nltk.download('stopwords') # to use stopwords\n",
    "nltk.download('punkt') # to use word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "73fee5d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# служебные функция для расчета памяти, заниаемой dataframe и csr\n",
    "BYTES_TO_MB_DIV = 0.000001\n",
    "def print_memory_usage_of_data_frame(df):\n",
    "    mem = round(df.memory_usage().sum() * BYTES_TO_MB_DIV, 3) \n",
    "    print(\"Memory usage is \" + str(mem) + \" MB\")\n",
    "    \n",
    "def get_csr_memory_usage(matrix):\n",
    "    mem = (matrix.data.nbytes + matrix.indptr.nbytes + matrix.indices.nbytes) * BYTES_TO_MB_DIV\n",
    "    print(\"Memory usage is \" + str(mem) + \" MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "15c9ebc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Это', 'простое', 'предложение', 'показывает', 'фильтрацию', 'на', 'стоп-слова']\n",
      "['Это', 'простое', 'предложение', 'показывает', 'фильтрацию', 'стоп-слова']\n",
      "Стоп-слова русского языка:\n",
      "['и', 'в', 'во', 'не', 'что', 'он', 'на', 'я', 'с', 'со', 'как', 'а', 'то', 'все', 'она', 'так', 'его', 'но', 'да', 'ты', 'к', 'у', 'же', 'вы', 'за', 'бы', 'по', 'только', 'ее', 'мне', 'было', 'вот', 'от', 'меня', 'еще', 'нет', 'о', 'из', 'ему', 'теперь', 'когда', 'даже', 'ну', 'вдруг', 'ли', 'если', 'уже', 'или', 'ни', 'быть', 'был', 'него', 'до', 'вас', 'нибудь', 'опять', 'уж', 'вам', 'ведь', 'там', 'потом', 'себя', 'ничего', 'ей', 'может', 'они', 'тут', 'где', 'есть', 'надо', 'ней', 'для', 'мы', 'тебя', 'их', 'чем', 'была', 'сам', 'чтоб', 'без', 'будто', 'чего', 'раз', 'тоже', 'себе', 'под', 'будет', 'ж', 'тогда', 'кто', 'этот', 'того', 'потому', 'этого', 'какой', 'совсем', 'ним', 'здесь', 'этом', 'один', 'почти', 'мой', 'тем', 'чтобы', 'нее', 'сейчас', 'были', 'куда', 'зачем', 'всех', 'никогда', 'можно', 'при', 'наконец', 'два', 'об', 'другой', 'хоть', 'после', 'над', 'больше', 'тот', 'через', 'эти', 'нас', 'про', 'всего', 'них', 'какая', 'много', 'разве', 'три', 'эту', 'моя', 'впрочем', 'хорошо', 'свою', 'этой', 'перед', 'иногда', 'лучше', 'чуть', 'том', 'нельзя', 'такой', 'им', 'более', 'всегда', 'конечно', 'всю', 'между']\n"
     ]
    }
   ],
   "source": [
    "# десмонтсранция идеи токенизации и стоп-слов\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords as stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "example_sent = \"\"\"Это простое предложение показывает фильтрацию на стоп-слова\"\"\"\n",
    " \n",
    "stop_words = set(stopwords.words('russian'))\n",
    "word_tokens = word_tokenize(example_sent)\n",
    "filtered_sentence = [w for w in word_tokens if not w.lower() in stop_words]\n",
    "filtered_sentence = []\n",
    " \n",
    "for w in word_tokens:\n",
    "    if w not in stop_words:\n",
    "        filtered_sentence.append(w)\n",
    "\n",
    "print(word_tokens)\n",
    "print(filtered_sentence)\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "print(\"Стоп-слова русского языка:\")\n",
    "print(stopwords.words('russian'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbeb98bc",
   "metadata": {},
   "source": [
    "## Создадим функции, позволяющие генерировать следующие ошибки в словах:\n",
    "\n",
    "1.   **Пропуск буквы:** ${резонанс - рзонанс}$\n",
    "2.   **Дублирование буквы:** ${резонанс - реезонанс}$\n",
    "3.   **Перестановка букв:** ${резонанс - рзеонанс}$\n",
    "4.   **Опечатка:** ${резонанс - ркзонанс}$\n",
    "5.   **Некоторые орфографические ошибки:**  ${резонанс - ризонанс}$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "92be3754",
   "metadata": {},
   "outputs": [],
   "source": [
    "# все ошибки на расстоянии 2 (расстояние Левинштейна)\n",
    "def distance2(word):\n",
    "    return {e2 for e1 in distance1(word) if e1 for e2 in distance1(e1)}\n",
    "\n",
    "# все ошибки на расстоянии 1 (расстояние Левинштейна)\n",
    "def distance1(word):\n",
    "    pairs      = splits(word)\n",
    "    transposes = [a+b[1]+b[0]+b[2:]  for (a,b) in pairs if len(b)>1]                           # перестановки\n",
    "    replaces   = [a+c+b[1:]          for (a,b) in pairs if b for c in replaces_set[b[0]] if b] # замены: \n",
    "                                                                                                        # опечатки, \n",
    "                                                                                                        # пропуски и дублирования букв, \n",
    "                                                                                                        # орфографические ошибки\n",
    "\n",
    "    last_replaces = [word[0:-1] + c for c in replaces_set[word[-1]]]                           # замены в конце слова\n",
    "    return set(transposes + replaces + last_replaces)\n",
    "    #return set(replaces + last_replaces)\n",
    "\n",
    "def splits(word):\n",
    "    return [(word[:i], word[i:])\n",
    "            for i in range (len(word)+1)]\n",
    "\n",
    "# список возможных замен в слове\n",
    "replaces_set = pd.Series([          \n",
    "               ['с', 'в', 'у', 'к', 'е', 'п', 'м', 'аа', ''],\n",
    "               ['ь', 'о', 'л', 'д', 'ю', 'бб', ''],\n",
    "               ['ч', 'ы', 'ц', 'у', 'к', 'а', 'с', 'вв', ''],\n",
    "               ['р', 'н', 'ш', 'л', 'о', 'гг', ''],\n",
    "               ['б', 'л', 'з', 'ж', 'ю', 'дд', ''],\n",
    "               ['а', 'к', 'н', 'р', 'п', 'ее', 'ё', 'и', ''],\n",
    "               ['ёё', 'й', 'е', ''],\n",
    "               ['ю', 'д', 'щ', 'з', 'х', 'э', 'жж', ''],\n",
    "               ['д', 'щ', 'х', 'э', 'ж', 'зз', 'с', ''],\n",
    "               ['м', 'а', 'п', 'р', 'т', 'ии', ''],\n",
    "               ['ц', 'ы', 'ф', 'йй',''],\n",
    "               ['а', 'в', 'у', 'е', 'п', 'а', 'кк', ''],\n",
    "               ['ь', 'о', 'г', 'ш', 'щ', 'д', 'б', 'лл', ''],\n",
    "               ['с', 'а', 'п', 'и', 'мм', ''],\n",
    "               ['р', 'п', 'е', 'г', 'о', 'нн', ''],\n",
    "               ['т', 'р', 'н', 'г', 'ш', 'л', 'ь', 'оо', ''],\n",
    "               ['м', 'а', 'к', 'е', 'н', 'р', 'и', 'пп', ''],\n",
    "               ['и', 'п', 'е', 'н', 'г', 'о', 'т', 'рр', 'р', ''],\n",
    "               ['ч', 'в', 'а', 'м', 'сс', ''],\n",
    "               ['и', 'п', 'р', 'о', 'ь', 'тт', ''],\n",
    "               ['в', 'ы', 'ц', 'к', 'а', 'уу', ''],\n",
    "               ['я', 'ч', 'ы', 'ц', 'й', 'фф', ''],\n",
    "               ['ж', 'з', 'ъ', 'э', 'хх', ''],\n",
    "               ['ы', 'ф', 'й', 'у', 'в', 'ы', 'цц', ''],\n",
    "               ['я', 'ф', 'ы', 'в', 'с', 'чч', ''],\n",
    "               ['л', 'о', 'г', 'щ', 'д', 'шш', 'шь', ''],\n",
    "               ['л', 'ш', 'з', 'ж', 'д', 'щщ', 'шь', 'ж', ''],\n",
    "               ['э', 'х', 'ъъ', 'ь', ''],\n",
    "               ['ч', 'я', 'ф', 'й', 'ц', 'у', 'в', 'ыы', ''],\n",
    "               ['т', 'о', 'л', 'б', 'ьь', ''],\n",
    "               ['ж', 'з', 'х', 'ъ', ''],\n",
    "               ['б', 'д', 'ж', 'юю', ''],\n",
    "               ['ф', 'ы', 'ч', 'яя', ''],\n",
    "               ], \n",
    "               index = ['а','б','в','г', 'д', 'е', 'ё', \n",
    "                            'ж', 'з', 'и', 'й', 'к', \n",
    "                            'л', 'м', 'н', 'о', 'п', \n",
    "                            'р', 'с', 'т', 'у', 'ф', \n",
    "                            'х', 'ц', 'ч', 'ш', 'щ', \n",
    "                            'ъ', 'ы', 'ь', 'э', 'ю', 'я'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39843dcf",
   "metadata": {},
   "source": [
    "**Импортируем текст**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "2a202440",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['что', 'тот', 'быть', 'весь', 'это', 'как', 'она', 'они', 'так', 'сказать', 'этот', 'который', 'может', 'человек', 'один', 'еще', 'такой', 'только', 'себя', 'свое']\n",
      "8714\n"
     ]
    }
   ],
   "source": [
    "import codecs\n",
    "with codecs.open('top10000.txt', 'r', encoding = 'utf-8') as file:\n",
    "#with codecs.open('russian.txt', 'r', encoding = 'windows 1251') as file:\n",
    "    TEXT = file.read().replace('\\n', ' ') # для текста, в котором слова разделены '\\n'\n",
    "    # TEXT = file.read().replace(' ', ' ') # для текста, в котором слова разделены ' '\n",
    "def tokens(text):\n",
    "    return re.findall(r'[а-ё]+', text.lower())\n",
    "list_of_words = tokens(TEXT)\n",
    "\n",
    "# удалим все слова, короче 3 символов (потому что слова из 2х символов неинформативны):\n",
    "tmp = [w for w in list_of_words if len(w) > 2]\n",
    "list_of_words = tmp\n",
    "\n",
    "# пока ограничимся 200 словами\n",
    "# del list_of_words[1000:]\n",
    "\n",
    "# выведем первые 20 слов\n",
    "print(list_of_words[:20])\n",
    "print(len(list_of_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bd35fe6",
   "metadata": {},
   "source": [
    "**Для каждого слова рассчитаем возможыне ошибки на расстоянии 1 или 2. Затем произведем токенизацию**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "94ac577d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Set generation: 0.4 secs\n",
      "    Total size: 467283\n"
     ]
    }
   ],
   "source": [
    "# words = []\n",
    "corpus = [] # все сгенерированные слова\n",
    "labels = [] # метки для классификации в порядке ошибочных слов\n",
    "\n",
    "\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "for item in list_of_words:\n",
    "    errors = distance1(item)#.union(distance2(item))# ??? как оптимизировать - неясно\n",
    "    # words.append([item, errors])\n",
    "    #tmp = [w for w in errors if len(w) > 2]\n",
    "    #errors = tmp\n",
    "    corpus += list(errors)\n",
    "    labels += [item]*len(errors)\n",
    "\n",
    "end = time.time()\n",
    "duration = round(end - start, 2)\n",
    "print(\"Set generation: \" + str(duration) + \" secs\")\n",
    "print(\"    Total size:\", len(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "b30cb68f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9020140303236026"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "47832/53028"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "6f47deca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['р', 'е', 'з', 'о', 'н', 'а', 'н', 'с']"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# токенизация слова на буквы\n",
    "'''\n",
    "def _tokenize(word):\n",
    "    return [a + a for a in word]\n",
    "'''\n",
    "def _tokenize(word):\n",
    "    return re.findall(r'[а-ё]', word.lower())\n",
    "_tokenize('резонанс')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f8312db",
   "metadata": {},
   "source": [
    "**Векторизация текста посредством *TfidfVectorizer (TF-IDF)***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "1d5ab93f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectorization: 2.83 secs\n",
      "Найденные токены:\n",
      "['а' 'б' 'в' 'г' 'д' 'е' 'ж' 'з' 'и' 'й' 'к' 'л' 'м' 'н' 'о' 'п' 'р' 'с'\n",
      " 'т' 'у' 'ф' 'х' 'ц' 'ч' 'ш' 'щ' 'ъ' 'ы' 'ь' 'э' 'ю' 'я' 'ё']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer(tokenizer = lambda x: _tokenize(x))\n",
    "\n",
    "start = time.time()\n",
    "V = vectorizer.fit_transform(corpus)\n",
    "end = time.time()\n",
    "duration = round(end - start, 2)\n",
    "print(\"Vectorization: \" + str(duration) + \" secs\")\n",
    "\n",
    "print(\"Найденные токены:\")\n",
    "print(vectorizer.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48245b83",
   "metadata": {},
   "source": [
    "***bool* - векторизация:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "800e1d26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Найденные токены:\n",
      "['а' 'б' 'в' 'г' 'д' 'е' 'ж' 'з' 'и' 'й' 'к' 'л' 'м' 'н' 'о' 'п' 'р' 'с'\n",
      " 'т' 'у' 'ф' 'х' 'ц' 'ч' 'ш' 'щ' 'ъ' 'ы' 'ь' 'э' 'ю' 'я' 'ё']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer_bin = CountVectorizer(binary = True, tokenizer = lambda x: _tokenize(x))\n",
    "V_bin = vectorizer_bin.fit_transform(corpus)\n",
    "print(\"Найденные токены:\")\n",
    "print(vectorizer_bin.get_feature_names_out())\n",
    "#print(V_bin.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63fd4e3d",
   "metadata": {},
   "source": [
    "***hash* - векторизация:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "69919112",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wequalwo\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:516: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Найденные токены:\n",
      "(467283, 33)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "vectorizer_hash = HashingVectorizer(n_features = 33, tokenizer = lambda x: _tokenize(x))\n",
    "V_hash = vectorizer_hash.fit_transform(corpus)\n",
    "print(\"Найденные токены:\")\n",
    "print(V_hash.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "27721f52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# для наглядности создадим dataframe с данными на основе векторизации TF-IDF\n",
    "df = pd.DataFrame(V.toarray(), columns = vectorizer_bin.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "29badeb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data size: 467283 467283\n",
      "['чтр', 'чо', 'чот', 'чтш', 'чтто', 'чро', 'ято', 'чтл', 'чтн', 'чьо']\n"
     ]
    }
   ],
   "source": [
    "print(\"Data size:\", len(corpus), len(labels))\n",
    "#print(13573258/len(corpus))\n",
    "print(corpus[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "28b1ee9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>а</th>\n",
       "      <th>б</th>\n",
       "      <th>в</th>\n",
       "      <th>г</th>\n",
       "      <th>д</th>\n",
       "      <th>е</th>\n",
       "      <th>ж</th>\n",
       "      <th>з</th>\n",
       "      <th>и</th>\n",
       "      <th>й</th>\n",
       "      <th>...</th>\n",
       "      <th>ш</th>\n",
       "      <th>щ</th>\n",
       "      <th>ъ</th>\n",
       "      <th>ы</th>\n",
       "      <th>ь</th>\n",
       "      <th>э</th>\n",
       "      <th>ю</th>\n",
       "      <th>я</th>\n",
       "      <th>ё</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>что</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>что</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>что</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.723302</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>что</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>что</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>467278</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.442428</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.530257</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.425332</td>\n",
       "      <td>0.0</td>\n",
       "      <td>яблоко</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>467279</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.481303</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.386392</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.462705</td>\n",
       "      <td>0.0</td>\n",
       "      <td>яблоко</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>467280</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.495554</td>\n",
       "      <td>0.346067</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.476404</td>\n",
       "      <td>0.0</td>\n",
       "      <td>яблоко</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>467281</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.448169</td>\n",
       "      <td>0.0</td>\n",
       "      <td>яблоко</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>467282</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.538335</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.517533</td>\n",
       "      <td>0.0</td>\n",
       "      <td>яблоко</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>467283 rows × 34 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          а         б         в    г         д    е    ж    з    и    й  ...  \\\n",
       "0       0.0  0.000000  0.000000  0.0  0.000000  0.0  0.0  0.0  0.0  0.0  ...   \n",
       "1       0.0  0.000000  0.000000  0.0  0.000000  0.0  0.0  0.0  0.0  0.0  ...   \n",
       "2       0.0  0.000000  0.000000  0.0  0.000000  0.0  0.0  0.0  0.0  0.0  ...   \n",
       "3       0.0  0.000000  0.000000  0.0  0.000000  0.0  0.0  0.0  0.0  0.0  ...   \n",
       "4       0.0  0.000000  0.000000  0.0  0.000000  0.0  0.0  0.0  0.0  0.0  ...   \n",
       "...     ...       ...       ...  ...       ...  ...  ...  ...  ...  ...  ...   \n",
       "467278  0.0  0.442428  0.000000  0.0  0.000000  0.0  0.0  0.0  0.0  0.0  ...   \n",
       "467279  0.0  0.481303  0.000000  0.0  0.386392  0.0  0.0  0.0  0.0  0.0  ...   \n",
       "467280  0.0  0.495554  0.346067  0.0  0.000000  0.0  0.0  0.0  0.0  0.0  ...   \n",
       "467281  0.0  0.000000  0.000000  0.0  0.000000  0.0  0.0  0.0  0.0  0.0  ...   \n",
       "467282  0.0  0.538335  0.000000  0.0  0.000000  0.0  0.0  0.0  0.0  0.0  ...   \n",
       "\n",
       "               ш    щ    ъ    ы    ь    э    ю         я    ё  labels  \n",
       "0       0.000000  0.0  0.0  0.0  0.0  0.0  0.0  0.000000  0.0     что  \n",
       "1       0.000000  0.0  0.0  0.0  0.0  0.0  0.0  0.000000  0.0     что  \n",
       "2       0.000000  0.0  0.0  0.0  0.0  0.0  0.0  0.000000  0.0     что  \n",
       "3       0.723302  0.0  0.0  0.0  0.0  0.0  0.0  0.000000  0.0     что  \n",
       "4       0.000000  0.0  0.0  0.0  0.0  0.0  0.0  0.000000  0.0     что  \n",
       "...          ...  ...  ...  ...  ...  ...  ...       ...  ...     ...  \n",
       "467278  0.530257  0.0  0.0  0.0  0.0  0.0  0.0  0.425332  0.0  яблоко  \n",
       "467279  0.000000  0.0  0.0  0.0  0.0  0.0  0.0  0.462705  0.0  яблоко  \n",
       "467280  0.000000  0.0  0.0  0.0  0.0  0.0  0.0  0.476404  0.0  яблоко  \n",
       "467281  0.000000  0.0  0.0  0.0  0.0  0.0  0.0  0.448169  0.0  яблоко  \n",
       "467282  0.000000  0.0  0.0  0.0  0.0  0.0  0.0  0.517533  0.0  яблоко  \n",
       "\n",
       "[467283 rows x 34 columns]"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['labels'] = labels\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "6844b43e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics  import f1_score, accuracy_score\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.datasets import make_classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da8ee923",
   "metadata": {},
   "source": [
    "## Перейдем к обучению модели"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52f97459",
   "metadata": {},
   "source": [
    "**Создадим pipeline для более простого тестирования в дальнейшем**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "a0803520",
   "metadata": {},
   "outputs": [],
   "source": [
    "# варианты моделей:\n",
    "# DecisionTreeClassifier()\n",
    "\n",
    "# MultinomialNB()\n",
    "# KNeighborsClassifier()\n",
    "# LogisticRegression()\n",
    "\n",
    "# требует доп настройки\n",
    "# from sklearn.ensemble import RandomForestClassifier\n",
    "# from sklearn.datasets import make_classification\n",
    "\n",
    "## pipline, основанный на TfidfVectorizer и DecisionTreeClassifier()\n",
    "from sklearn.pipeline import Pipeline\n",
    "model_pipeline = Pipeline([\n",
    "    (\"vecorizer\", TfidfVectorizer(tokenizer = lambda x: _tokenize(x))),\n",
    "    (\"model\", KNeighborsClassifier())\n",
    "]\n",
    ")\n",
    "X = corpus\n",
    "y = labels\n",
    "\n",
    "#LogisticRegression(solver = 'sag', random_state = 42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2832e96",
   "metadata": {},
   "source": [
    "**обучим модель**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "34cc7091",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train-test split: 0.02 secs\n",
      "Training: 0.35 secs\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.01, random_state = 42)\n",
    "end = time.time()\n",
    "duration = round(end - start, 2)\n",
    "print(\"Train-test split: \" + str(duration) + \" secs\")\n",
    "start = time.time()\n",
    "model_pipeline.fit(X_train, y_train)\n",
    "end = time.time()\n",
    "duration = round(end - start, 2)\n",
    "print(\"Training: \" + str(duration) + \" secs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "2063751f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "группа\n"
     ]
    }
   ],
   "source": [
    "print(model_pipeline.predict(['грейпрукт'])[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17a9d8dd",
   "metadata": {},
   "source": [
    "**Теперь протестируем обученную модель**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "75fb980c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing: 1.0 secs\n",
      "accuracy: 0.7099811676082862\n",
      "      f1: 0.5566137566137566\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "pred = model_pipeline.predict(X_test)\n",
    "end = time.time()\n",
    "duration = round(end - start, 2)\n",
    "\n",
    "print(\"Testing: \" + str(duration) + \" secs\")\n",
    "print('accuracy:', accuracy_score(y_test, pred))\n",
    "print('      f1:', f1_score(y_test, pred, average = 'macro'))\n",
    "# print('    mean:', mean_absolute_error(y_test, pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34e82ae8",
   "metadata": {},
   "source": [
    "accuracy: 0.6722338204592901\n",
    "      f1: 0.5107402786442088"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e13bc7cf",
   "metadata": {},
   "source": [
    "## Проведем \"живой\" тест на модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "0ded9cb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "назат\n",
      "зато "
     ]
    }
   ],
   "source": [
    "pins = input()\n",
    "pins = re.findall(r'[а-я]+', pins.lower())\n",
    "\n",
    "for pin in pins:\n",
    "    if len(pin) > 2:\n",
    "        print(model_pipeline.predict([pin])[0], end = ' ')\n",
    "    else:\n",
    "        print(pin, end = ' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "dc2d5649",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "print('назад' in labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da7d48ff",
   "metadata": {},
   "source": [
    "## #попытка обучить большие объемы данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "70cf199e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = V\n",
    "y = labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "898a16f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train-test split: 10.29 secs\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Expected 2D array, got 1D array instead:\narray=['систуация' 'пота' 'здохоуть' ... 'багый' 'баддонь' 'кроводдить'].\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[1;32mIn [25]\u001b[0m, in \u001b[0;36m<cell line: 9>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTrain-test split: \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(duration) \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m secs\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      8\u001b[0m start \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m----> 9\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     11\u001b[0m end \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m     12\u001b[0m duration \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mround\u001b[39m(end \u001b[38;5;241m-\u001b[39m start, \u001b[38;5;241m2\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\neighbors\\_classification.py:198\u001b[0m, in \u001b[0;36mKNeighborsClassifier.fit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    179\u001b[0m \u001b[38;5;124;03m\"\"\"Fit the k-nearest neighbors classifier from the training dataset.\u001b[39;00m\n\u001b[0;32m    180\u001b[0m \n\u001b[0;32m    181\u001b[0m \u001b[38;5;124;03mParameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    194\u001b[0m \u001b[38;5;124;03m    The fitted k-nearest neighbors classifier.\u001b[39;00m\n\u001b[0;32m    195\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    196\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweights \u001b[38;5;241m=\u001b[39m _check_weights(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweights)\n\u001b[1;32m--> 198\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\neighbors\\_base.py:400\u001b[0m, in \u001b[0;36mNeighborsBase._fit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    398\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_tags()[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrequires_y\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[0;32m    399\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(X, (KDTree, BallTree, NeighborsBase)):\n\u001b[1;32m--> 400\u001b[0m         X, y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcsr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmulti_output\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    402\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_classifier(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    403\u001b[0m         \u001b[38;5;66;03m# Classification targets require a specific format\u001b[39;00m\n\u001b[0;32m    404\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m y\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m y\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m y\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\base.py:581\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[1;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[0;32m    579\u001b[0m         y \u001b[38;5;241m=\u001b[39m check_array(y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_y_params)\n\u001b[0;32m    580\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 581\u001b[0m         X, y \u001b[38;5;241m=\u001b[39m check_X_y(X, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_params)\n\u001b[0;32m    582\u001b[0m     out \u001b[38;5;241m=\u001b[39m X, y\n\u001b[0;32m    584\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m check_params\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mensure_2d\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m):\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\validation.py:964\u001b[0m, in \u001b[0;36mcheck_X_y\u001b[1;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[0m\n\u001b[0;32m    961\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m y \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    962\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my cannot be None\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 964\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[43mcheck_array\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    965\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    966\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maccept_sparse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    967\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccept_large_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maccept_large_sparse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    968\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    969\u001b[0m \u001b[43m    \u001b[49m\u001b[43morder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43morder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    970\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    971\u001b[0m \u001b[43m    \u001b[49m\u001b[43mforce_all_finite\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_all_finite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    972\u001b[0m \u001b[43m    \u001b[49m\u001b[43mensure_2d\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mensure_2d\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    973\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_nd\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mallow_nd\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    974\u001b[0m \u001b[43m    \u001b[49m\u001b[43mensure_min_samples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mensure_min_samples\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    975\u001b[0m \u001b[43m    \u001b[49m\u001b[43mensure_min_features\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mensure_min_features\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    976\u001b[0m \u001b[43m    \u001b[49m\u001b[43mestimator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    977\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    979\u001b[0m y \u001b[38;5;241m=\u001b[39m _check_y(y, multi_output\u001b[38;5;241m=\u001b[39mmulti_output, y_numeric\u001b[38;5;241m=\u001b[39my_numeric)\n\u001b[0;32m    981\u001b[0m check_consistent_length(X, y)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\validation.py:769\u001b[0m, in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator)\u001b[0m\n\u001b[0;32m    767\u001b[0m     \u001b[38;5;66;03m# If input is 1D raise error\u001b[39;00m\n\u001b[0;32m    768\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m array\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m--> 769\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    770\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected 2D array, got 1D array instead:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124marray=\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    771\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReshape your data either using array.reshape(-1, 1) if \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    772\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myour data has a single feature or array.reshape(1, -1) \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    773\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mif it contains a single sample.\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(array)\n\u001b[0;32m    774\u001b[0m         )\n\u001b[0;32m    776\u001b[0m \u001b[38;5;66;03m# make sure we actually converted to numeric:\u001b[39;00m\n\u001b[0;32m    777\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dtype_numeric \u001b[38;5;129;01mand\u001b[39;00m array\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;241m.\u001b[39mkind \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOUSV\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "\u001b[1;31mValueError\u001b[0m: Expected 2D array, got 1D array instead:\narray=['систуация' 'пота' 'здохоуть' ... 'багый' 'баддонь' 'кроводдить'].\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample."
     ]
    }
   ],
   "source": [
    "model = KNeighborsClassifier()\n",
    "start = time.time()\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42)\n",
    "end = time.time()\n",
    "duration = round(end - start, 2)\n",
    "print(\"Train-test split: \" + str(duration) + \" secs\")\n",
    "\n",
    "start = time.time()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "end = time.time()\n",
    "duration = round(end - start, 2)\n",
    "print(\"Training: \" + str(duration) + \" secs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a4e09e6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "f745c78a",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [210]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m start \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m----> 2\u001b[0m pred \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m end \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m      4\u001b[0m duration \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mround\u001b[39m(end \u001b[38;5;241m-\u001b[39m start, \u001b[38;5;241m2\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\neighbors\\_classification.py:214\u001b[0m, in \u001b[0;36mKNeighborsClassifier.predict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    200\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpredict\u001b[39m(\u001b[38;5;28mself\u001b[39m, X):\n\u001b[0;32m    201\u001b[0m     \u001b[38;5;124;03m\"\"\"Predict the class labels for the provided data.\u001b[39;00m\n\u001b[0;32m    202\u001b[0m \n\u001b[0;32m    203\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    212\u001b[0m \u001b[38;5;124;03m        Class labels for each data sample.\u001b[39;00m\n\u001b[0;32m    213\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 214\u001b[0m     neigh_dist, neigh_ind \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkneighbors\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    215\u001b[0m     classes_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclasses_\n\u001b[0;32m    216\u001b[0m     _y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_y\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\neighbors\\_base.py:752\u001b[0m, in \u001b[0;36mKNeighborsMixin.kneighbors\u001b[1;34m(self, X, n_neighbors, return_distance)\u001b[0m\n\u001b[0;32m    749\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    750\u001b[0m         kwds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39meffective_metric_params_\n\u001b[1;32m--> 752\u001b[0m     chunked_results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m    753\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpairwise_distances_chunked\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    754\u001b[0m \u001b[43m            \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    755\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_X\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    756\u001b[0m \u001b[43m            \u001b[49m\u001b[43mreduce_func\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreduce_func\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    757\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmetric\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meffective_metric_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    758\u001b[0m \u001b[43m            \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    759\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    760\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    761\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    763\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fit_method \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mball_tree\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkd_tree\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[0;32m    764\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m issparse(X):\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\pairwise.py:1717\u001b[0m, in \u001b[0;36mpairwise_distances_chunked\u001b[1;34m(X, Y, reduce_func, metric, n_jobs, working_memory, **kwds)\u001b[0m\n\u001b[0;32m   1715\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1716\u001b[0m     X_chunk \u001b[38;5;241m=\u001b[39m X[sl]\n\u001b[1;32m-> 1717\u001b[0m D_chunk \u001b[38;5;241m=\u001b[39m pairwise_distances(X_chunk, Y, metric\u001b[38;5;241m=\u001b[39mmetric, n_jobs\u001b[38;5;241m=\u001b[39mn_jobs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m   1718\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (X \u001b[38;5;129;01mis\u001b[39;00m Y \u001b[38;5;129;01mor\u001b[39;00m Y \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mand\u001b[39;00m PAIRWISE_DISTANCE_FUNCTIONS\u001b[38;5;241m.\u001b[39mget(\n\u001b[0;32m   1719\u001b[0m     metric, \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1720\u001b[0m ) \u001b[38;5;129;01mis\u001b[39;00m euclidean_distances:\n\u001b[0;32m   1721\u001b[0m     \u001b[38;5;66;03m# zeroing diagonal, taking care of aliases of \"euclidean\",\u001b[39;00m\n\u001b[0;32m   1722\u001b[0m     \u001b[38;5;66;03m# i.e. \"l2\"\u001b[39;00m\n\u001b[0;32m   1723\u001b[0m     D_chunk\u001b[38;5;241m.\u001b[39mflat[sl\u001b[38;5;241m.\u001b[39mstart :: _num_samples(X) \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\pairwise.py:1889\u001b[0m, in \u001b[0;36mpairwise_distances\u001b[1;34m(X, Y, metric, n_jobs, force_all_finite, **kwds)\u001b[0m\n\u001b[0;32m   1886\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m distance\u001b[38;5;241m.\u001b[39msquareform(distance\u001b[38;5;241m.\u001b[39mpdist(X, metric\u001b[38;5;241m=\u001b[39mmetric, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds))\n\u001b[0;32m   1887\u001b[0m     func \u001b[38;5;241m=\u001b[39m partial(distance\u001b[38;5;241m.\u001b[39mcdist, metric\u001b[38;5;241m=\u001b[39mmetric, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[1;32m-> 1889\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _parallel_pairwise(X, Y, func, n_jobs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\pairwise.py:1430\u001b[0m, in \u001b[0;36m_parallel_pairwise\u001b[1;34m(X, Y, func, n_jobs, **kwds)\u001b[0m\n\u001b[0;32m   1427\u001b[0m X, Y, dtype \u001b[38;5;241m=\u001b[39m _return_float_dtype(X, Y)\n\u001b[0;32m   1429\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m effective_n_jobs(n_jobs) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m-> 1430\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m func(X, Y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m   1432\u001b[0m \u001b[38;5;66;03m# enforce a threading backend to prevent data communication overhead\u001b[39;00m\n\u001b[0;32m   1433\u001b[0m fd \u001b[38;5;241m=\u001b[39m delayed(_dist_wrapper)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\pairwise.py:330\u001b[0m, in \u001b[0;36meuclidean_distances\u001b[1;34m(X, Y, Y_norm_squared, squared, X_norm_squared)\u001b[0m\n\u001b[0;32m    324\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m Y_norm_squared\u001b[38;5;241m.\u001b[39mshape \u001b[38;5;241m!=\u001b[39m (\u001b[38;5;241m1\u001b[39m, Y\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]):\n\u001b[0;32m    325\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    326\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIncompatible dimensions for Y of shape \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mY\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m and \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    327\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mY_norm_squared of shape \u001b[39m\u001b[38;5;132;01m{\u001b[39;00moriginal_shape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    328\u001b[0m         )\n\u001b[1;32m--> 330\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_euclidean_distances\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_norm_squared\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY_norm_squared\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msquared\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\pairwise.py:371\u001b[0m, in \u001b[0;36m_euclidean_distances\u001b[1;34m(X, Y, X_norm_squared, Y_norm_squared, squared)\u001b[0m\n\u001b[0;32m    368\u001b[0m     distances \u001b[38;5;241m=\u001b[39m _euclidean_distances_upcast(X, XX, Y, YY)\n\u001b[0;32m    369\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    370\u001b[0m     \u001b[38;5;66;03m# if dtype is already float64, no need to chunk and upcast\u001b[39;00m\n\u001b[1;32m--> 371\u001b[0m     distances \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[43msafe_sparse_dot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mT\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdense_output\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    372\u001b[0m     distances \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m XX\n\u001b[0;32m    373\u001b[0m     distances \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m YY\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\extmath.py:153\u001b[0m, in \u001b[0;36msafe_sparse_dot\u001b[1;34m(a, b, dense_output)\u001b[0m\n\u001b[0;32m    151\u001b[0m         ret \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mdot(a, b)\n\u001b[0;32m    152\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 153\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43ma\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m@\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mb\u001b[49m\n\u001b[0;32m    155\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    156\u001b[0m     sparse\u001b[38;5;241m.\u001b[39missparse(a)\n\u001b[0;32m    157\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m sparse\u001b[38;5;241m.\u001b[39missparse(b)\n\u001b[0;32m    158\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m dense_output\n\u001b[0;32m    159\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(ret, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtoarray\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    160\u001b[0m ):\n\u001b[0;32m    161\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ret\u001b[38;5;241m.\u001b[39mtoarray()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\scipy\\sparse\\_base.py:620\u001b[0m, in \u001b[0;36mspmatrix.__matmul__\u001b[1;34m(self, other)\u001b[0m\n\u001b[0;32m    617\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m isscalarlike(other):\n\u001b[0;32m    618\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mScalar operands are not allowed, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    619\u001b[0m                      \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muse \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m*\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m instead\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 620\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_mul_dispatch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mother\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\scipy\\sparse\\_base.py:534\u001b[0m, in \u001b[0;36mspmatrix._mul_dispatch\u001b[1;34m(self, other)\u001b[0m\n\u001b[0;32m    532\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m!=\u001b[39m other\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]:\n\u001b[0;32m    533\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdimension mismatch\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m--> 534\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_mul_sparse_matrix\u001b[49m\u001b[43m(\u001b[49m\u001b[43mother\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    536\u001b[0m \u001b[38;5;66;03m# If it's a list or whatever, treat it like a matrix\u001b[39;00m\n\u001b[0;32m    537\u001b[0m other_a \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masanyarray(other)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\scipy\\sparse\\_compressed.py:533\u001b[0m, in \u001b[0;36m_cs_matrix._mul_sparse_matrix\u001b[1;34m(self, other)\u001b[0m\n\u001b[0;32m    530\u001b[0m data \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mempty(nnz, dtype\u001b[38;5;241m=\u001b[39mupcast(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdtype, other\u001b[38;5;241m.\u001b[39mdtype))\n\u001b[0;32m    532\u001b[0m fn \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(_sparsetools, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mformat \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_matmat\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m--> 533\u001b[0m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mM\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mN\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43masarray\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindptr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43midx_dtype\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    534\u001b[0m \u001b[43m   \u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43masarray\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindices\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43midx_dtype\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    535\u001b[0m \u001b[43m   \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    536\u001b[0m \u001b[43m   \u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43masarray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mother\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindptr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43midx_dtype\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    537\u001b[0m \u001b[43m   \u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43masarray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mother\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindices\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43midx_dtype\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    538\u001b[0m \u001b[43m   \u001b[49m\u001b[43mother\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    539\u001b[0m \u001b[43m   \u001b[49m\u001b[43mindptr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindices\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    541\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m((data, indices, indptr), shape\u001b[38;5;241m=\u001b[39m(M, N))\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "pred = model.predict(X_test[:50])\n",
    "end = time.time()\n",
    "duration = round(end - start, 2)\n",
    "print(\"Testing: \" + str(duration) + \" secs\")\n",
    "print('accuracy:', accuracy_score(y_test[:50], pred))\n",
    "print('      f1:', f1_score(y_test, pred[:50], average = 'macro'))\n",
    "#print('    mean:', mean_absolute_error(y_test, pred))\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "9c5da951",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = model.predict(X_test[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1abb983",
   "metadata": {},
   "source": [
    "## bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73f86ab9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa5a4ade",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "id": "24d8cb4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage is 182.116 MB\n"
     ]
    }
   ],
   "source": [
    "def convert_to_sparse_pandas(df, exclude_columns = []):\n",
    "    df = df.copy()\n",
    "    exclude_columns = set(exclude_columns)\n",
    "\n",
    "    for (columnName, columnData) in df.iteritems():\n",
    "        if columnName in exclude_columns:\n",
    "            continue\n",
    "        df[columnName] = pd.arrays.SparseArray(columnData.values, dtype = np.float16)\n",
    "\n",
    "    return df\n",
    "\n",
    "df_sparse = convert_to_sparse_pandas(df, exclude_columns = [\"label\"])\n",
    "# display(df_sparse.dtypes)\n",
    "print_memory_usage_of_data_frame(df_sparse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "bffcb3cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6571622078283824\n",
      "0.5426987035755111\n",
      "53.354150671547565\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "print(accuracy_score(y_test, pred))\n",
    "print(f1_score(y_test, pred, average = 'macro'))\n",
    "\n",
    "print(mean_absolute_error(y_test, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "id": "344750e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X:\n",
      "Memory usage is 233.391 MB\n",
      "X_sparse:\n",
      "Memory usage is 175.044 MB\n",
      "X_csr:\n",
      "Memory usage is 68.581552 MB\n",
      "X_binary:\n",
      "Memory usage is 68.581552 MB\n"
     ]
    }
   ],
   "source": [
    "y = df['label']\n",
    "X = df[df.columns.difference(['label'])]\n",
    "\n",
    "y_sparse = df_sparse['label']\n",
    "X_sparse = df_sparse[df_sparse.columns.difference(['label'])]\n",
    "\n",
    "y_csr = df['label']\n",
    "X_csr = V\n",
    "\n",
    "y_binary = df['label']\n",
    "X_binary = V_bin\n",
    "\n",
    "print('X:')\n",
    "print_memory_usage_of_data_frame(X)\n",
    "print('X_sparse:')\n",
    "print_memory_usage_of_data_frame(X_sparse)\n",
    "print('X_csr:')\n",
    "get_csr_memory_usage(X_csr)\n",
    "print('X_binary:')\n",
    "get_csr_memory_usage(X_binary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "id": "fed72e5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scipy sparse matrix\n",
      "Testing: 0.32 secs\n",
      "accuracy: 0.011158744881569125\n",
      "      f1: 0.014289106223557658\n",
      "    mean: 188.53618532678777\n",
      "\n",
      "\n",
      "Scipy sparse matrix binary\n",
      "Testing: 0.49 secs\n",
      "accuracy: 0.708187227111282\n",
      "      f1: 0.5981014691555474\n",
      "    mean: 45.30782978530869\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(models)):\n",
    "    print(list_of_names[i])\n",
    "    test(models[i], X_tests_arr[i], Y_tests_arr[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "id": "a331e81e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(mod, Xtest, Ytest):\n",
    "    start = time.time()\n",
    "    pred = mod.predict(Xtest)\n",
    "    end = time.time()\n",
    "    duration = round(end - start, 2)\n",
    "    print(\"Testing: \" + str(duration) + \" secs\")\n",
    "    print('accuracy:', accuracy_score(Ytest, pred))\n",
    "    print('      f1:', f1_score(Ytest, pred, average = 'macro'))\n",
    "    print('    mean:', mean_absolute_error(Ytest, pred))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "id": "e1e2ab54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scipy sparse matrix\n",
      "Train-test split: 0.26 secs\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [313]\u001b[0m, in \u001b[0;36m<cell line: 23>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTrain-test split: \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(duration) \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m secs\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     37\u001b[0m start \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m---> 38\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     40\u001b[0m end \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m     41\u001b[0m duration \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mround\u001b[39m(end \u001b[38;5;241m-\u001b[39m start, \u001b[38;5;241m2\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\tree\\_classes.py:937\u001b[0m, in \u001b[0;36mDecisionTreeClassifier.fit\u001b[1;34m(self, X, y, sample_weight, check_input, X_idx_sorted)\u001b[0m\n\u001b[0;32m    899\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfit\u001b[39m(\n\u001b[0;32m    900\u001b[0m     \u001b[38;5;28mself\u001b[39m, X, y, sample_weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, check_input\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, X_idx_sorted\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdeprecated\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    901\u001b[0m ):\n\u001b[0;32m    902\u001b[0m     \u001b[38;5;124;03m\"\"\"Build a decision tree classifier from the training set (X, y).\u001b[39;00m\n\u001b[0;32m    903\u001b[0m \n\u001b[0;32m    904\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    934\u001b[0m \u001b[38;5;124;03m        Fitted estimator.\u001b[39;00m\n\u001b[0;32m    935\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 937\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    938\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    939\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    940\u001b[0m \u001b[43m        \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    941\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcheck_input\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcheck_input\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    942\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX_idx_sorted\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX_idx_sorted\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    943\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    944\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\tree\\_classes.py:420\u001b[0m, in \u001b[0;36mBaseDecisionTree.fit\u001b[1;34m(self, X, y, sample_weight, check_input, X_idx_sorted)\u001b[0m\n\u001b[0;32m    409\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    410\u001b[0m     builder \u001b[38;5;241m=\u001b[39m BestFirstTreeBuilder(\n\u001b[0;32m    411\u001b[0m         splitter,\n\u001b[0;32m    412\u001b[0m         min_samples_split,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    417\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmin_impurity_decrease,\n\u001b[0;32m    418\u001b[0m     )\n\u001b[1;32m--> 420\u001b[0m \u001b[43mbuilder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuild\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtree_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    422\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_outputs_ \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m is_classifier(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    423\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_classes_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_classes_[\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "models = []\n",
    "list_of_names = []\n",
    "X_tests_arr = []\n",
    "Y_tests_arr = []\n",
    "\n",
    "model = DecisionTreeClassifier()\n",
    "# model = BernoulliNB()\n",
    "\n",
    "# model = GaussianNB()\n",
    "# model = MultinomialNB()\n",
    "# msodel = GaussianNB()\n",
    "# model = MultinomialNB()\n",
    "# model = KNeighborsClassifier()\n",
    "# model = LogisticRegression()\n",
    "\n",
    "\n",
    "#     'Pandas dataframe': [X, y],\n",
    "vector_dict = {\n",
    "     'Scipy sparse matrix': [X_csr, y_csr],\n",
    "     'Scipy sparse matrix binary': [X_binary, y_binary]\n",
    "    }\n",
    "\n",
    "for key, item in vector_dict.items():\n",
    "    print(key)\n",
    "    list_of_names.append(key)\n",
    "    start = time.time()\n",
    "    if (key != 'Pandas dataframe'):\n",
    "        XX = item[0].toarray()\n",
    "    else:\n",
    "        XX = X\n",
    "        \n",
    "    X_train, X_test, y_train, y_test = train_test_split(XX, y, test_size = 0.2, random_state = 42)\n",
    "    end = time.time()\n",
    "    duration = round(end - start, 2)\n",
    "    print(\"Train-test split: \" + str(duration) + \" secs\")\n",
    "\n",
    "    start = time.time()\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    end = time.time()\n",
    "    duration = round(end - start, 2)\n",
    "    \n",
    "    models.append(model)\n",
    "    X_tests_arr.append(X_test)\n",
    "    Y_tests_arr.append(y_test)\n",
    "    print(\"Training: \" + str(duration) + \" secs\")\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06e21790",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokets_list = vectorizer_bin.get_feature_names_out()\n",
    "\n",
    "def _tokenize2(pin):\n",
    "    return [a + a for a in pin]\n",
    "\n",
    "pins = input()\n",
    "pins = re.findall(r'[а-я]+', pins.lower())\n",
    "\n",
    "for pin in pins:\n",
    "    if(len(pin) < 3):\n",
    "        print(pin, end = ' ')\n",
    "        continue\n",
    "    pin_vec = [0]*33\n",
    "    pin = _tokenize2(pin)\n",
    "    for i in range(33):\n",
    "        if tokets_list[i] in pin:\n",
    "            pin_vec[i] = 1\n",
    "\n",
    "    predict = models[0].predict([pin_vec])\n",
    "    print(list_of_words[predict[0]], end = ' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "a6252d62",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "\n",
    "class OnlinePipeline(Pipeline):\n",
    "    def partial_fit(self, X, y = None):\n",
    "        for i, step in enumerate(self.steps):\n",
    "            name, est = step\n",
    "            est.partial_fit(X, y)\n",
    "            if i < len(self.steps) - 1:\n",
    "                X = est.transform(X)\n",
    "        return self\n",
    "\n",
    "model_pipeline = OnlinePipeline([\n",
    "    (\"vecorizer\", TfidfVectorizer(tokenizer = lambda x: _tokenize(x))),\n",
    "    (\"model\", DecisionTreeClassifier())\n",
    "]\n",
    ")\n",
    "\n",
    "X = corpus\n",
    "y = labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "b70ce696",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train-test split: 0.1 secs\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "OnlinePipeline(steps=[('vecorizer',\n",
       "                       TfidfVectorizer(tokenizer=<function <lambda> at 0x000001C00208BF40>)),\n",
       "                      ('model', DecisionTreeClassifier())])"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start = time.time()\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42)\n",
    "end = time.time()\n",
    "duration = round(end - start, 2)\n",
    "print(\"Train-test split: \" + str(duration) + \" secs\")\n",
    "\n",
    "a = int(len(X_train)/3)\n",
    "sum_t = 0\n",
    "\n",
    "\n",
    "div = 100\n",
    "part = int(len(X_train)/div)\n",
    "\n",
    "'''for i in range(1, div + 1):\n",
    "    start = time.time()\n",
    "    print((i*(part) - part), \":\", i*(part))\n",
    "    model_pipeline.fit(X_train[(i*(part) - part): i*(part)], y_train[(i*(part) - part): i*(part)])\n",
    "    end = time.time()\n",
    "    duration = round(end - start, 2)\n",
    "    sum_t += duration\n",
    "    if(i % 10 == 1):\n",
    "        print(\"Training round \" + str(i) + \": \" + str(sum_t) + \" secs\")'''\n",
    "    \n",
    "\n",
    "#print(\"        Training: \" + str(sum_t) + \" secs\")\n",
    "t = int(len(X_train)/2)\n",
    "model_pipeline.fit(X_train[:t], y_train[:t])\n",
    "model_pipeline.fit(X_train[t:], y_train[t:])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
