{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Автокорректор ошибок на Python**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "%pylab is deprecated, use %matplotlib inline and import the required libraries.\n",
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\wequalwo\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\IPython\\core\\magics\\pylab.py:162: UserWarning: pylab import has clobbered these variables: ['random', 'sample']\n",
      "`%matplotlib` prevents importing * from pylab and numpy\n",
      "  warn(\"pylab import has clobbered these variables: %s\"  % clobbered +\n"
     ]
    }
   ],
   "source": [
    "# Импортируй и властвуй\n",
    "%pylab inline\n",
    "import re\n",
    "import math\n",
    "import string\n",
    "from collections import Counter\n",
    "import requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для начала немного теории. \n",
    "Пусть дано слово, будем пытаться отыскать слово, в котором с наибольшей вероятностью исправлены допущенные ошибки (если ошибок нет, то таким словом будет данное). Разумеется, мы не сможем гарантировать 100% исправления всех ошибок. (Например, если нам дано слово «пак», то правильным будет слово «паз» или «парк» ?), именно поэтому мы используем вероятностный (или другими словами стохастический) подход. \n",
    "Будем говорить, что мы пытаемся выбрать такое слово c из всех возможных слов-исправлений, что вероятность появления именно слова c при данном слове w будет максимальна:\n",
    "\n",
    "argmaxc P(c|w)\n",
    "\n",
    "\n",
    "Согласно теореме Байеса - выражение, записанное выше, эквивалентно следующему выражению:\n",
    "\n",
    "***argmaxc P(w|c) P(c) / P(w)***\n",
    "\n",
    "Поскольку P(w) одинакова для всех c мы можем отбросить P(w), что даст нам:\n",
    "\n",
    "***argmaxc P(w|c) P(c)***\n",
    "\n",
    "\n",
    "В этом выражении присутствуют три части.  \n",
    "Справа налево:\n",
    "\n",
    "***P(c)*** – вероятность появления слова c (частотность употребления c). Эта вероятность обусловлена самим языком (точнее моделью языка). Иначе говоря, P(c) определяет как часто c встречается в текстах на русском языке. P(«превед») будет достаточно высока, тогда как P(«благоденствовать») будет меньше, а P(«ыгввыцшы») будет около нуля.\n",
    "\n",
    "***P(w|c)*** – вероятность того, что автор опечатался и написал w, хотя имел в виду c. По сути дела эта вероятность обусловлена частотностью тех или иных ошибок в языке (и называется моделью ошибок языка).\n",
    "\n",
    "***argmaxc*** – оператор, перебирающий все возможные c в поиске наиболее (вероятнее всего) подходящего из них (т.е. данный оператор ищет такое допустимое c, которе максимизирует условную вероятность появления w при данном c).\n",
    "\n",
    "Может возникнуть очевидный вопрос – зачем мы преобразовали простое выражение «argmaxc P(c|w)» с помощью какого-то Байеса в более сложное выражение, в котором используются аж две языковые модели, вместо одной? Дело в том, что P(c|w) учитывает в себе сразу обе языковых модели, поэтому очевидно, что проще выделить эти модели и работать с ними по отдельности. Предположим у нас есть слово с опечаткой – «езать», это может быть как «ехать», так и «резать». Для какого из исправлений P(c|w) будет максимально ? Оба исправления имеют примерно одинаковую частотность в русском языке. Хорошо допустим «х» и «з» близко расположены в русской раскладке клавиатуры и это повышает вероятность варианта «ехать», но это не повод, чтоб отбрасывать «резать», ведь «е» и «р» тоже близки. Поэтому лучше не рассматривать P(c|w) как единую величину, ибо нам приходится учитывать и частность исправления c и вероятность исправления c для данной опечатки в w. Удобнее работать с этими двумя вероятностями по отдельности."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Начнем с P(c)**. Мы читаем большой текстовый файл, russian.txt, в котором записано много слов."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "После этого мы извлекаем отдельные слова из файла \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import codecs\n",
    "with codecs.open('russian.txt', 'r', encoding = 'windows 1251') as file:\n",
    "    TEXT = file.read().replace('\\n', ' ') # для текста, в котором слова разделены '\\n'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokens(text):\n",
    "    \"\"\"Возвращает список токенов (подряд идущих буквенных последовательностей) в тексте. \n",
    "       Текст при этом приводится к нижнему регистру.\"\"\"\n",
    "    return re.findall(r'[а-ё]+', text.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1536030"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "WORDS = tokens(TEXT)\n",
    "len(WORDS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Хочу заметить, что сейчас слова появляются в нашем списке в том порядке, как они располагались в файле"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Модель: Мешок слов (aka Bag of Words)***\n",
    "\n",
    "Мы создали список *WORDS* - список слов в том порядке, как они следуют в *TEXT*. Мы можем использовать этот список в качестве порождающей модели (generative model) текста. Язык - очень сложная штука и мы создаем крайне упрощенную модель языка, которая может ухватить часть этой сложной структуры. \n",
    "\n",
    "В модели мешка слов , мы полностью игнорируем порядок слов, зато соблюдаем их частоту. Представить это можно себе так: вы берете все слова текста и забрасываете их в мешок. Теперь, если вы хотите сгенерировать предложение с помощью этого мешка, вы просто трясете его(слова там перемешиваются) и достаете указанное количество слов по одному (мешок непрозрачный, так что слоа вы достаете наугад). Почти наверное полученное предложение будет грамматически некорректным, но слова в этом предложении будут в +- правильной пропорции (более частые будут встречаться чаще, более редкие - реже). Вот функция, которая сэмплирует(от англ. sample) предложение из n слов с помощью нашего мешка:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "def sample(bag, n=10):\n",
    "    \"Sample a random n-word sentence from the model described by the bag of words.\"\n",
    "    return ' '.join(random.choice(bag) for _ in range(n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'поправленных недослышано непоседливые точившуюся проверяю наитягчайшее обволакивающего дерзают полицейских ограничите'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample(WORDS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Другое представление мешка слов - Counter. Это словарь, состоящий из пар {'слово': кол-во вхождений слова в текст}. Например,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'между': 1,\n",
       "         'нами': 1,\n",
       "         'провода': 1,\n",
       "         'города': 1,\n",
       "         'да': 6,\n",
       "         'я': 1,\n",
       "         'сказал': 1,\n",
       "         'иди': 1,\n",
       "         'сюда': 1,\n",
       "         'и': 1,\n",
       "         'ты': 1,\n",
       "         'сказала': 1})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(tokens('Между нами провода, Города да да да. Я сказал иди сюда, И ты сказала: «Да, да, да..»'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Counter очень похож на словарь из Python - тип dict , но у него есть ряд дополнительных методов. Давайте завернем в Counter наш список слов WORDS и посмотрим, что получится:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('по', 218), ('военно', 49), ('то', 31), ('красный', 28), ('социал', 26), ('пресс', 25), ('пол', 25), ('советский', 24), ('технический', 22), ('генерал', 22)]\n"
     ]
    }
   ],
   "source": [
    "COUNTS = Counter(WORDS)\n",
    "\n",
    "print(COUNTS.most_common(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 самые\n",
      "1 редкие\n",
      "1 слова\n",
      "0 крыжить\n",
      "1 михрютка\n",
      "1 драдедамовый\n"
     ]
    }
   ],
   "source": [
    "for w in tokens('самые редкие слова: Крыжить, Михрютка, Драдедамовый '):\n",
    "    print(COUNTS[w], w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В 1935, лингвист Джордж Ципф отметил, что в любом большом тексте n-тое наиболее часто встречающееся слово появляется с частотой ~ 1/n от частоты наиболее часто встречающегося слова. Это наблюдение получило название Закона Ципфа, несмотря на то, что Феликс Ауэрбах заметил это еще в 1913 году. Если нарисовать частоты слов, начиная от самого часто встречающегося, на log-log-графике, они должны приблизительно следовать прямой линии, если закон Ципфа верен. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x19990ad9a20>]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib\n",
    "M = COUNTS['и']\n",
    "yscale('log')\n",
    "xscale('log')\n",
    "title('Частота n-того наиболее частого слова и линия 1/n.')\n",
    "plot([c for (w, c) in COUNTS.most_common()])\n",
    "plot([M/i for i in range(1, len(COUNTS)+1)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Задача: Проверка Правописания***\n",
    "\n",
    "Для данного слова w нужно найти наиболее вероятную правку *c = correct(w).*\n",
    "\n",
    "**Подход:** Найти все кандидаты c, достаточно близкие к w. Выбрать наиболее вероятный из них.\n",
    "\n",
    "Осталось понять, что такое близкие и наиболее вероятный.\n",
    "\n",
    "Применим наивный подход: всегда будем брать более близкое слово, если проверки на близость недостаточно, берем слово с максимальной частотой из WORDS. \n",
    "Сейчас мы будем измерять близость с помощью расстояния Левенштейна: минимального необходимого количества удалений, перестановок, вставок, и замен символов, необходимых чтобы одно слово превратить в другое. Конечно же это не единственный возможный подход. Методом проб и ошибок можно понять, что поиск слов в пределах расстояния 2 уже даст пристойные результаты (или можно почитать в литературе). Тогда остается определить функцию *correct(w):*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correct(word):\n",
    "    \"Поиск лучшего исправления ошибки для данного слова.\"\n",
    "    # предрассчитать edit_distance==0, затем 1, затем 2; в противном случае оставить слово \"как есть\".\n",
    "    candidates = (known(edits0(word)) or \n",
    "                  known(edits1(word)) or \n",
    "                  known(edits2(word)) or \n",
    "                  [word])\n",
    "    return max(candidates, key=COUNTS.get)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Функции known и edits0 простые; функция edits2 iлегко получается из функции edits1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# поменять функции\n",
    "def known(words):\n",
    "    \"Вернуть подмножество слов, которое есть в нашем словаре.\"\n",
    "    return {w for w in words if w in COUNTS}\n",
    "\n",
    "def edits0(word): \n",
    "    \"Вернуть все строки, которые находятся на edit_distance == 0 от word (т.е., просто само слово).\"\n",
    "    return {word}\n",
    "\n",
    "def edits2(word):\n",
    "    \"Вернуть все строки, которые находятся на edit_distance == 2 от word.\"\n",
    "    return {e2 for e1 in edits1(word) for e2 in edits1(e1)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def edits1(word):\n",
    "    \"Возвращает список всех строк на расстоянии edit_distance == 1 от word.\"\n",
    "    pairs      = splits(word)\n",
    "    deletes    = [a+b[1:]           for (a, b) in pairs if b]\n",
    "    transposes = [a+b[1]+b[0]+b[2:] for (a, b) in pairs if len(b) > 1]\n",
    "    replaces   = [a+c+b[1:]         for (a, b) in pairs for c in alphabet if b]\n",
    "    inserts    = [a+c+b             for (a, b) in pairs for c in alphabet]\n",
    "    return set(deletes + transposes + replaces + inserts)\n",
    "\n",
    "def splits(word):\n",
    "    \"Возвращает список всех возможных разбиений слова на пару (a, b).\"\n",
    "    return [(word[:i], word[i:]) \n",
    "            for i in range(len(word)+1)]\n",
    "\n",
    "alphabet = 'абвгдеёжзийклмнопрстуфхцчшщъыьяюя'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('', 'атец'), ('а', 'тец'), ('ат', 'ец'), ('ате', 'ц'), ('атец', '')]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "splits('атец')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "33"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(alphabet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'атец'}\n"
     ]
    }
   ],
   "source": [
    "print(edits0('атец'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ътец', 'атет', 'атюец', 'аетец', 'атрц', 'атею', 'атеь', 'ате', 'ттец', 'атетц', 'атвц', 'атмец', 'ател', 'атцец', 'атбц', 'ажец', 'атёец', 'атсец', 'атецз', 'атеч', 'татец', 'иатец', 'аюец', 'аеец', 'затец', 'атеяц', 'юатец', 'датец', 'атхц', 'атец', 'атеж', 'аытец', 'атлец', 'чтец', 'амтец', 'утец', 'ютец', 'ахец', 'атщц', 'атдец', 'атецф', 'ьтец', 'цтец', 'аткец', 'атце', 'фтец', 'отец', 'атнц', 'абтец', 'атъц', 'атеа', 'ааец', 'атеъ', 'йатец', 'атеёц', 'мтец', 'атей', 'атещ', 'атеюц', 'атецш', 'атецг', 'атфц', 'абец', 'атьц', 'атеё', 'аттец', 'атац', 'аутец', 'атяец', 'атее', 'тец', 'таец', 'ачец', 'атшц', 'цатец', 'чатец', 'атфец', 'атеци', 'айтец', 'атес', 'атецъ', 'атецл', 'атжец', 'атеьц', 'хтец', 'атгец', 'атекц', 'ытец', 'атехц', 'ёатец', 'атзец', 'атепц', 'атеу', 'атвец', 'ыатец', 'аетц', 'атецд', 'атерц', 'атецм', 'афтец', 'атецй', 'атнец', 'азец', 'атеы', 'атиец', 'атеуц', 'атзц', 'аотец', 'ашец', 'атецт', 'лтец', 'агец', 'нтец', 'штец', 'атев', 'атжц', 'аътец', 'аютец', 'атяц', 'атезц', 'атуц', 'атещц', 'агтец', 'атеф', 'катец', 'атецц', 'арец', 'аьец', 'атег', 'аьтец', 'аец', 'атеец', 'атыец', 'атедц', 'атегц', 'ватец', 'атщец', 'атцц', 'еатец', 'артец', 'атез', 'атенц', 'атпц', 'дтец', 'атецв', 'адтец', 'атдц', 'ахтец', 'атуец', 'втец', 'атем', 'атецо', 'атоц', 'аткц', 'ауец', 'атрец', 'яатец', 'атецх', 'атецж', 'уатец', 'фатец', 'атеоц', 'патец', 'атеш', 'атйец', 'ьатец', 'атйц', 'ащтец', 'атиц', 'ачтец', 'атейц', 'атецы', 'атешц', 'аыец', 'атеи', 'атер', 'аёец', 'алтец', 'астец', 'атек', 'атьец', 'ателц', 'актец', 'ъатец', 'атен', 'ратец', 'атхец', 'батец', 'атечц', 'анец', 'ктец', 'стец', 'ащец', 'атаец', 'атевц', 'атчец', 'ажтец', 'алец', 'гатец', 'атшец', 'атецё', 'атыц', 'азтец', 'атефц', 'атецю', 'ётец', 'аъец', 'атецб', 'атець', 'атея', 'атецк', 'итец', 'атц', 'атоец', 'атецн', 'атеыц', 'атеб', 'автец', 'матец', 'аётец', 'атецс', 'атеце', 'атежц', 'атлц', 'оатец', 'атеца', 'атёц', 'атеъц', 'атемц', 'атеац', 'сатец', 'атъец', 'аятец', 'аоец', 'щатец', 'атецп', 'атецр', 'атеиц', 'жатец', 'аяец', 'бтец', 'птец', 'ртец', 'атеп', 'атецу', 'йтец', 'атмц', 'акец', 'атед', 'айец', 'атео', 'адец', 'ацтец', 'аштец', 'аитец', 'зтец', 'авец', 'атесц', 'латец', 'атех', 'антец', 'атбец', 'амец', 'атсц', 'атчц', 'атеця', 'атецч', 'аттц', 'аптец', 'атпец', 'шатец', 'гтец', 'ацец', 'етец', 'жтец', 'атебц', 'атецщ', 'афец', 'атюц', 'асец', 'атгц', 'щтец', 'аатец', 'натец', 'хатец', 'ятец', 'аиец', 'апец'}\n"
     ]
    }
   ],
   "source": [
    "print(edits1('атец'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36956\n"
     ]
    }
   ],
   "source": [
    "print(len(edits2('атец')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ана', 'ни', 'зочит', 'в', 'шшколу']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens('Ана ни зочит в шшколу.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ана', 'ни', 'хотит', 'в', 'школу']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(map(correct, tokens('Ана ни хочит в шшколу.')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "print('хотит' in WORDS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Теория: От счетчика слов к вероятностям последовательностей слов***\n",
    "\n",
    "Нам нужно научиться подсчитывать вероятности слов, P(w). Делать мы это будем с помощью функции pdist, которая на вход принимает Counter (наш мешок слов) и возвращает функцию, выполняющую роль вероятностного распределения на множестве всех возможных слов. В вероятностном распределении вероятность каждого слова лежит между 0 и 1, и сложение вероятностей всех слов дает 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pdist(counter):\n",
    "    \"Превращает частоты из Counter в вероятностное распределение.\"\n",
    "    N = sum(list(counter.values()))\n",
    "    return lambda x: counter[x]/N\n",
    "\n",
    "P = pdist(COUNTS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0181897488981335e-05 то\n",
      "2.6041158050298497e-06 мать\n",
      "1.3020579025149248e-06 пирогов\n",
      "6.510289512574624e-07 напечет\n",
      "2.0181897488981335e-05 то\n",
      "6.510289512574624e-07 бабушка\n",
      "6.510289512574624e-06 с\n",
      "6.510289512574624e-07 булочками\n",
      "6.510289512574624e-07 приедет\n"
     ]
    }
   ],
   "source": [
    "for w in tokens('То мать пирогов напечет, то бабушка с булочками приедет'):\n",
    "    print(P(w), w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Итак, что же такое вероятность последовательности слов? Используем определение совместной вероятности:\n",
    "$P(w_1 ... w_n) = P(w_1)*P(w_2|w_1)*P(w_3|w_1w_2)...*...P(w_n|w_1...w_n-1)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Модель мешка слов подразумевает, что каждое слово из мешка достается независимо от других. Это дает нам упрощенную аппроксимацию:\n",
    "P(w_1 ... w_n) = P(w_1)*P(w_2)*P(w_3)...*...P(w_n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Известный статистик Джордж Бокс сказал Все модели неверны, но некоторые полезны."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как же нам посчитать P(w_1 ... w_n)? Мы будем использовать другое название, чтобы не обманывать себя, Pwords вместо P, и посчитаем ее как произведение индивидуальных вероятностей:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Pwords(words):\n",
    "    \"Вероятности слов, при условии, что они независимы.\"\n",
    "    return product(P(w) for w in words)\n",
    "\n",
    "def product(nums):\n",
    "    \"Перемножим числа.  (Это как `sum`, только с умножением.)\"\n",
    "    result = 1\n",
    "    for x in nums:\n",
    "        result *= x\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.2383869537539137e-13 Это тест\n",
      "2.7593126135257213e-19 Это необычный текст\n",
      "0.0 Это эпико-концентрированный тест\n"
     ]
    }
   ],
   "source": [
    "tests = ['Это тест', \n",
    "         'Это необычный текст',\n",
    "         'Это эпико-концентрированный тест']\n",
    "\n",
    "for test in tests:\n",
    "    print(Pwords(tokens(test)), test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ВОУ—кажется, присвоить последнюю вероятность 0, неправильно; Она просто должна быть маленькой. К этому вернемся попозже. Ну а другие вероятности кажутся +- адекватными."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Задача: Разбиение слов на сегменты***\n",
    "Ситуации, когда слова пишутся слитно (по ошибке или нет)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Подход 2: Делаем одно разбиение - на первое слово и все остальное. Если предположить, что слова независимы, можно максимизировать вероятность первого слова + лучшего разбиения оставшихся букв.\n",
    "\n",
    "<code>\n",
    "assert segment('choosespain') == ['choose', 'spain']\n",
    "segment('choosespain') ==\n",
    "   max(Pwords(['c'] + segment('hoosespain')),\n",
    "       Pwords(['ch'] + segment('oosespain')),\n",
    "       Pwords(['cho'] + segment('osespain')),\n",
    "       Pwords(['choo'] + segment('sespain')),\n",
    "       ...\n",
    "       Pwords(['choosespain'] + segment('')))\n",
    "</code>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Чтобы сделать это хоть сколько-нибудь эффективным, нужно избежать слишком большого числа пересчетов оставшейся части слова. Это можно сделать или с помощью динамического программирования или с помощью мемоизации aka кэширования. Кроме того, для первого слова не обязательно брать все возможные варианты разбиений - мы можем установить максимальную длину. Какой она должна быть? Чуть большей, чем длина самого длинного слова, которое мы видели."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def memo(f):\n",
    "    \"Запомнить результаты исполнения функции f, чьи аргументы args должны быть хешируемыми.\"\n",
    "    cache = {}\n",
    "    def fmemo(*args):\n",
    "        if args not in cache:\n",
    "            cache[args] = f(*args)\n",
    "        return cache[args]\n",
    "    fmemo.cache = cache\n",
    "    return fmemo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "34"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(len(w) for w in COUNTS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def splits(text, start=0, L=20):\n",
    "    \"Вернуть список всех пар (a, b); start <= len(a) <= L.\"\n",
    "    return [(text[:i], text[i:]) \n",
    "            for i in range(start, min(len(text), L)+1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('', 'слово'), ('с', 'лово'), ('сл', 'ово'), ('сло', 'во'), ('слов', 'о'), ('слово', '')]\n",
      "[('дли', 'нныйтекст'), ('длин', 'ныйтекст'), ('длинн', 'ыйтекст'), ('длинны', 'йтекст'), ('длинный', 'текст')]\n"
     ]
    }
   ],
   "source": [
    "print(splits('слово'))\n",
    "print(splits('длинныйтекст', 3, 7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "@memo\n",
    "def segment(text):\n",
    "    \"Вернуть список слов, который является наиболее вероятной сегментацией нашего текста.\"\n",
    "    if not text: \n",
    "        return []\n",
    "    else:\n",
    "        candidates = ([first] + segment(rest) \n",
    "                      for (first, rest) in splits(text, 1))\n",
    "        return max(candidates, key=Pwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['фотоальбом']"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "segment('фотоальбом')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "decl = ('Сложноепредложениеэтосинтаксическаяконструкциясостоящаяиздвухиболеепростыхпредложенийсвязанныхпосмыслуиинтонационноспомощьюсочинительнойподчинительнойилибессоюзнойсвязи.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['С', 'л', 'о', 'ж', 'н', 'о', 'е', 'п', 'р', 'е', 'д', 'л', 'о', 'ж', 'е', 'н', 'и', 'е', 'э', 'т', 'о', 'с', 'и', 'н', 'т', 'а', 'к', 'с', 'и', 'ч', 'е', 'с', 'к', 'а', 'я', 'к', 'о', 'н', 'с', 'т', 'р', 'у', 'к', 'ц', 'и', 'я', 'с', 'о', 'с', 'т', 'о', 'я', 'щ', 'а', 'я', 'и', 'з', 'д', 'в', 'у', 'х', 'и', 'б', 'о', 'л', 'е', 'е', 'п', 'р', 'о', 'с', 'т', 'ы', 'х', 'п', 'р', 'е', 'д', 'л', 'о', 'ж', 'е', 'н', 'и', 'й', 'с', 'в', 'я', 'з', 'а', 'н', 'н', 'ы', 'х', 'п', 'о', 'с', 'м', 'ы', 'с', 'л', 'у', 'и', 'и', 'н', 'т', 'о', 'н', 'а', 'ц', 'и', 'о', 'н', 'н', 'о', 'с', 'п', 'о', 'м', 'о', 'щ', 'ь', 'ю', 'с', 'о', 'ч', 'и', 'н', 'и', 'т', 'е', 'л', 'ь', 'н', 'о', 'й', 'п', 'о', 'д', 'ч', 'и', 'н', 'и', 'т', 'е', 'л', 'ь', 'н', 'о', 'й', 'и', 'л', 'и', 'б', 'е', 'с', 'с', 'о', 'ю', 'з', 'н', 'о', 'й', 'с', 'в', 'я', 'з', 'и', '.']\n"
     ]
    }
   ],
   "source": [
    "print(segment(decl))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Насколько дорого превращать одно слово в другое?***\n",
    "\n",
    "Динамическое программирование позволяет разбить задачу на подзадачи, решив которые можно скомпоновать финальное решение. Мы будем пытаться превратить строку source[0..i] в строку target[0..j], мы сосчитаем все возможные комбинации подстрок substrings[i, j] и рассчитаем их edit_distance до нашей исходной. Мы будем сохранять результаты в таблицу и переиспользовать их для расчета дальнейших изменений.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def min_edit_distance(source, target, ins_cost = 1, del_cost = 1, rep_cost = 2):\n",
    "    '''\n",
    "    Input: \n",
    "        source: строка-исходник\n",
    "        target: строка, в которую мы должны исходник превратить\n",
    "        ins_cost: цена вставки\n",
    "        del_cost: цена удаления\n",
    "        rep_cost: цена замены буквы\n",
    "    Output:\n",
    "        D: матрица размера len(source)+1 на len(target)+1 содержащая минимальные расстояния edit_distance\n",
    "        med: минимальное расстояние edit_distance (med), необходимое, \n",
    "        чтобы превратить строку source в строку target\n",
    "    '''\n",
    "    # стоимость удаления и вставки = 1\n",
    "    m = len(source)\n",
    "    n = len(target)\n",
    "\n",
    "    # Заткнем нашу матрицу нулями\n",
    "    D = np.zeros((m+1, n+1), dtype=int) \n",
    "    \n",
    "    # Заполним первую колонку\n",
    "    for row in range(1,m+1): \n",
    "        D[row,0] = D[row-1,0] + del_cost\n",
    "        \n",
    "    # Заполним первую строку\n",
    "    for col in range(1,n+1): \n",
    "        D[0,col] = D[0,col-1] + ins_cost\n",
    "        \n",
    "    # Теперь пойдем от 1 к m-той строке\n",
    "    for row in range(1,m+1): \n",
    "        \n",
    "        # итерируемся по колонкам от 1 до n\n",
    "        for col in range(1,n+1):\n",
    "            \n",
    "            # r_cost - стоимость замены\n",
    "            r_cost = rep_cost\n",
    "            \n",
    "            # Совпадает ли буква исходного слова из предыдущей строки\n",
    "            # с буквой целевого слова из предыдущей колонки, \n",
    "            if source[row-1] == target[col-1]:\n",
    "                # Если они не нужны, то замена не нужна -> стоимость = 0\n",
    "                r_cost = 0\n",
    "                \n",
    "            # Обновляем значение ячейки на базе предыдущих значений \n",
    "            # Считаем D[i,j] как минимум из трех возможных стоимостей (как в формуле выше)\n",
    "            D[row,col] = min([D[row-1,col]+del_cost, D[row,col-1]+ins_cost, D[row-1,col-1]+r_cost])\n",
    "          \n",
    "    # установить edit_distance в значение из правого нижнего угла\n",
    "    med = D[m,n]\n",
    "    \n",
    "\n",
    "    return D, med"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Расстояние:  3 \n",
      "\n",
      "   #  к  и  т  ы\n",
      "#  0  1  2  3  4\n",
      "к  1  0  1  2  3\n",
      "р  2  1  2  3  4\n",
      "о  3  2  3  4  5\n",
      "т  4  3  4  3  4\n",
      "ы  5  4  5  4  3\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "source =  'кроты'\n",
    "target = 'киты'\n",
    "matrix, min_edits = min_edit_distance(source, target)\n",
    "\n",
    "print(\"Расстояние: \",min_edits, \"\\n\")\n",
    "\n",
    "idx = list('#' + source)\n",
    "cols = list('#' + target)\n",
    "df = pd.DataFrame(matrix, index=idx, columns= cols)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Нам мало миллионов слов в \"обучающей выборке\" давайте перейдем к МИЛЛИАРДАМ слов. Получив такой огромный объем информации, можно перейти к анализу пар последоваительных слов, не ожидая, что вероятности слишком часто будут обнуляться (представьте себе, сколько в языке может быть грамматически корректных сочетаний из двух слов). Мы вновь позаимствуем уже собранные данные у мистера Норвига. Лежат они на его сайте в формате \"word \\t count\" для отдельных слов и в формате \"word1 word2 \\t count\" для биграмм. Считаем их и упакуем в наши словари с вероятностями:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Валидация***\n",
    "\n",
    "До настоящего момента мы пытались интуитивно оценить результаты нашей работы. Тем не менее, никаких численных оценок качества мы пока не получили. Важно понимать, что без четких метрик слова \"плохо\"/\"хорошо\" не имеют никакого смысла. Более того - мы даже не можем четко ответить, было ли наше обновление модели в лучшую сторону или худшую. Обычно при построении неких прогностических моделей данные разбиваются на три части:\n",
    "\n",
    "**Обучающая выборка**: То, что мы использовали для создания модели исправления ошибок; У нас это был файл russian.txt file.\n",
    "**Тестовая выборка**: Набор данных, который можно использовать для оценки качества вашей модели по ходу разработки.\n",
    "**Валидационная выборка**: Набор данных, который мы используем для оценки работы программы после того как программа готова. Тестовая выборка для этого быть использована не может—Стоит разработчику посмотреть на результаты на тестовой выборке, она уже \"испорчена\". В принципе, программист может изменить программу так, чтобы она \"подгонялась\" под тестовую выборку, а это будет \"переобучением\". Вот почему нам нужен отдельный набор тестов, который рассматривается только после завершения разработки..\n",
    "\n",
    "Для нашей программы обучающая выборка - словарь слов. Сделаем валидационную выборку."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_segmenter(segmenter, tests):\n",
    "    \"Оценка сегментатора на тестовых данных; вывести на печать ошибки; вернуть долю верно разбитого.\"\n",
    "    return sum([test_one_segment(segmenter, test) \n",
    "               for test in tests]), len(tests)\n",
    "\n",
    "def test_one_segment(segmenter, test):\n",
    "    words = tokens(test)\n",
    "    result = segmenter(''.join(words))\n",
    "    correct = (result == words)\n",
    "    if not correct:\n",
    "        print('expected', words)\n",
    "        print('got     ', result) \n",
    "    return correct\n",
    "\n",
    "proverbs = (\"\"\"тут какой-то тест\"\"\"\n",
    "  .splitlines())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "expected ['тут', 'какой', 'то', 'тест']\n",
      "got      ['тут', 'какойто', 'тест']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0, 1)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_segmenter(segment, proverbs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Место для анекдота про Лапласа**\n",
    "\n",
    "Однажды французского математика Лапласа спросили: \"Какова вероятность того, что Солнце завтра взойдет?\". Из данных, что оно из  ближайших дней взошло n раз следует оценка максимального правдоподобия n/n = 1. Но Лапласу хотелось чуть сбалансировать оценку на шанс того, что завтра Солнце может и не взойти, поэтому он дал оценку (n+1)/(n+2).\n",
    "\n",
    "То, что мы знаем, ограничено, а то, чего мы не знаем,-бесконечно\n",
    "— Пьер Симон Лаплас, 1749-1827"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pdist_additive_smoothed(counter, c=1):\n",
    "    \"\"\"Вероятность слова, при условии данных из Counter'a.\n",
    "    добавляем c к частоте каждого слова + слово 'unknown'.\"\"\"\n",
    "    N = sum(list(counter.values()))          # суммарное кол-во слов\n",
    "    Nplus = N + c * (len(counter) + 1) # кол-во слов + сглаживание\n",
    "    return lambda word: (counter[word] + c) / Nplus \n",
    "\n",
    "P1w = pdist_additive_smoothed(COUNTS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь еще одна проблема ... у нас появились незнакомые слова с ненулевой вероятностью. А что если 10-12 - приемлемая вероятность для слов нашего текста: то есть, если я читаю новый текст, вероятность того, что следующее слово мне незнакомо, может быть порядка 10-12. Но если я случайно генерирую 20-буквенный последовательности, вероятность того, что одна из них будет реальным словом намного меньше чем 10-12.\n",
    "\n",
    "У нас две проблемы:\n",
    "\n",
    "Во-первых, у нас нет четкой модели для неизвестных слов. Мы говорим \"неизвестное слово\", но не различаем более вероятные неизвестные слова и менее вероятные неизвестные слова. Ну, например, вероятнее ли 8-буквенное неизвестное слово чем 20-буквенное неизвестное слово?\n",
    "\n",
    "Во-вторых, мы не берем в расчет информацию из частей неизвестных слов. Например, \"unglobulate\" явно должно быть более вероятным чем \"zxfkogultae\".\n",
    "\n",
    "Для нашего следующего подхода мы используем идеи метода Гуда - Тьюринга. Он оценивает вероятности слов, не встретившихся в нашем Counter'е, на основании вероятностей слов, встретившихся единожды (Можно туда же подключить вероятности для встретившихся 2 раза и т.д.).\n",
    "\n",
    "Итак, сколько слов встретилось 1 раз в COUNTS? (В COUNTS1 ни одного такого слова нет.) И какие длины у этих слов? Давайте посмотрим:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(10, 202295),\n",
       " (11, 199782),\n",
       " (12, 180391),\n",
       " (9, 177565),\n",
       " (13, 143641),\n",
       " (8, 141456),\n",
       " (14, 105857),\n",
       " (7, 94808),\n",
       " (15, 73043),\n",
       " (6, 52919),\n",
       " (16, 46367),\n",
       " (17, 28248),\n",
       " (5, 25639),\n",
       " (18, 16265),\n",
       " (19, 9203),\n",
       " (4, 9063),\n",
       " (20, 4818),\n",
       " (21, 2639),\n",
       " (3, 2412),\n",
       " (22, 1320),\n",
       " (23, 515),\n",
       " (2, 345),\n",
       " (24, 270),\n",
       " (25, 127),\n",
       " (26, 30),\n",
       " (27, 24),\n",
       " (1, 13),\n",
       " (28, 7),\n",
       " (29, 1),\n",
       " (34, 1)]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "singletons = (w for w in COUNTS if COUNTS[w] == 1)\n",
    "lengths = list(map(len, singletons))\n",
    "Counter(lengths).most_common()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([3.58000e+02, 2.41200e+03, 9.06300e+03, 2.56390e+04, 5.29190e+04,\n",
       "        9.48080e+04, 1.41456e+05, 1.77565e+05, 2.02295e+05, 1.99782e+05,\n",
       "        3.24032e+05, 1.05857e+05, 7.30430e+04, 4.63670e+04, 2.82480e+04,\n",
       "        1.62650e+04, 9.20300e+03, 4.81800e+03, 2.63900e+03, 1.32000e+03,\n",
       "        7.85000e+02, 1.27000e+02, 3.00000e+01, 2.40000e+01, 7.00000e+00,\n",
       "        1.00000e+00, 0.00000e+00, 0.00000e+00, 0.00000e+00, 1.00000e+00]),\n",
       " array([ 1. ,  2.1,  3.2,  4.3,  5.4,  6.5,  7.6,  8.7,  9.8, 10.9, 12. ,\n",
       "        13.1, 14.2, 15.3, 16.4, 17.5, 18.6, 19.7, 20.8, 21.9, 23. , 24.1,\n",
       "        25.2, 26.3, 27.4, 28.5, 29.6, 30.7, 31.8, 32.9, 34. ]),\n",
       " <BarContainer object of 30 artists>)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "matplotlib.pyplot.hist(lengths, bins=len(set(lengths)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Длины таких слов распределены похоже на нормальное распределение :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pdist_good_turing_hack(counter, onecounter, base=1/26., prior=1e-8):\n",
    "    \"\"\"Вероятность слова при условии данных из счетчика.\n",
    "    Для неизвестных слов, смотрим на слова, встретившиеся единожды из onecounter, \n",
    "    вероятность выбираем, основываясь на длине.\n",
    "    Воспользуемся идеей метода Гуда-Тьюринга(полностью мы его здесь не реализуем).\n",
    "    prior -добавочный фактор, который сделает неизвестные слова менее вероятными.\n",
    "    base -то, насколько мы уменьшаем вероятность за длину слова больше максимального.\"\"\"\n",
    "    N = sum(list(counter.values()))\n",
    "    N2 = sum(list(onecounter.values()))\n",
    "    lengths = list(map(len, [w for w in onecounter if onecounter[w] == 1]))\n",
    "    ones = Counter(lengths)\n",
    "    longest = max(ones)\n",
    "    return (lambda word: \n",
    "            counter[word] / N if (word in counter) \n",
    "            else prior * (ones[len(word)] / N2 or \n",
    "                          ones[longest] / N2 * base ** (len(word)-longest)))\n",
    "#Переопределим P1w\n",
    "P1w = pdist_good_turing_hack(COUNTS, COUNTS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['к', 'а', 'к', 'о', 'й', '-', 'т', 'о', ' ', 'сегмент']"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "segment.cache.clear()\n",
    "segment('какой-то сегмент')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Задача: Что если слово находится очень далеко по edit_distance, но звучит точно так же?***\n",
    "Часто можно встретить ошибки в текстах, вызванные неграмотным написанием слов. Особенно часто это происходит в случае иностранных фамилий или транслитерированной терминологии. Обычно в таких случаях в пример приводят написание фамилии"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для такого случая можно использовать следующую методологию. Давайте привлечем лингвистов и составим правила, которые одинаково звучащим словам будут ставить в соответствие один и тот же код. Допустим, с помощью лингвистов мы такой алгоритм придумали. Тогда дальнейшие наши действия таковы:\n",
    "\n",
    "1) Сделать словарь с вероятностями слов (как мы делали из мешка слов)\n",
    "\n",
    "2) Сделать словарь соответствий код слова -> слово (с помощью того самого алгоритма от лингвистов). \n",
    "    Если есть в списке есть слова с одинаковым кодом, выбирать будем наиболее частое слово.\n",
    "\n",
    "3) Сделаем аналогичный edit_distance алгоритм на множестве кодов слов\n",
    "\n",
    "4) Найдя соответствующую замену для слова в виде его кода, восстановим слово с помощью словаря из пункта 2\n",
    "\n",
    "Алгоритм, про который мы поговорим, называется Double Metaphone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "import metaphone\n",
    "from metaphone import doublemetaphone"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Алгоритм возвращает кортеж из двух возможных фонетических кодов слова. Правило такое:\n",
    "\n",
    " (Primary Key = Primary Key) = Идеальное совпадение\n",
    " \n",
    " (Secondary Key = Primary Key) = Совпадение\n",
    " \n",
    " (Primary Key = Secondary Key) = Совпадение\n",
    " \n",
    " (Alternate Key = Alternate Key) = Совпадение +-\n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# THE END"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "У Норвига в статье код занимает 21 строчку: \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re, collections\n",
    "def words(text):\n",
    "    return re.findall('[a-z]+', text.lower())\n",
    "def train(features):\n",
    "    model = collections.defaultdict(lambda: 1)\n",
    "    for f in features:\n",
    "        model[f] += 1\n",
    "    return model\n",
    "NWORDS = train(words(file('big.txt').read()))\n",
    "alphabet = 'abcdefghijklmnopqrstuvwxyz'\n",
    "def edits1(word):\n",
    "    n = len(word)\n",
    "    return set( [word[0:i]+word[i+1:] for i in range(n)] +                      # deletion\n",
    "                [word[0:i]+word[i+1]+word[i]+word[i+2:] for i in range(n-1)] +   # transposition\n",
    "                [word[0:i]+c+word[i+1:] for i in range(n) for c in alphabet] +    # alteration\n",
    "                [word[0:i]+c+word[i:] for i in range(n+1) for c in alphabet])    # insertion\n",
    "def known_edits2(word):\n",
    "    return set(e2 for e1 in edits1(word) for e2 in edits1(e1) if e2 in NWORDS)\n",
    "def known(words): return set(w for w in words if w in NWORDS)\n",
    "def correct(word):\n",
    "    candidates = known([word]) or known(edits1(word)) or known_edits2(word) or [word]\n",
    "    return max(candidates, key=lambda w: NWORDS[w])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "возможно так переделано для русского:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re, collections\n",
    "def words(text):\n",
    "    return re.findall('[а-ё]+', text.lower())\n",
    "def train(features):\n",
    "    model = collections.defaultdict(lambda: 1)\n",
    "    for f in features:\n",
    "        model[f] += 1\n",
    "    return model\n",
    "    \n",
    "with codecs.open('russian.txt', 'r', encoding = 'windows 1251') as file:\n",
    "    TEXT = file.read().replace('\\n', ' ')\n",
    "\n",
    "NWORDS = train(words(TEXT))\n",
    "alphabet = 'абвгдеёжзийклмнопрстуфхцчшщъыьэюя'\n",
    "def edits1(word):\n",
    "    n = len(word)\n",
    "    return set( [word[0:i]+word[i+1:] for i in range(n)] +                      # deletion\n",
    "                [word[0:i]+word[i+1]+word[i]+word[i+2:] for i in range(n-1)] +   # transposition\n",
    "                [word[0:i]+c+word[i+1:] for i in range(n) for c in alphabet] +    # alteration\n",
    "                [word[0:i]+c+word[i:] for i in range(n+1) for c in alphabet])    # insertion\n",
    "def known_edits2(word):\n",
    "    return set(e2 for e1 in edits1(word) for e2 in edits1(e1) if e2 in NWORDS)\n",
    "def known(words): return set(w for w in words if w in NWORDS)\n",
    "def correct(word):\n",
    "    candidates = known([word]) or known(edits1(word)) or known_edits2(word) or [word]\n",
    "    return max(candidates, key=lambda w: NWORDS[w])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Использовать этот код нужно следующим образом –\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Ана не хочит в шкалу'"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correct('Ана не хочит в шклу')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "87852089012c4b81b9af9dc676d2b889d7d3b7bd761d748065844c07d5d6aa6d"
  },
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
