{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Автокорректор ошибок на Python**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "%pylab is deprecated, use %matplotlib inline and import the required libraries.\n",
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "# Импортируй и властвуй\n",
    "%pylab inline\n",
    "import re\n",
    "import math\n",
    "import string\n",
    "from collections import Counter\n",
    "import requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для начала немного теории. \n",
    "Пусть дано слово, будем пытаться отыскать слово, в котором с наибольшей вероятностью исправлены допущенные ошибки (если ошибок нет, то таким словом будет данное). Разумеется, мы не сможем гарантировать 100% исправления всех ошибок. (Например, если нам дано слово «пак», то правильным будет слово «паз» или «парк» ?), именно поэтому мы используем вероятностный (или другими словами стохастический) подход. \n",
    "Будем говорить, что мы пытаемся выбрать такое слово c из всех возможных слов-исправлений, что вероятность появления именно слова c при данном слове w будет максимальна:\n",
    "\n",
    "argmaxc P(c|w)\n",
    "\n",
    "\n",
    "Согласно теореме Байеса - выражение, записанное выше, эквивалентно следующему выражению:\n",
    "\n",
    "***argmaxc P(w|c) P(c) / P(w)***\n",
    "\n",
    "Поскольку P(w) одинакова для всех c мы можем отбросить P(w), что даст нам:\n",
    "\n",
    "***argmaxc P(w|c) P(c)***\n",
    "\n",
    "\n",
    "В этом выражении присутствуют три части.  \n",
    "Справа налево:\n",
    "\n",
    "***P(c)*** – вероятность появления слова c (частотность употребления c). Эта вероятность обусловлена самим языком (точнее моделью языка). Иначе говоря, P(c) определяет как часто c встречается в текстах на русском языке. P(«превед») будет достаточно высока, тогда как P(«благоденствовать») будет меньше, а P(«ыгввыцшы») будет около нуля.\n",
    "\n",
    "***P(w|c)*** – вероятность того, что автор опечатался и написал w, хотя имел в виду c. По сути дела эта вероятность обусловлена частотностью тех или иных ошибок в языке (и называется моделью ошибок языка).\n",
    "\n",
    "***argmaxc*** – оператор, перебирающий все возможные c в поиске наиболее (вероятнее всего) подходящего из них (т.е. данный оператор ищет такое допустимое c, которе максимизирует условную вероятность появления w при данном c).\n",
    "\n",
    "Может возникнуть очевидный вопрос – зачем мы преобразовали простое выражение «argmaxc P(c|w)» с помощью какого-то Байеса в более сложное выражение, в котором используются аж две языковые модели, вместо одной? Дело в том, что P(c|w) учитывает в себе сразу обе языковых модели, поэтому очевидно, что проще выделить эти модели и работать с ними по отдельности. Предположим у нас есть слово с опечаткой – «езать», это может быть как «ехать», так и «резать». Для какого из исправлений P(c|w) будет максимально ? Оба исправления имеют примерно одинаковую частотность в русском языке. Хорошо допустим «х» и «з» близко расположены в русской раскладке клавиатуры и это повышает вероятность варианта «ехать», но это не повод, чтоб отбрасывать «резать», ведь «е» и «р» тоже близки. Поэтому лучше не рассматривать P(c|w) как единую величину, ибо нам приходится учитывать и частность исправления c и вероятность исправления c для данной опечатки в w. Удобнее работать с этими двумя вероятностями по отдельности."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Начнем с P(c)**. \n",
    "\n",
    "Мы читаем большой текстовый файл, words.txt, в котором записано много слов из таких произведений как: \"Тихий Дон\", \"Преступление и наказание\", \"Котлован\", \"Каштанка\", \"Капитанская дочка\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "После этого мы извлекаем отдельные слова из файла \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import codecs\n",
    "with codecs.open('words.txt', 'r', encoding = 'utf-8') as file:\n",
    "    TEXT = file.read().replace('\\n', ' ') # для текста, в котором слова разделены '\\n'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokens(text):\n",
    "    \"\"\"Возвращает список токенов (подряд идущих буквенных последовательностей) в тексте. \n",
    "       Текст при этом приводится к нижнему регистру.\"\"\"\n",
    "    return re.findall(r'[а-ё]+', text.lower()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "194976\n",
      "37223\n"
     ]
    }
   ],
   "source": [
    "WORDS = tokens(TEXT)\n",
    "print(len(WORDS))\n",
    "print(len(set(WORDS)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "print('между' in WORDS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Хочу заметить, что сейчас слова появляются в нашем списке в том порядке, как они располагались в файле"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Модель: Мешок слов (aka Bag of Words)***\n",
    "\n",
    "Мы создали список *WORDS* - список слов в том порядке, как они следуют в *TEXT*. Мы можем использовать этот список в качестве порождающей модели (generative model) текста. Язык - очень сложная штука и мы создаем крайне упрощенную модель языка, которая может ухватить часть этой сложной структуры. \n",
    "\n",
    "В модели мешка слов , мы полностью игнорируем порядок слов, зато соблюдаем их частоту. Представить это можно себе так: вы берете все слова текста и забрасываете их в мешок. Теперь, если вы хотите сгенерировать предложение с помощью этого мешка, вы просто трясете его(слова там перемешиваются) и достаете указанное количество слов по одному (мешок непрозрачный, так что слоа вы достаете наугад). Почти наверное полученное предложение будет грамматически некорректным, но слова в этом предложении будут в +- правильной пропорции (более частые будут встречаться чаще, более редкие - реже). Вот функция, которая сэмплирует(от англ. sample) предложение из n слов с помощью нашего мешка:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "def sample(bag, n=10):\n",
    "    \"Выборка случайного предложения из n слов из модели, описанной мешком слов.\"\n",
    "    return ' '.join(random.choice(bag) for _ in range(n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'как глазах бога взглянула бездействии шагов бы у убедить щеках'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample(WORDS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Другое представление мешка слов - Counter. Это словарь, состоящий из пар {'слово': кол-во вхождений слова в текст}. Например,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'между': 1,\n",
       "         'нами': 1,\n",
       "         'провода': 1,\n",
       "         'города': 1,\n",
       "         'да': 6,\n",
       "         'я': 1,\n",
       "         'сказал': 1,\n",
       "         'иди': 1,\n",
       "         'сюда': 1,\n",
       "         'и': 1,\n",
       "         'ты': 1,\n",
       "         'сказала': 1})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(tokens('Между нами провода, Города да да да. Я сказал иди сюда, И ты сказала: «Да, да, да..»'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Counter очень похож на словарь из Python - тип dict , но у него есть ряд дополнительных методов. Давайте завернем в Counter наш список слов WORDS и посмотрим, что получится:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('и', 8683), ('в', 4641), ('не', 3610), ('на', 3290), ('что', 2781), ('с', 2587), ('он', 2565), ('я', 2090), ('а', 2077), ('то', 1677)]\n"
     ]
    }
   ],
   "source": [
    "COUNTS = Counter(WORDS)\n",
    "\n",
    "print(COUNTS.most_common(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9 самые\n",
      "5 редкие\n",
      "98 слова\n",
      "0 крыжить\n",
      "0 михрютка\n",
      "2 драдедамовый\n"
     ]
    }
   ],
   "source": [
    "for w in tokens('самые редкие слова: Крыжить, Михрютка, Драдедамовый '):\n",
    "    print(COUNTS[w], w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В 1935, лингвист Джордж Ципф отметил, что в любом большом тексте n-тое наиболее часто встречающееся слово появляется с частотой ~ 1/n от частоты наиболее часто встречающегося слова. Это наблюдение получило название Закона Ципфа, несмотря на то, что Феликс Ауэрбах заметил это еще в 1913 году. Если нарисовать частоты слов, начиная от самого часто встречающегося, на log-log-графике, они должны приблизительно следовать прямой линии, если закон Ципфа верен. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x29d611f0c70>]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "M = COUNTS['и'] # вместо \"и\" самое часто встречающееся слово\n",
    "yscale('log') \n",
    "xscale('log')\n",
    "title('Частота n-того наиболее частого слова и линия 1/n.')\n",
    "plot([c for (w, c) in COUNTS.most_common()])\n",
    "plot([M/i for i in range(1, len(COUNTS)+1)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Задача: Проверка Правописания***\n",
    "\n",
    "Для данного слова w нужно найти наиболее вероятную правку *c = correct(w).*\n",
    "\n",
    "**Подход:** Найти все кандидаты c, достаточно близкие к w. Выбрать наиболее вероятный из них.\n",
    "\n",
    "Осталось понять, что такое близкие и наиболее вероятный.\n",
    "\n",
    "Применим наивный подход: всегда будем брать более близкое слово, если проверки на близость недостаточно, берем слово с максимальной частотой из WORDS. \n",
    "Сейчас мы будем измерять близость с помощью расстояния Левенштейна: минимального необходимого количества удалений, перестановок, вставок, и замен символов, необходимых чтобы одно слово превратить в другое. Конечно же это не единственный возможный подход. Тогда остается определить функцию *correct(w):*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correct(word):\n",
    "    \"Поиск лучшего исправления ошибки для данного слова.\"\n",
    "    # предрассчитать edit_distance==0, затем 1, затем 2; в противном случае оставить слово \"как есть\".\n",
    "    candidates = (known(edits0(word)) or \n",
    "                  known(edits1(word)) or \n",
    "                  known(edits2(word)) or \n",
    "                  [word])\n",
    "    return max(candidates, key=COUNTS.get)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Функции known и edits0 простые; функция edits2 iлегко получается из функции edits1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def known(words):\n",
    "    \"Вернуть подмножество слов, которое есть в нашем словаре.\"\n",
    "    return {w for w in words if w in COUNTS}\n",
    "\n",
    "def edits0(word): \n",
    "    \"Вернуть все строки, которые находятся на edit_distance == 0 от word (т.е., просто само слово).\"\n",
    "    return {word}\n",
    "\n",
    "def edits2(word):\n",
    "    \"Вернуть все строки, которые находятся на edit_distance == 2 от word.\"\n",
    "    return {e2 for e1 in edits1(word) for e2 in edits1(e1)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def edits1(word):\n",
    "    \"Возвращает список всех строк на расстоянии edit_distance == 1 от word.\"\n",
    "    pairs      = splits(word)\n",
    "    deletes    = [a+b[1:]           for (a, b) in pairs if b]\n",
    "    transposes = [a+b[1]+b[0]+b[2:] for (a, b) in pairs if len(b) > 1]\n",
    "    replaces   = [a+c+b[1:]         for (a, b) in pairs for c in alphabet if b]\n",
    "    inserts    = [a+c+b             for (a, b) in pairs for c in alphabet]\n",
    "    return set(deletes + transposes + replaces + inserts)\n",
    "\n",
    "def splits(word):\n",
    "    \"Возвращает список всех возможных разбиений слова на пару (a, b).\"\n",
    "    return [(word[:i], word[i:]) \n",
    "            for i in range(len(word)+1)]\n",
    "\n",
    "alphabet = 'абвгдеёжзийклмнопрстуфхцчшщъыьяюя'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('', 'ятвойотец'),\n",
       " ('я', 'твойотец'),\n",
       " ('ят', 'войотец'),\n",
       " ('ятв', 'ойотец'),\n",
       " ('ятво', 'йотец'),\n",
       " ('ятвой', 'отец'),\n",
       " ('ятвойо', 'тец'),\n",
       " ('ятвойот', 'ец'),\n",
       " ('ятвойоте', 'ц'),\n",
       " ('ятвойотец', '')]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "splits('ятвойотец')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "33"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(alphabet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'атец'}\n"
     ]
    }
   ],
   "source": [
    "print(edits0('атец'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'атеоц', 'атецы', 'атецу', 'апец', 'агтец', 'атес', 'атщц', 'ктец', 'аяец', 'атегц', 'атюц', 'атыц', 'атех', 'щатец', 'ачтец', 'тец', 'аоец', 'хтец', 'ацец', 'ватец', 'атёц', 'щтец', 'иатец', 'юатец', 'атеец', 'бтец', 'анец', 'атецъ', 'атецд', 'ьтец', 'атрец', 'ртец', 'атен', 'аътец', 'атц', 'атеы', 'атецн', 'автец', 'атецт', 'зтец', 'втец', 'атецз', 'атеа', 'ттец', 'аетц', 'атев', 'цатец', 'атецб', 'атеца', 'атее', 'атет', 'аитец', 'аутец', 'атжец', 'аптец', 'ате', 'атемц', 'атеьц', 'атецг', 'йатец', 'натец', 'ютец', 'ъатец', 'атдц', 'атеюц', 'атерц', 'атшец', 'атеч', 'катец', 'атецш', 'ащец', 'чатец', 'атеу', 'уатец', 'атед', 'ыатец', 'атуц', 'атеця', 'атлц', 'атоец', 'чтец', 'атгец', 'атеж', 'гтец', 'ратец', 'атеуц', 'атеце', 'атецм', 'атхец', 'атац', 'атхц', 'етец', 'атзц', 'афец', 'атея', 'еатец', 'ятец', 'атеп', 'затец', 'ауец', 'алтец', 'алец', 'атвец', 'атео', 'йтец', 'атей', 'аётец', 'атежц', 'аютец', 'атецж', 'аетец', 'атнц', 'лтец', 'татец', 'атецй', 'атбц', 'атем', 'атейц', 'атмец', 'ытец', 'атфц', 'атуец', 'ацтец', 'атебц', 'атечц', 'ащтец', 'акец', 'хатец', 'аштец', 'аттц', 'атяц', 'атбец', 'атезц', 'атеци', 'атею', 'аятец', 'атецч', 'атецо', 'атце', 'атеёц', 'атщец', 'атецк', 'атецц', 'азец', 'ачец', 'аьтец', 'аотец', 'антец', 'аец', 'латец', 'оатец', 'птец', 'атмц', 'атыец', 'атжц', 'атекц', 'атепц', 'атеё', 'ёатец', 'атецв', 'атзец', 'итец', 'мтец', 'азтец', 'атенц', 'ажтец', 'атоц', 'атешц', 'атсц', 'атег', 'шатец', 'атпец', 'атьец', 'атьц', 'актец', 'атшц', 'арец', 'сатец', 'амтец', 'атеыц', 'атеац', 'артец', 'ьатец', 'афтец', 'атефц', 'ътец', 'атйец', 'атаец', 'ахец', 'айтец', 'атрц', 'аттец', 'атцец', 'атек', 'атедц', 'ажец', 'атещ', 'гатец', 'атъец', 'атяец', 'атфец', 'аытец', 'атеб', 'атцц', 'атёец', 'атер', 'атеь', 'ател', 'атец', 'таец', 'атецщ', 'дтец', 'ётец', 'аьец', 'аткец', 'атсец', 'утец', 'атець', 'аюец', 'ашец', 'атецф', 'аатец', 'атиец', 'атехц', 'атецр', 'аёец', 'аиец', 'айец', 'атеъ', 'атещц', 'абтец', 'жтец', 'атйц', 'атецё', 'ахтец', 'атпц', 'атесц', 'атецх', 'аткц', 'батец', 'цтец', 'штец', 'атиц', 'яатец', 'авец', 'амец', 'атеш', 'атъц', 'атюец', 'атеф', 'атецс', 'стец', 'атчец', 'аъец', 'атнец', 'атеи', 'адец', 'фтец', 'атеъц', 'аеец', 'абец', 'асец', 'атез', 'атевц', 'атеиц', 'фатец', 'жатец', 'атетц', 'ателц', 'атдец', 'атгц', 'датец', 'ааец', 'агец', 'отец', 'аыец', 'матец', 'атецю', 'атвц', 'патец', 'атецп', 'астец', 'адтец', 'атлец', 'атеяц', 'атчц', 'нтец', 'атецл'}\n"
     ]
    }
   ],
   "source": [
    "print(edits1('атец'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36956\n"
     ]
    }
   ],
   "source": [
    "print(len(edits2('атец')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ана', 'ни', 'зочит', 'в', 'шшколу']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens('Ана ни зочит в шшколу.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['на', 'ни', 'почти', 'в', 'школу']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(map(correct, tokens('Ана ни зочит в шшколу.')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Теория: От счетчика слов к вероятностям последовательностей слов***\n",
    "\n",
    "Нам нужно научиться подсчитывать вероятности слов, P(w). Делать мы это будем с помощью функции pdist, которая на вход принимает Counter (наш мешок слов) и возвращает функцию, выполняющую роль вероятностного распределения на множестве всех возможных слов. В вероятностном распределении вероятность каждого слова лежит между 0 и 1, и сложение вероятностей всех слов дает 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pdist(counter):\n",
    "    \"Превращает частоты из Counter в вероятностное распределение.\"\n",
    "    N = sum(list(counter.values()))\n",
    "    return lambda x: counter[x]/N\n",
    "\n",
    "P = pdist(COUNTS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.008601058591826687 то\n",
      "0.0004462087641555884 мать\n",
      "0.0 пирогов\n",
      "0.0 напечет\n",
      "0.008601058591826687 то\n",
      "2.0515345478417856e-05 бабушка\n",
      "0.013268299688166748 с\n",
      "0.0 булочками\n",
      "1.5386509108813393e-05 приедет\n"
     ]
    }
   ],
   "source": [
    "for w in tokens('То мать пирогов напечет, то бабушка с булочками приедет'):\n",
    "    print(P(w), w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Итак, что же такое вероятность последовательности слов? Используем определение совместной вероятности:\n",
    "$P(w_1 ... w_n) = P(w_1)*P(w_2|w_1)*P(w_3|w_1w_2)...*...P(w_n|w_1...w_n-1)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Модель мешка слов подразумевает, что каждое слово из мешка достается независимо от других. Это дает нам упрощенную аппроксимацию:\n",
    "$P(w_1 ... w_n) = P(w_1)*P(w_2)*P(w_3)...*...P(w_n)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Известный статистик Джордж Бокс сказал Все модели неверны, но некоторые полезны."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как же нам посчитать $P(w_1 ... w_n)$? Мы будем использовать другое название, чтобы не обманывать себя, Pwords вместо P, и посчитаем ее как произведение индивидуальных вероятностей:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Pwords(words):\n",
    "    \"Вероятности слов, при условии, что они независимы.\"\n",
    "    return product(P(w) for w in words)\n",
    "\n",
    "def product(nums):\n",
    "    \"Перемножим числа.  (Это как `sum`, только с умножением.)\"\n",
    "    result = 1\n",
    "    for x in nums:\n",
    "        result *= x\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0 тест\n",
      "0.0003436320367634991 дом\n",
      "0.0 Крыжить\n"
     ]
    }
   ],
   "source": [
    "tests = ['тест', \n",
    "         'дом',\n",
    "         'Крыжить']\n",
    "\n",
    "for test in tests:\n",
    "    print(Pwords(tokens(test)), test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "кажется, присвоить последнюю вероятность 0, неправильно; Она просто должна быть маленькой. К этому вернемся попозже. Ну а другие вероятности кажутся +- адекватными."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Задача: Разбиение слов на сегменты***\n",
    "Ситуации, когда слова пишутся слитно (по ошибке или нет)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Подход 2: Делаем одно разбиение - на первое слово и все остальное. Если предположить, что слова независимы, можно максимизировать вероятность первого слова + лучшего разбиения оставшихся букв.\n",
    "\n",
    "<code>\n",
    "assert segment('фотоальбом') == ['фото', 'альбом']\n",
    "segment('choosespain') ==\n",
    "   max(Pwords(['ф'] + segment('отоальбом')),\n",
    "       Pwords(['фо'] + segment('тоальбом')),\n",
    "       Pwords(['фот'] + segment('оальбом')),\n",
    "       Pwords(['фото'] + segment('альбом')),\n",
    "       ...\n",
    "       Pwords(['фотоальбом'] + segment('')))\n",
    "</code>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Чтобы сделать это хоть сколько-нибудь эффективным, нужно избежать слишком большого числа пересчетов оставшейся части слова. Это можно сделать или с помощью динамического программирования или с помощью мемоизации aka кэширования. Кроме того, для первого слова не обязательно брать все возможные варианты разбиений - мы можем установить максимальную длину. Какой она должна быть? Чуть большей, чем длина самого длинного слова, которое мы видели."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def memo(f):\n",
    "    \"Запомнить результаты исполнения функции f, чьи аргументы args должны быть хешируемыми.\"\n",
    "    cache = {}\n",
    "    def fmemo(*args):\n",
    "        if args not in cache:\n",
    "            cache[args] = f(*args)\n",
    "        return cache[args]\n",
    "    fmemo.cache = cache\n",
    "    return fmemo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(len(w) for w in COUNTS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def splits(text, start=0, L=20):\n",
    "    \"Вернуть список всех пар (a, b); start <= len(a) <= L.\"\n",
    "    return [(text[:i], text[i:]) \n",
    "            for i in range(start, min(len(text), L)+1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('', 'слово'), ('с', 'лово'), ('сл', 'ово'), ('сло', 'во'), ('слов', 'о'), ('слово', '')]\n",
      "[('дли', 'нныйтекст'), ('длин', 'ныйтекст'), ('длинн', 'ыйтекст'), ('длинны', 'йтекст'), ('длинный', 'текст')]\n"
     ]
    }
   ],
   "source": [
    "print(splits('слово'))\n",
    "print(splits('длинныйтекст', 3, 7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "@memo\n",
    "def segment(text):\n",
    "    \"Вернуть список слов, который является наиболее вероятной сегментацией нашего текста.\"\n",
    "    if not text: \n",
    "        return []\n",
    "    else:\n",
    "        candidates = ([first] + segment(rest) \n",
    "                      for (first, rest) in splits(text, 1))\n",
    "        return max(candidates, key=Pwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['картина', 'маслом']"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "segment('картинамаслом')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "decl = ('Сложноепредложениеэтосинтаксическаяконструкциясостоящаяиздвухиболеепростыхпредложенийсвязанныхпосмыслуиинтонационноспомощьюсочинительнойподчинительнойилибессоюзнойсвязи.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['С', 'л', 'о', 'ж', 'н', 'о', 'е', 'п', 'р', 'е', 'д', 'л', 'о', 'ж', 'е', 'н', 'и', 'е', 'э', 'т', 'о', 'с', 'и', 'н', 'т', 'а', 'к', 'с', 'и', 'ч', 'е', 'с', 'к', 'а', 'я', 'к', 'о', 'н', 'с', 'т', 'р', 'у', 'к', 'ц', 'и', 'я', 'с', 'о', 'с', 'т', 'о', 'я', 'щ', 'а', 'я', 'и', 'з', 'д', 'в', 'у', 'х', 'и', 'б', 'о', 'л', 'е', 'е', 'п', 'р', 'о', 'с', 'т', 'ы', 'х', 'п', 'р', 'е', 'д', 'л', 'о', 'ж', 'е', 'н', 'и', 'й', 'с', 'в', 'я', 'з', 'а', 'н', 'н', 'ы', 'х', 'п', 'о', 'с', 'м', 'ы', 'с', 'л', 'у', 'и', 'и', 'н', 'т', 'о', 'н', 'а', 'ц', 'и', 'о', 'н', 'н', 'о', 'с', 'п', 'о', 'м', 'о', 'щ', 'ь', 'ю', 'с', 'о', 'ч', 'и', 'н', 'и', 'т', 'е', 'л', 'ь', 'н', 'о', 'й', 'п', 'о', 'д', 'ч', 'и', 'н', 'и', 'т', 'е', 'л', 'ь', 'н', 'о', 'й', 'и', 'л', 'и', 'б', 'е', 'с', 'с', 'о', 'ю', 'з', 'н', 'о', 'й', 'с', 'в', 'я', 'з', 'и', '.']\n"
     ]
    }
   ],
   "source": [
    "print(segment(decl))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Насколько дорого превращать одно слово в другое?***\n",
    "\n",
    "Динамическое программирование позволяет разбить задачу на подзадачи, решив которые можно скомпоновать финальное решение. Мы будем пытаться превратить строку $source[0..i]$ в строку $target[0..j]$, мы сосчитаем все возможные комбинации подстрок $substrings[i, j]$ и рассчитаем их *edit_distance* до нашей исходной. Мы будем сохранять результаты в таблицу и переиспользовать их для расчета дальнейших изменений.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def min_edit_distance(source, target, ins_cost = 1, del_cost = 1, rep_cost = 2):\n",
    "    '''\n",
    "    Input: \n",
    "        source: строка-исходник\n",
    "        target: строка, в которую мы должны исходник превратить\n",
    "        ins_cost: цена вставки\n",
    "        del_cost: цена удаления\n",
    "        rep_cost: цена замены буквы\n",
    "    Output:\n",
    "        D: матрица размера len(source)+1 на len(target)+1 содержащая минимальные расстояния edit_distance\n",
    "        med: минимальное расстояние edit_distance (med), необходимое, \n",
    "        чтобы превратить строку source в строку target\n",
    "    '''\n",
    "    # стоимость удаления и вставки = 1\n",
    "    m = len(source)\n",
    "    n = len(target)\n",
    "\n",
    "    # Заткнем нашу матрицу нулями\n",
    "    D = np.zeros((m+1, n+1), dtype=int) \n",
    "    \n",
    "    # Заполним первую колонку\n",
    "    for row in range(1,m+1): \n",
    "        D[row,0] = D[row-1,0] + del_cost\n",
    "        \n",
    "    # Заполним первую строку\n",
    "    for col in range(1,n+1): \n",
    "        D[0,col] = D[0,col-1] + ins_cost\n",
    "        \n",
    "    # Теперь пойдем от 1 к m-той строке\n",
    "    for row in range(1,m+1): \n",
    "        \n",
    "        # итерируемся по колонкам от 1 до n\n",
    "        for col in range(1,n+1):\n",
    "            \n",
    "            # r_cost - стоимость замены\n",
    "            r_cost = rep_cost\n",
    "            \n",
    "            # Совпадает ли буква исходного слова из предыдущей строки\n",
    "            # с буквой целевого слова из предыдущей колонки, \n",
    "            if source[row-1] == target[col-1]:\n",
    "                # Если они не нужны, то замена не нужна -> стоимость = 0\n",
    "                r_cost = 0\n",
    "                \n",
    "            # Обновляем значение ячейки на базе предыдущих значений \n",
    "            # Считаем D[i,j] как минимум из трех возможных стоимостей (как в формуле выше)\n",
    "            D[row,col] = min([D[row-1,col]+del_cost, D[row,col-1]+ins_cost, D[row-1,col-1]+r_cost])\n",
    "          \n",
    "    # установить edit_distance в значение из правого нижнего угла\n",
    "    med = D[m,n]\n",
    "    \n",
    "\n",
    "    return D, med"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Расстояние:  3 \n",
      "\n",
      "   #  к  и  т  ы\n",
      "#  0  1  2  3  4\n",
      "к  1  0  1  2  3\n",
      "р  2  1  2  3  4\n",
      "о  3  2  3  4  5\n",
      "т  4  3  4  3  4\n",
      "ы  5  4  5  4  3\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "source =  'кроты'\n",
    "target = 'киты'\n",
    "matrix, min_edits = min_edit_distance(source, target)\n",
    "\n",
    "print(\"Расстояние: \",min_edits, \"\\n\")\n",
    "\n",
    "idx = list('#' + source)\n",
    "cols = list('#' + target)\n",
    "df = pd.DataFrame(matrix, index=idx, columns= cols)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Нам мало миллионов слов в \"обучающей выборке\" давайте перейдем к МИЛЛИАРДАМ слов. Получив такой огромный объем информации, можно перейти к анализу пар последоваительных слов, не ожидая, что вероятности слишком часто будут обнуляться (представьте себе, сколько в языке может быть грамматически корректных сочетаний из двух слов). Мы вновь позаимствуем уже собранные данные у мистера Норвига. Лежат они на его сайте в формате \"word \\t count\" для отдельных слов и в формате \"word1 word2 \\t count\" для биграмм. Считаем их и упакуем в наши словари с вероятностями:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_counts(text, sep='\\t'):\n",
    "    \"\"\"Возвращает Counter, полученный из пар ключ-значение, в каждой строке файла.\"\"\"\n",
    "    C = Counter()\n",
    "    for i in [l.split('\\t') for l in text.split('\\n')][:-1]:\n",
    "        key, count = i\n",
    "        C[key] = int(count)\n",
    "    return C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "COUNTS1 = список слов с частотой их встречания\n",
    "COUNTS2 = список слов с частотой их встречания\n",
    "\n",
    "P1w = pdist(COUNTS1)\n",
    "P2w = pdist(COUNTS2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(COUNTS1), sum(list(COUNTS1.values()))/1e9)\n",
    "print(len(COUNTS2), sum(list(COUNTS2.values()))/1e9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ужас! Сотни миллиардов. Но мы справились."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "COUNTS2.most_common(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Сегментация с помощью биграмм**\n",
    "Чуть менее неправильная аппроксимация:\n",
    "$P(w_1...w_n)=P(w_1)*P(w_2|w_1)*P(w_3|w_2)*...*...P(w_n|w_n-1)$\n",
    "Эта штука называется биграммной моделью. Представьте, что вы взяли текст, достали из него все возможные пары подряд идущих слов и положили каждую пару в мешок, промаркированный ПЕРВЫМ словом из пары. После этого, чтобы сгенерировать кусок текста, мы берем первое слово из исходного мешка слов , а каждое следующее слово вынимаем из соответствующего мешка биграмм.\n",
    "\n",
    "Начнем с определения вероятности текущего слова при условии данного предыдущего слова из Counter:\n",
    "\n",
    "Отмечу, что для английского языка биграммная модель будет выглядеть так:\n",
    "$P(w_1...w_n)=P(w_1)*P(w_2|w_1)*P(w_3|w_2)*...*...P(w_n|w_n-1)$ условная вероятность слова при условии предыдущего слова определяется так:\n",
    "$P(w_n|w_n-1)=P(w_n-1w_n)/P(w_n-1)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Pwords2(words, prev='<S>'):\n",
    "    \"Вероятность последовательности слов с помощью биграммной модели(при условии предыдущего слова).\"\n",
    "    return product(cPword(w, (prev if (i == 0) else words[i-1]) )\n",
    "                   for (i, w) in enumerate(words))\n",
    "\n",
    "# Перепишем Pwords на большой словарь P1w вместо Pword\n",
    "def Pwords(words):\n",
    "    \"Вероятности слов при условии их независимости.\"\n",
    "    return product(P1w(w) for w in words)\n",
    "\n",
    "def cPword(word, prev):\n",
    "    \"Условная вероятность слова при условии предыдущего.\"\n",
    "    bigram = prev + ' ' + word\n",
    "    if P2w(bigram) > 0 and P1w(prev) > 0:\n",
    "        return P2w(bigram) / P1w(prev)\n",
    "    else: # если что-то не встретилось, поставим среднее между P1w и 0\n",
    "        return P1w(word) / 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(Pwords(tokens('Он приехал в город')))\n",
    "print(Pwords2(tokens('Приехал он в город')))\n",
    "print(Pwords2(tokens('Город в он приехал')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Чтобы сделать segment2, скопируем segment, добавим в аргументы предыдущий токен, а вероятности будем считать с помощью Pwords2 вместо Pwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "@memo \n",
    "def segment2(text, prev='<S>'): \n",
    "    \"Возвращает наилучшее разбиение текста, используя статистику биграмм.\" \n",
    "    if not text: \n",
    "        return []\n",
    "    else:\n",
    "        candidates = ([first] + segment2(rest, first) \n",
    "                      for (first, rest) in splits(text, 1))\n",
    "        return max(candidates, key=lambda words: Pwords2(words, prev))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(segment2('фотоальбом'))\n",
    "print(segment2('поехалаонанет'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ну и что теперь? Биграммная модель вроде бы лучше, но не очень. Сотен миллиардов слов все равно может быть недостаточно. (Ну а почему бы не триллион слов?)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Валидация***\n",
    "\n",
    "До настоящего момента мы пытались интуитивно оценить результаты нашей работы. Тем не менее, никаких численных оценок качества мы пока не получили. Важно понимать, что без четких метрик слова \"плохо\"/\"хорошо\" не имеют никакого смысла. Более того - мы даже не можем четко ответить, было ли наше обновление модели в лучшую сторону или худшую. Обычно при построении неких прогностических моделей данные разбиваются на три части:\n",
    "\n",
    "**Обучающая выборка**: То, что мы использовали для создания модели исправления ошибок; У нас это был файл words.txt file.\n",
    "**Тестовая выборка**: Набор данных, который можно использовать для оценки качества вашей модели по ходу разработки.\n",
    "**Валидационная выборка**: Набор данных, который мы используем для оценки работы программы после того как программа готова. Тестовая выборка для этого быть использована не может—Стоит разработчику посмотреть на результаты на тестовой выборке, она уже \"испорчена\". В принципе, программист может изменить программу так, чтобы она \"подгонялась\" под тестовую выборку, а это будет \"переобучением\". Вот почему нам нужен отдельный набор тестов, который рассматривается только после завершения разработки..\n",
    "\n",
    "Для нашей программы обучающая выборка - словарь слов. Сделаем валидационную выборку."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_segmenter(segmenter, tests):\n",
    "    \"Оценка сегментатора на тестовых данных; вывести на печать ошибки; вернуть долю верно разбитого.\"\n",
    "    return sum([test_one_segment(segmenter, test) \n",
    "               for test in tests]), len(tests)\n",
    "\n",
    "def test_one_segment(segmenter, test):\n",
    "    words = tokens(test)\n",
    "    result = segmenter(''.join(words))\n",
    "    correct = (result == words)\n",
    "    if not correct:\n",
    "        print('expected', words)\n",
    "        print('got     ', result) \n",
    "    return correct\n",
    "\n",
    "proverbs = (\"\"\"тут какой-то тест\"\"\"\n",
    "  .splitlines())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_segmenter(segment, proverbs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Место для анекдота про Лапласа**\n",
    "\n",
    "Однажды французского математика Лапласа спросили: \"Какова вероятность того, что Солнце завтра взойдет?\". Из данных, что оно из  ближайших дней взошло n раз следует оценка максимального правдоподобия n/n = 1. Но Лапласу хотелось чуть сбалансировать оценку на шанс того, что завтра Солнце может и не взойти, поэтому он дал оценку (n+1)/(n+2).\n",
    "\n",
    "То, что мы знаем, ограничено, а то, чего мы не знаем,-бесконечно\n",
    "— Пьер Симон Лаплас, 1749-1827"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pdist_additive_smoothed(counter, c=1):\n",
    "    \"\"\"Вероятность слова, при условии данных из Counter'a.\n",
    "    добавляем c к частоте каждого слова + слово 'unknown'.\"\"\"\n",
    "    N = sum(list(counter.values()))          # суммарное кол-во слов\n",
    "    Nplus = N + c * (len(counter) + 1) # кол-во слов + сглаживание\n",
    "    return lambda word: (counter[word] + c) / Nplus \n",
    "\n",
    "P1w = pdist_additive_smoothed(COUNTS1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь еще одна проблема ... у нас появились незнакомые слова с ненулевой вероятностью. А что если 10-12 - приемлемая вероятность для слов нашего текста: то есть, если я читаю новый текст, вероятность того, что следующее слово мне незнакомо, может быть порядка 10-12. Но если я случайно генерирую 20-буквенный последовательности, вероятность того, что одна из них будет реальным словом намного меньше чем 10-12.\n",
    "\n",
    "У нас две проблемы:\n",
    "\n",
    "Во-первых, у нас нет четкой модели для неизвестных слов. Мы говорим \"неизвестное слово\", но не различаем более вероятные неизвестные слова и менее вероятные неизвестные слова. Ну, например, вероятнее ли 8-буквенное неизвестное слово чем 20-буквенное неизвестное слово?\n",
    "\n",
    "Во-вторых, мы не берем в расчет информацию из частей неизвестных слов. Например, \"unglobulate\" явно должно быть более вероятным чем \"zxfkogultae\".\n",
    "\n",
    "Для нашего следующего подхода мы используем идеи метода Гуда - Тьюринга. Он оценивает вероятности слов, не встретившихся в нашем Counter'е, на основании вероятностей слов, встретившихся единожды (Можно туда же подключить вероятности для встретившихся 2 раза и т.д.).\n",
    "\n",
    "Итак, сколько слов встретилось 1 раз в COUNTS? (В COUNTS1 ни одного такого слова нет.) И какие длины у этих слов? Давайте посмотрим:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(8, 3911),\n",
       " (7, 3755),\n",
       " (9, 3380),\n",
       " (10, 2663),\n",
       " (6, 2594),\n",
       " (11, 1995),\n",
       " (5, 1477),\n",
       " (12, 1204),\n",
       " (13, 614),\n",
       " (4, 521),\n",
       " (14, 327),\n",
       " (15, 146),\n",
       " (3, 143),\n",
       " (16, 76),\n",
       " (2, 36),\n",
       " (17, 29),\n",
       " (18, 18),\n",
       " (19, 4),\n",
       " (1, 3),\n",
       " (20, 2),\n",
       " (21, 1),\n",
       " (22, 1),\n",
       " (23, 1)]"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "singletons = (w for w in COUNTS if COUNTS[w] == 1)\n",
    "lengths = list(map(len, singletons))\n",
    "Counter(lengths).most_common()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "matplotlib.pyplot.hist(lengths, bins=len(set(lengths)))\n",
    "print(len(set(lengths)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Длины таких слов распределены похоже на нормальное распределение :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pdist_good_turing_hack(counter, onecounter, base=1/33., prior=1e-8):\n",
    "    \"\"\"Вероятность слова при условии данных из счетчика.\n",
    "    Для неизвестных слов, смотрим на слова, встретившиеся единожды из onecounter, \n",
    "    вероятность выбираем, основываясь на длине.\n",
    "    Воспользуемся идеей метода Гуда-Тьюринга(полностью мы его здесь не реализуем).\n",
    "    prior -добавочный фактор, который сделает неизвестные слова менее вероятными.\n",
    "    base -то, насколько мы уменьшаем вероятность за длину слова больше максимального.\"\"\"\n",
    "    N = sum(list(counter.values()))\n",
    "    N2 = sum(list(onecounter.values()))\n",
    "    lengths = list(map(len, [w for w in onecounter if onecounter[w] == 1]))\n",
    "    ones = Counter(lengths)\n",
    "    longest = max(ones)\n",
    "    return (lambda word: \n",
    "            counter[word] / N if (word in counter) \n",
    "            else prior * (ones[len(word)] / N2 or \n",
    "                          ones[longest] / N2 * base ** (len(word)-longest)))\n",
    "#Переопределим P1w\n",
    "P1w = pdist_good_turing_hack(COUNTS1, COUNTS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "segment.cache.clear()\n",
    "segment('какой-то сегмент')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Задача: Что если слово находится очень далеко по edit_distance, но звучит точно так же?***\n",
    "Часто можно встретить ошибки в текстах, вызванные неграмотным написанием слов. Особенно часто это происходит в случае иностранных фамилий или транслитерированной терминологии. Обычно в таких случаях в пример приводят написание фамилии"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для такого случая можно использовать следующую методологию. Давайте привлечем лингвистов и составим правила, которые одинаково звучащим словам будут ставить в соответствие один и тот же код. Допустим, с помощью лингвистов мы такой алгоритм придумали. Тогда дальнейшие наши действия таковы:\n",
    "\n",
    "1) Сделать словарь с вероятностями слов (как мы делали из мешка слов)\n",
    "\n",
    "2) Сделать словарь соответствий код слова -> слово (с помощью того самого алгоритма от лингвистов). \n",
    "    Если есть в списке есть слова с одинаковым кодом, выбирать будем наиболее частое слово.\n",
    "\n",
    "3) Сделаем аналогичный edit_distance алгоритм на множестве кодов слов\n",
    "\n",
    "4) Найдя соответствующую замену для слова в виде его кода, восстановим слово с помощью словаря из пункта 2\n",
    "\n",
    "Алгоритм, про который мы поговорим, называется Double Metaphone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "import metaphone\n",
    "from metaphone import doublemetaphone"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Алгоритм возвращает кортеж из двух возможных фонетических кодов слова. Правило такое:\n",
    "\n",
    " (Primary Key = Primary Key) = Идеальное совпадение\n",
    " \n",
    " (Secondary Key = Primary Key) = Совпадение\n",
    " \n",
    " (Primary Key = Secondary Key) = Совпадение\n",
    " \n",
    " (Alternate Key = Alternate Key) = Совпадение +-\n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# THE END"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "У Норвига в статье код занимает 21 строчку: \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re, collections\n",
    "def words(text):\n",
    "    return re.findall('[а-ё]+', text.lower())\n",
    "def train(features):\n",
    "    model = collections.defaultdict(lambda: 1)\n",
    "    for f in features:\n",
    "        model[f] += 1\n",
    "    return model\n",
    "    \n",
    "with codecs.open('words.txt', 'r', encoding = 'utf-8') as file:\n",
    "    TEXT = file.read().replace('\\n', ' ')\n",
    "\n",
    "NWORDS = train(words(TEXT))\n",
    "alphabet = 'абвгдеёжзийклмнопрстуфхцчшщъыьэюя'\n",
    "def edits1(word):\n",
    "    n = len(word)\n",
    "    return set( [word[0:i]+word[i+1:] for i in range(n)] +                      # deletion\n",
    "                [word[0:i]+word[i+1]+word[i]+word[i+2:] for i in range(n-1)] +   # transposition\n",
    "                [word[0:i]+c+word[i+1:] for i in range(n) for c in alphabet] +    # alteration\n",
    "                [word[0:i]+c+word[i:] for i in range(n+1) for c in alphabet])    # insertion\n",
    "def known_edits2(word):\n",
    "    return set(e2 for e1 in edits1(word) for e2 in edits1(e1) if e2 in NWORDS)\n",
    "def known(words): return set(w for w in words if w in NWORDS)\n",
    "\n",
    "def correct(word):\n",
    "    candidates = known([word]) or known(edits1(word)) or known_edits2(word) or [word]\n",
    "    return max(candidates, key=lambda w: NWORDS[w])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Использовать этот код нужно следующим образом –\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "сонечка не хочет в шашку "
     ]
    }
   ],
   "source": [
    "to_correct = tokens('Саничка не хочит в шшклу')\n",
    "\n",
    "for word in to_correct:\n",
    "    print(correct(word), end = ' ') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "87852089012c4b81b9af9dc676d2b889d7d3b7bd761d748065844c07d5d6aa6d"
  },
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
