{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Автокорректор ошибок на Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Вначале немного теории.**\n",
    "\n",
    "Перед нами стояла следующая задача: изучить алгоритмы, основанных на сравнении слова со списком правильных слов. Запрограммировать эти алгоритмы.\n",
    "\n",
    "Для алгоритмов сравнения со словарём рекомендую использовать: Расстояние Жаккара и Расстояние Левенштейна."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 Часть. Сравнение расстояний Жаккара и Левенштейна.\n",
    "\n",
    "Создадим две рекомендательные системы правописания, которые смогут принимать входные данные пользователя и рекомендовать правильно написанное слово, одно при помощи расстояния Жаккара, а другое - Левенштейна."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import words\n",
    "from nltk.metrics.distance import (\n",
    "edit_distance,\n",
    "jaccard_distance,\n",
    ")\n",
    "from nltk.util import ngrams\n",
    "import pandas\n",
    "\n",
    "correct_spellings = #список правильных слов\n",
    "spellings_series = pandas.Series(correct_spellings) # проиндексированный список этих словv\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Метрика: расстояние Жаккара**\n",
    "\n",
    "Расстояние Жаккара является мерой того, насколько непохожи два набора сдлва, и может быть найдено как дополнение к индексу Жаккара (т. е. расстояние Жаккара = 100% - индекс Жаккара).\n",
    "\n",
    "Рассматриваемые слова будут итеративно сравниваться с каждым словом в spellings_series.\n",
    "\n",
    "Spellings создаст список возможных слов на основе первой буквы строки (предполагается, что первая буква напечатана правильно).\n",
    "\n",
    "Далее, Distances будет итеративно вычислять соответствующие расстояния Жаккара для слов в написании с помощью встроенной функции jaccard_distance.\n",
    "\n",
    "Наконец, closest даст результирующее слово с лучшим соответствием через функцию min на расстояниях. Затем это слово будет добавлено в список результатов, и этот список будет возвращен после завершения работы функции.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jaccard(entries, gram_number):\n",
    "\n",
    "outcomes = []\n",
    "for entry in entries: \n",
    "    for loopspellings = spellings_series[spellings_series.str.startswith(entry[0])]\n",
    "    distances = ((jaccard_distance(set(ngrams(entry, gram_number)), set(ngrams(word, gram_number))), word) for word in spellings)\n",
    "    closest = min(distances)\n",
    "    outcomes.append(closest[1])\n",
    "return outcomes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def JDreco(entries=['ево', 'чуство', 'юмара']):\n",
    " #finds the closest word based on jaccard distance\n",
    "    return jaccard(entries, 3)\n",
    "print(JDreco())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Метрика: расстояние Левенштейна**\n",
    "\n",
    "Этот метод оценивает, насколько непохожи две строки, на основе минимального количества операций, необходимых для преобразования одной строки в другую.\n",
    "\n",
    "Функция итеративно сравнивает записи со списком правильных слов и возвращает Расстояние Левенштейна Затем слово с наименьшим расстоянием будет считаться наиболее правильным словом."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def editreco(entries=['мома', 'мфла', 'рму']):\n",
    "\n",
    "outcomes = []\n",
    "for entry in entries:\n",
    "    distances = ((edit_distance(entry, word), word) for word in correct_spellings)\n",
    "    closest = min(distances)\n",
    "    outcomes.append(closest[1])\n",
    "return outcomes\n",
    "\n",
    "\n",
    "print(editreco())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь дававйте сравним результаты двух работ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "userinput = []\n",
    "for i in range(0,3):\n",
    "    word = input(\"Я лублю пильмени: \")\n",
    "    userinput.append(word)\n",
    "JDreco(userinput)\n",
    "editreco(userinput)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Место для анализа результатов**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2часть. Модель Bag of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "%pylab is deprecated, use %matplotlib inline and import the required libraries.\n",
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "# Импортируй и властвуй\n",
    "%pylab inline\n",
    "import re\n",
    "import math\n",
    "import string\n",
    "from collections import Counter\n",
    "import requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Прежде чем что-то делать со словами, надо эти слова откуда-то взять. Мы сами сделали свой текстовый файл, text.txt, в котором записано много слов из таких произведений как: \"Тихий Дон\", \"Преступление и наказание\", \"Котлован\", \"Каштанка\", \"Капитанская дочка\", а также *дополнить*. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'charmap' codec can't decode byte 0x98 in position 899: character maps to <undefined>",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\wequalwo\\source\\python\\Alter.ipynb Cell 6'\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/wequalwo/source/python/Alter.ipynb#ch0000005?line=0'>1</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mcodecs\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/wequalwo/source/python/Alter.ipynb#ch0000005?line=1'>2</a>\u001b[0m \u001b[39mwith\u001b[39;00m codecs\u001b[39m.\u001b[39mopen(\u001b[39m'\u001b[39m\u001b[39mwords.txt\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mr\u001b[39m\u001b[39m'\u001b[39m, encoding \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mwindows 1251\u001b[39m\u001b[39m'\u001b[39m) \u001b[39mas\u001b[39;00m file:\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/wequalwo/source/python/Alter.ipynb#ch0000005?line=2'>3</a>\u001b[0m     TEXT \u001b[39m=\u001b[39m file\u001b[39m.\u001b[39;49mread()\u001b[39m.\u001b[39mreplace(\u001b[39m'\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39m \u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\wequalwo\\AppData\\Local\\Programs\\Python\\Python310\\lib\\codecs.py:701\u001b[0m, in \u001b[0;36mStreamReaderWriter.read\u001b[1;34m(self, size)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/wequalwo/AppData/Local/Programs/Python/Python310/lib/codecs.py?line=698'>699</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mread\u001b[39m(\u001b[39mself\u001b[39m, size\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m):\n\u001b[1;32m--> <a href='file:///c%3A/Users/wequalwo/AppData/Local/Programs/Python/Python310/lib/codecs.py?line=700'>701</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mreader\u001b[39m.\u001b[39;49mread(size)\n",
      "File \u001b[1;32mc:\\Users\\wequalwo\\AppData\\Local\\Programs\\Python\\Python310\\lib\\codecs.py:504\u001b[0m, in \u001b[0;36mStreamReader.read\u001b[1;34m(self, size, chars, firstline)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/wequalwo/AppData/Local/Programs/Python/Python310/lib/codecs.py?line=501'>502</a>\u001b[0m     \u001b[39mbreak\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/wequalwo/AppData/Local/Programs/Python/Python310/lib/codecs.py?line=502'>503</a>\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> <a href='file:///c%3A/Users/wequalwo/AppData/Local/Programs/Python/Python310/lib/codecs.py?line=503'>504</a>\u001b[0m     newchars, decodedbytes \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdecode(data, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49merrors)\n\u001b[0;32m    <a href='file:///c%3A/Users/wequalwo/AppData/Local/Programs/Python/Python310/lib/codecs.py?line=504'>505</a>\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mUnicodeDecodeError\u001b[39;00m \u001b[39mas\u001b[39;00m exc:\n\u001b[0;32m    <a href='file:///c%3A/Users/wequalwo/AppData/Local/Programs/Python/Python310/lib/codecs.py?line=505'>506</a>\u001b[0m     \u001b[39mif\u001b[39;00m firstline:\n",
      "File \u001b[1;32mc:\\Users\\wequalwo\\AppData\\Local\\Programs\\Python\\Python310\\lib\\encodings\\cp1251.py:15\u001b[0m, in \u001b[0;36mCodec.decode\u001b[1;34m(self, input, errors)\u001b[0m\n\u001b[0;32m     <a href='file:///c%3A/Users/wequalwo/AppData/Local/Programs/Python/Python310/lib/encodings/cp1251.py?line=13'>14</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecode\u001b[39m(\u001b[39mself\u001b[39m,\u001b[39minput\u001b[39m,errors\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mstrict\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[1;32m---> <a href='file:///c%3A/Users/wequalwo/AppData/Local/Programs/Python/Python310/lib/encodings/cp1251.py?line=14'>15</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m codecs\u001b[39m.\u001b[39;49mcharmap_decode(\u001b[39minput\u001b[39;49m,errors,decoding_table)\n",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m: 'charmap' codec can't decode byte 0x98 in position 899: character maps to <undefined>"
     ]
    }
   ],
   "source": [
    "import codecs\n",
    "with codecs.open('text.txt', 'r', encoding = 'utf-8') as file:\n",
    "    TEXT = file.read().replace('\\n', ' ') # для текста, в котором слова разделены '\\n'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with codecs.open('russian.txt', 'r', encoding = 'windows 1251') as file:\n",
    "    TEXT2 = file.read().replace('\\n', ' ') # для текста, в котором слова разделены '\\n'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Затем нужно этот текст разбить на слова (необходимо выполнить токенизацию). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokens(text):\n",
    "    #Возвращает список токенов (подряд идущих буквенных последовательностей) в тексте. \n",
    "       #Текст при этом приводится к нижнему регистру.\n",
    "    return re.findall(r'[а-ё]+', text.lower()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WORDS = tokens(TEXT)\n",
    "print('Количество слов в словаре: ' + len(WORDS))\n",
    "print( 'Количество неповторяющихся слов в словаре: '+ len(set(WORDS)))\n",
    "\n",
    "WORDS2 = tokens(TEXT2)\n",
    "print(len(WORDS2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Хочу заметить, что сейчас слова появляются в нашем списке в том порядке, как они располагались в файле"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Модель: Мешок слов (aka Bag of Words)***\n",
    "\n",
    "Мы создали список `WORDS` - список слов в том порядке, как они следуют в `TEXT`. Мы можем использовать этот список в качестве порождающей модели текста. Язык - очень сложная штука и мы создаем крайне упрощенную модель языка, которая может ухватить часть этой сложной структуры. \n",
    "\n",
    "В модели мешка слов, мы полностью игнорируем порядок слов, зато соблюдаем их частоту. Представить это можно себе так: вы берете все слова текста и забрасываете их в мешок. Теперь, если вы хотите сгенерировать предложение с помощью этого мешка, вы просто трясете его(слова там перемешиваются) и достаете указанное количество слов по одному (мешок непрозрачный, так что слоа вы достаете наугад). Почти наверное полученное предложение будет грамматически некорректным, но слова в этом предложении будут в +- правильной пропорции (более частые будут встречаться чаще, более редкие - реже). Вот функция, которая сэмплирует предложение из $n$ слов с помощью нашего мешка:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "def sample(bag, n=10):\n",
    "    \"Выборка случайного предложения из n слов из модели, описанной мешком слов.\"\n",
    "    return ' '.join(random.choice(bag) for _ in range(n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample(WORDS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Другое представление мешка слов - **`Counter`**. Это словарь, состоящий из пар \n",
    "\n",
    "`{'слово': кол-во вхождений слова в текст}`. \n",
    "\n",
    "Например:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Counter(tokens('Между нами провода, Города да да да. Я сказал иди сюда, И ты сказала: «Да, да, да..»'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Давайте завернем в Counter наш список слов WORDS и посмотрим, что получится:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "COUNTS = Counter(WORDS)\n",
    "\n",
    "print('Десять самых часто встречающихся в тексте слов ' + COUNTS.most_common(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for w in tokens('самые редкие слова: Крыжить, Михрютка, Драдедамовый '):\n",
    "    print(COUNTS[w], w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------------------------------------------------------------------\n",
    "![Джордж Ципф](http://www.factroom.ru/facts/wp-content/uploads/2013/12/224.jpg \" \")\n",
    "\n",
    "В 1935, лингвист Джордж Ципф отметил, что в любом большом тексте $n - $тое наиболее часто встречающееся слово появляется с частотой ~ $ 1/n$ от частоты наиболее часто встречающегося слова. Это наблюдение получило название Закона Ципфа, несмотря на то, что Феликс Ауэрбах заметил это еще в 1913 году. Если нарисовать частоты слов, начиная от самого часто встречающегося, на log-log-графике, они должны приблизительно следовать прямой линии, если закон Ципфа верен. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "M = COUNTS['и'] \n",
    "yscale('log'); \n",
    "xscale('log'); \n",
    "title('Частота n-того наиболее частого слова и линия 1/n.')\n",
    "plot([c for (w, c) in COUNTS.most_common()])\n",
    "plot([M/i for i in range(1, len(COUNTS)+1)]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Анализ полученных результатов:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------------------------------------------------------------------\n",
    "*дополнительное, незнаю, что с этим делать*\n",
    "Но чем плох Bag of words? Тем, что для него кошк|у| != кошк|а| - два разных слова. Как это побороть? А что если мы вырежем все окончания и суффиксы, одним словом сделаем **стемминг Портера**. Тем самым мы будем считать что слово одно, а его форм много, но они уже не считаются за разные слова."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#что-то реализую\n",
    "WORDS += WORDS2\n",
    "#WORDS = [w for w in WORDS if len(w) > 2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Но тогда возникает проблема, существуют слова, для которых стемминг не приведет к одному и тому же слову, например \"шел\" и \"идти\". Тогда на помощь приходят морфологические анализаторы, которые приводят слово к начальной форме (происходит лемматизация)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymystem3 import Mystem\n",
    "text = \"Красивая мама красиво мыла раму\" \n",
    "m = Mystem()\n",
    "lemmas = m.lemmatize(text)\n",
    "print(''.join(lemmas))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymystem3\n",
    "m1=pymystem3.Mystem()\n",
    "\n",
    "m1.analyze('шел')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Но в таком случае возникает проблема, что делать с \"стекло\" или \"эти типы стали есть в прокатном цехе\".\n",
    "Морфологические анализаторы иногда делают ошибки и без контекста им не разобраться.\n",
    "Но pymystem вроде как +- справляются\n",
    "----------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 часть. Проверка Правописания\n",
    "\n",
    "Применим наивный подход: найдем все кандидаты, достаточно близкие к w и выберем более близкое слово, если проверки на близость недостаточно, берем из подходящих слово с максимальной частотой из WORDS. \n",
    "\n",
    "Сейчас мы будем измерять близость с помощью расстояния Левенштейна: минимального необходимого количества удалений, перестановок, вставок, и замен символов, необходимых чтобы одно слово превратить в другое. \n",
    "`Расстояние Левенштейна:\n",
    "Исходное слово --> поиск\n",
    "Вставка --> происк\n",
    "Удаление --> п_иск\n",
    "Замена --> поеск`\n",
    "\n",
    "Тогда остается определить функцию *correct(w):*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correct(word):\n",
    "    \"Поиск лучшего исправления ошибки для данного слова.\"\n",
    "    # предрассчитать edit_distance==0, затем 1, затем 2; в противном случае оставить слово \"как есть\".\n",
    "    candidates = (known(edits0(word)) or \n",
    "                  known(edits1(word)) or \n",
    "                  known(edits2(word)) or \n",
    "                  [word])\n",
    "    return max(candidates, key=COUNTS.get)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "функция `edits2` iлегко получается из функции `edits1`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def known(words):\n",
    "    \"Вернуть подмножество слов, которое есть в нашем словаре.\"\n",
    "    return {w for w in words if w in COUNTS}\n",
    "\n",
    "def edits0(word): \n",
    "    \"Вернуть все строки, которые находятся на edit_distance == 0  (т.е., просто само слово).\"\n",
    "    return {word}\n",
    "\n",
    "def edits2(word):\n",
    "    \"Вернуть все строки, которые находятся на edit_distance == 2 .\"\n",
    "    return {e2 for e1 in edits1(word) for e2 in edits1(e1)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Функция `edits1(word)` должна возвращать множество слов, находящихся на расстоянии `edit_distance == 1`. Например для слова \"киты\" это множество будет включать слова \"коты\" (замена и на o) и слово \"ккиты\" (вставка к),  а также \"икты\" (перемена к и и местами); после чего может быть применена функция known для фильтрации и выбора подходящих кандидатов). \n",
    "Как же нам получить их? Например можно разбить исходное слово на пару всеми возможными способами (каждое разбиение даст нам пару \"слов\"), (a, b), первая часть - до места разбиения, а вторая - после, и в каждом месте разбиения можно: удалить, поменять местами, заменить или вставить букву:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def edits1(word):\n",
    "    \"Возвращает список всех строк на расстоянии edit_distance == 1 от word.\"\n",
    "    pairs      = splits(word)\n",
    "    deletes    = [a+b[1:]           for (a, b) in pairs if b]\n",
    "    transposes = [a+b[1]+b[0]+b[2:] for (a, b) in pairs if len(b) > 1]\n",
    "    replaces   = [a+c+b[1:]         for (a, b) in pairs for c in alphabet if b]\n",
    "    inserts    = [a+c+b             for (a, b) in pairs for c in alphabet]\n",
    "    return set(deletes + transposes + replaces + inserts)\n",
    "\n",
    "def splits(word):\n",
    "    \"Возвращает список всех возможных разбиений слова на пару (a, b).\"\n",
    "    return [(word[:i], word[i:]) \n",
    "            for i in range(len(word)+1)]\n",
    "\n",
    "alphabet = 'абвгдеёжзийклмнопрстуфхцчшщъыьяюя'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(alphabet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(edits0('атец'))\n",
    "print(edits1('атец'))\n",
    "print(len(edits2('атец')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens('Ана ни зочит в шшколу.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(map(correct, tokens('Ана ни зочит в шшколу.')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выходные данные можно сделать покрасивее, сохранив регистр"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correct_text(text):\n",
    "    \"Исправить все слова с опечатками в тексте.\"\n",
    "    return re.sub('[а-ёА-Ё]+', correct_match, text)\n",
    "\n",
    "def correct_match(match):\n",
    "    \"Исправить слово word в match-группе, сохранив регистр: upper/lower/title.\"\n",
    "    word = match.group()\n",
    "    return case_of(word)(correct(word.lower()))\n",
    "\n",
    "def case_of(text):\n",
    "    \"Возвращает функцию регистра по тексту: upper, lower, title, или str.\"\n",
    "    return (str.upper if text.isupper() else\n",
    "            str.lower if text.islower() else\n",
    "            str.title if text.istitle() else\n",
    "            str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Насколько дорого превращать одно слово в другое?***\n",
    "\n",
    "Динамическое программирование позволяет разбить задачу на подзадачи, решив которые можно скомпоновать финальное решение. Мы будем пытаться превратить строку $source[0..i]$ в строку $target[0..j]$, мы сосчитаем все возможные комбинации подстрок $substrings[i, j]$ и рассчитаем их *edit_distance* до нашей исходной. Мы будем сохранять результаты в таблицу и переиспользовать их для расчета дальнейших изменений.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def min_edit_distance(source, target, ins_cost = 1, del_cost = 1, rep_cost = 2):\n",
    "    '''\n",
    "    Input: \n",
    "        source: строка-исходник\n",
    "        target: строка, в которую мы должны исходник превратить\n",
    "        ins_cost: цена вставки\n",
    "        del_cost: цена удаления\n",
    "        rep_cost: цена замены буквы\n",
    "    Output:\n",
    "        D: матрица размера len(source)+1 на len(target)+1 содержащая минимальные расстояния edit_distance\n",
    "        med: минимальное расстояние edit_distance (med), необходимое, \n",
    "        чтобы превратить строку source в строку target\n",
    "    '''\n",
    "    # стоимость удаления и вставки = 1\n",
    "    m = len(source)\n",
    "    n = len(target)\n",
    "\n",
    "    # Заткнем нашу матрицу нулями\n",
    "    D = np.zeros((m+1, n+1), dtype=int) \n",
    "    \n",
    "    # Заполним первую колонку\n",
    "    for row in range(1,m+1): \n",
    "        D[row,0] = D[row-1,0] + del_cost\n",
    "        \n",
    "    # Заполним первую строку\n",
    "    for col in range(1,n+1): \n",
    "        D[0,col] = D[0,col-1] + ins_cost\n",
    "        \n",
    "    # Теперь пойдем от 1 к m-той строке\n",
    "    for row in range(1,m+1): \n",
    "        \n",
    "        # итерируемся по колонкам от 1 до n\n",
    "        for col in range(1,n+1):\n",
    "            \n",
    "            # r_cost - стоимость замены\n",
    "            r_cost = rep_cost\n",
    "            \n",
    "            # Совпадает ли буква исходного слова из предыдущей строки\n",
    "            # с буквой целевого слова из предыдущей колонки, \n",
    "            if source[row-1] == target[col-1]:\n",
    "                # Если они не нужны, то замена не нужна -> стоимость = 0\n",
    "                r_cost = 0\n",
    "                \n",
    "            # Обновляем значение ячейки на базе предыдущих значений \n",
    "            # Считаем D[i,j] как минимум из трех возможных стоимостей (как в формуле выше)\n",
    "            D[row,col] = min([D[row-1,col]+del_cost, D[row,col-1]+ins_cost, D[row-1,col-1]+r_cost])\n",
    "          \n",
    "    # установить edit_distance в значение из правого нижнего угла\n",
    "    med = D[m,n]\n",
    "    \n",
    "\n",
    "    return D, med"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "source =  'кроты'\n",
    "target = 'киты'\n",
    "matrix, min_edits = min_edit_distance(source, target)\n",
    "\n",
    "print(\"Расстояние: \",min_edits, \"\\n\")\n",
    "\n",
    "idx = list('#' + source)\n",
    "cols = list('#' + target)\n",
    "df = pd.DataFrame(matrix, index=idx, columns= cols)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 часть. От счетчика слов к вероятностям последовательностей слов. Разбиение на сегменты\n",
    "\n",
    "Нам нужно научиться подсчитывать вероятности слов, $P(w)$. Делать мы это будем с помощью функции `pdist`, которая на вход принимает Counter (мешок слов) и возвращает функцию, выполняющую роль вероятностного распределения на множестве всех возможных слов. В вероятностном распределении вероятность каждого слова лежит между 0 и 1, и сложение вероятностей всех слов дает 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pdist(counter):\n",
    "    \"Превращает частоты из Counter в вероятностное распределение.\"\n",
    "    N = sum(list(counter.values()))\n",
    "    return lambda x: counter[x]/N\n",
    "\n",
    "P = pdist(COUNTS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for w in tokens('То мать пирогов напечет, то бабушка с булочками приедет'):\n",
    "    print(P(w), w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Что такое вероятность последовательности слов? \n",
    "Используем определение совместной вероятности:\n",
    "$P(w_1 ... w_n) = P(w_1)*P(w_2|w_1)*P(w_3|w_1w_2)...*...P(w_n|w_1...w_n-1)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Модель мешка слов подразумевает, что каждое слово из мешка достается независимо от других. Это дает нам упрощенную аппроксимацию:\n",
    "$P(w_1 ... w_n) = P(w_1)*P(w_2)*P(w_3)...*...P(w_n)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Известный статистик Джордж Бокс сказал Все модели неверны, но некоторые полезны."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как же нам посчитать $P(w_1 ... w_n)$? Мы будем использовать другое название `Pwords` вместо `P`, и посчитаем ее как произведение индивидуальных вероятностей:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Pwords(words):\n",
    "    \"Вероятности слов, при условии, что они независимы.\"\n",
    "    return product(P(w) for w in words)\n",
    "\n",
    "def product(nums):\n",
    "    \"Перемножим числа.\"\n",
    "    result = 1\n",
    "    for x in nums:\n",
    "        result *= x\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tests = ['тест', \n",
    "         'дом',\n",
    "         'Крыжить']\n",
    "\n",
    "for test in tests:\n",
    "    print(Pwords(tokens(test)), test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "кажется, присвоить последнюю вероятность 0, неправильно; Она просто должна быть маленькой. К этому вернемся попозже. Ну а другие вероятности кажутся +- адекватными."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Разбиение слов на сегменты\n",
    "Исправляем ошибки, когда слова пишутся слитно (по ошибке или нет)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Подход 1:** Перенумеруем все возможные разбиения и выберем то, у которого максимальная Pwords\n",
    "\n",
    "Вопрос: Как выбрать количество сегментов для строки длины n?\n",
    "\n",
    "**Подход 2:** Делаем одно разбиение - на первое слово и все остальное. Если предположить, что слова независимы, можно максимизировать вероятность первого слова + лучшего разбиения оставшихся букв.\n",
    "\n",
    "\n",
    "`assert segment('фотоальбом') == ['фото', 'альбом']\n",
    "segment('фотоальбом') ==\n",
    "   max(Pwords(['ф'] + segment('отоальбом')),\n",
    "       Pwords(['фо'] + segment('тоальбом')),\n",
    "       Pwords(['фот'] + segment('оальбом')),\n",
    "       Pwords(['фото'] + segment('альбом')),\n",
    "       ...\n",
    "       Pwords(['фотоальбом'] + segment('')))`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Чтобы сделать это эффективным, нужно избежать слишком большого числа пересчетов оставшейся части слова. Это можно сделать c помощью кэширования. Кроме того, для первого слова не обязательно брать все возможные варианты разбиений - мы можем установить максимальную длину. Какой она должна быть? Чуть большей, чем длина самого длинного слова, которое мы видели."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def memo(f):\n",
    "    \"Запомнить результаты исполнения функции f, чьи аргументы args должны быть хешируемыми.\"\n",
    "    cache = {}\n",
    "    def fmemo(*args):\n",
    "        if args not in cache:\n",
    "            cache[args] = f(*args)\n",
    "        return cache[args]\n",
    "    fmemo.cache = cache\n",
    "    return fmemo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max(len(w) for w in COUNTS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def splits(text, start=0, L=20):\n",
    "    \"Вернуть список всех пар (a, b); start <= len(a) <= L.\"\n",
    "    return [(text[:i], text[i:]) \n",
    "            for i in range(start, min(len(text), L)+1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(splits('слово'))\n",
    "print(splits('длинныйтекст', 3, 7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@memo\n",
    "def segment(text):\n",
    "    \"Вернуть список слов, который является наиболее вероятной сегментацией нашего текста.\"\n",
    "    if not text: \n",
    "        return []\n",
    "    else:\n",
    "        candidates = ([first] + segment(rest) \n",
    "                      for (first, rest) in splits(text, 1))\n",
    "        return max(candidates, key=Pwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "segment('сдырочкой')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decl = ('предложениеэтосложнаясинтаксическаяконструкция')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(segment(decl))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Анализ:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 часть. Анализ с помощью биграмм"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---------------------------------------------------------------------------\n",
    "Можно перейти к анализу пар последоваительных слов, не ожидая, что вероятности слишком часто будут обнуляться (представьте себе, сколько в языке может быть грамматически корректных сочетаний из двух слов)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_counts(text, sep='\\t'):\n",
    "    \"\"\"Возвращает Counter, полученный из пар ключ-значение,в каждой строке файла.\"\"\"\n",
    "    C = Counter()\n",
    "    for i in [l.split('\\t') for l in text.split('\\n')][:-1]:\n",
    "        key, count = i\n",
    "        C[key] = int(count)\n",
    "    return C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "COUNTS1 = список слов с частотой их встречания\n",
    "COUNTS2 = список слов с частотой их встречания\n",
    "\n",
    "P1w = pdist(COUNTS1)\n",
    "P2w = pdist(COUNTS2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(COUNTS1), sum(list(COUNTS1.values()))/1e9)\n",
    "print(len(COUNTS2), sum(list(COUNTS2.values()))/1e9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "COUNTS2.most_common(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Сегментация с помощью биграмм**\n",
    "\n",
    "Чуть менее неправильная аппроксимация:\n",
    "$P(w_1...w_n)=P(w_1)*P(w_2|w_1)*P(w_3|w_2)*...*...P(w_n|w_n-1)$\n",
    "Эта штука называется биграммной моделью. Представьте, что вы взяли текст, достали из него все возможные пары подряд идущих слов и положили каждую пару в мешок, промаркированный ПЕРВЫМ словом из пары. После этого, чтобы сгенерировать кусок текста, мы берем первое слово из исходного мешка слов , а каждое следующее слово вынимаем из соответствующего мешка биграмм.\n",
    "\n",
    "Начнем с определения вероятности текущего слова при условии данного предыдущего слова из Counter:\n",
    "\n",
    "Отмечу, что для английского языка биграммная модель будет выглядеть так:\n",
    "$P(w_1...w_n)=P(w_1)*P(w_2|w_1)*P(w_3|w_2)*...*...P(w_n|w_n-1)$ условная вероятность слова при условии предыдущего слова определяется так:\n",
    "$P(w_n|w_n-1)=P(w_n-1w_n)/P(w_n-1)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Pwords2(words, prev='<S>'):\n",
    "    \"Вероятность последовательности слов с помощью биграммной модели(при условии предыдущего слова).\"\n",
    "    return product(cPword(w, (prev if (i == 0) else words[i-1]) )\n",
    "                   for (i, w) in enumerate(words))\n",
    "\n",
    "# Перепишем Pwords на большой словарь P1w вместо Pword\n",
    "def Pwords(words):\n",
    "    \"Вероятности слов при условии их независимости.\"\n",
    "    return product(P1w(w) for w in words)\n",
    "\n",
    "def cPword(word, prev):\n",
    "    \"Условная вероятность слова при условии предыдущего.\"\n",
    "    bigram = prev + ' ' + word\n",
    "    if P2w(bigram) > 0 and P1w(prev) > 0:\n",
    "        return P2w(bigram) / P1w(prev)\n",
    "    else: # если что-то не встретилось, поставим среднее между P1w и 0\n",
    "        return P1w(word) / 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(Pwords(tokens('Он приехал в город')))\n",
    "print(Pwords2(tokens('Приехал он в город')))\n",
    "print(Pwords2(tokens('Город в он приехал')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Чтобы сделать `segment2`, скопируем `segment`, добавим в аргументы предыдущий токен, а вероятности будем считать с помощью `Pwords2` вместо `Pwords`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@memo \n",
    "def segment2(text, prev='<S>'): \n",
    "    \"Возвращает наилучшее разбиение текста, используя статистику биграмм.\" \n",
    "    if not text: \n",
    "        return []\n",
    "    else:\n",
    "        candidates = ([first] + segment2(rest, first) \n",
    "                      for (first, rest) in splits(text, 1))\n",
    "        return max(candidates, key=lambda words: Pwords2(words, prev))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(segment2('фотоальбом'))\n",
    "print(segment2('поехалаонанет'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Анализ Биграммной модели:  вроде бы лучше, но не очень.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6 часть.Валидация\n",
    "\n",
    "До настоящего момента мы пытались интуитивно оценить результаты нашей работы. Тем не менее, никаких численных оценок качества мы пока не получили. Важно понимать, что без четких метрик слова \"плохо\"/\"хорошо\" не имеют никакого смысла. Более того - мы даже не можем четко ответить, было ли наше обновление модели в лучшую сторону или худшую. \n",
    "\n",
    "Обычно при построении неких прогностических моделей данные разбиваются на три части:\n",
    "\n",
    "* **Обучающая выборка:** то, что мы использовали для создания модели исправления ошибок; У нас это был файл text.txt.\n",
    "* **Тестовая выборка:** набор данных, который можно использовать для оценки качества вашей модели по ходу разработки.\n",
    "* **Валидационная выборка:** Набор данных, который мы используем для оценки работы программы после того как программа готова. Тестовая выборка для этого быть использована не может. \n",
    "\n",
    "Для нашей программы обучающая выборка - словарь слов с частотами, а тестовая выборка - набор примеров типа \"фотоальбом\". \n",
    "\n",
    "Сделаем валидационную выборку."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_segmenter(segmenter, tests):\n",
    "    \"Оценка сегментатора на тестовых данных; вывести на печать ошибки; вернуть долю верно разбитого.\"\n",
    "    return sum([test_one_segment(segmenter, test) \n",
    "               for test in tests]), len(tests)\n",
    "\n",
    "def test_one_segment(segmenter, test):\n",
    "    words = tokens(test)\n",
    "    result = segmenter(''.join(words))\n",
    "    correct = (result == words)\n",
    "    if not correct:\n",
    "        print('expected', words)\n",
    "        print('got     ', result) \n",
    "    return correct\n",
    "\n",
    "proverbs = (\"\"\"Унас под крыльцом живут ежи. По вечерам вся смья выходит гулять.\"\"\".splitlines())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_segmenter(segment, proverbs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7часть. Сглаживание"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проблема в том, что если в последовательности вероятность одного из слов - 0, то вся вероятность бы обнулилась (мы же считаем произведение). Кажется, что это слишком строгое условие; Словарь не идеален и точно существуют реальные слова, которых мы не увидели. Давайте не будем все сразу обнулять. \n",
    "Например точно должна быть оценка вероятности того, что слово настоящее. Скажем, \"сверхзвуковой\" явно должно быть более вероятным чем \"рыбркле\" и должно иметь бОльшую вероятность.\n",
    "\n",
    "Проблему можно побороть, присвоив таким \"не встретившимся\" словам ненулевую вероятность. Еще более важным этот пункт становится при переходе к токенам из нескольких слов (биграммам, например), потому что чем больше слов в токене, тем больше вероятность, что какой-то реальный токен в нашей обучающей выборке отсутствует.\n",
    "\n",
    "Нашу модель можно представить в виде забора вероятностей, где столбик равен вероятности слова/токена, которое/который в выборке было/был, и равен 0, если слова/токена в выборке не было; Мы хотим сгладить наше распределение вокруг этих пиков, чтобы модель давала какой-то ответ вне зависимости от наличия или отсутствия слова в корпусе. Этот процесс и называется сглаживанием."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Место для анекдота про Лапласа**\n",
    "\n",
    "Однажды французского математика Лапласа спросили: \"Какова вероятность того, что Солнце завтра взойдет?\". Из данных, что оно из  ближайших дней взошло n раз следует оценка максимального правдоподобия $n/n = 1$. Но Лапласу хотелось чуть сбалансировать оценку на шанс того, что завтра Солнце может и не взойти, поэтому он дал оценку $(n+1)/(n+2)$.\n",
    "\n",
    "То, что мы знаем, ограничено, а то, чего мы не знаем,-бесконечно\n",
    "— Пьер Симон Лаплас, 1749-1827"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pdist_additive_smoothed(counter, c=1):\n",
    "    \"\"\"Вероятность слова, при условии данных из Counter'a.\n",
    "    добавляем c к частоте каждого слова + слово 'unknown'.\"\"\"\n",
    "    N = sum(list(counter.values()))          # суммарное кол-во слов\n",
    "    Nplus = N + c * (len(counter) + 1) # кол-во слов + сглаживание\n",
    "    return lambda word: (counter[word] + c) / Nplus \n",
    "\n",
    "P1w = pdist_additive_smoothed(COUNTS1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P1w('сверхзвуковой')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь еще одна проблема ... у нас появились незнакомые слова с ненулевой вероятностью. А что если 10-12 - приемлемая вероятность для слов нашего текста: то есть, если я читаю новый текст, вероятность того, что следующее слово мне незнакомо, может быть порядка 10-12. Но если я случайно генерирую 20-буквенный последовательности, вероятность того, что одна из них будет реальным словом намного меньше чем 10-12.\n",
    "\n",
    "У нас две проблемы:\n",
    "\n",
    "* Во-первых, у нас нет четкой модели для неизвестных слов. Мы говорим \"неизвестное слово\", но не различаем более вероятные неизвестные слова и менее вероятные неизвестные слова. Ну, например, вероятнее ли 8-буквенное неизвестное слово чем 20-буквенное неизвестное слово?\n",
    "\n",
    "* Во-вторых, мы не берем в расчет информацию из частей неизвестных слов. Например, \"сверхзвуковой\" явно должно быть более вероятным чем \"рыбркле\".\n",
    "\n",
    "Для нашего следующего подхода мы используем идеи метода Гуда - Тьюринга. Он оценивает вероятности слов, не встретившихся в нашем Counter'е, на основании вероятностей слов, встретившихся единожды (Можно туда же подключить вероятности для встретившихся 2 раза и т.д.).\n",
    "\n",
    "Итак, сколько слов встретилось 1 раз в COUNTS? (В COUNTS1 ни одного такого слова нет.) И какие длины у этих слов? Давайте посмотрим:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "singletons = (w for w in COUNTS if COUNTS[w] == 1)\n",
    "lengths = list(map(len, singletons))\n",
    "Counter(lengths).most_common()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matplotlib.pyplot.hist(lengths, bins=len(set(lengths)));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Анализ:** Длины таких слов распределены похоже на нормальное распределение :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pdist_good_turing_hack(counter, onecounter, base=1/33., prior=1e-8):\n",
    "    \"\"\"Вероятность слова при условии данных из счетчика.\n",
    "    Для неизвестных слов, смотрим на слова, встретившиеся единожды из onecounter, \n",
    "    вероятность выбираем, основываясь на длине.\n",
    "    Воспользуемся идеей метода Гуда-Тьюринга(полностью мы его здесь не реализуем).\n",
    "    prior -добавочный фактор, который сделает неизвестные слова менее вероятными.\n",
    "    base -то, насколько мы уменьшаем вероятность за длину слова больше максимального.\"\"\"\n",
    "    N = sum(list(counter.values()))\n",
    "    N2 = sum(list(onecounter.values()))\n",
    "    lengths = list(map(len, [w for w in onecounter if onecounter[w] == 1]))\n",
    "    ones = Counter(lengths)\n",
    "    longest = max(ones)\n",
    "    return (lambda word: \n",
    "            counter[word] / N if (word in counter) \n",
    "            else prior * (ones[len(word)] / N2 or \n",
    "                          ones[longest] / N2 * base ** (len(word)-longest)))\n",
    "#Переопределим P1w\n",
    "P1w = pdist_good_turing_hack(COUNTS1, COUNTS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "segment.cache.clear()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# THE END"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "У Норвига в статье код занимает 21 строчку: \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re, collections\n",
    "def words(text):\n",
    "    return re.findall('[а-ё]+', text.lower())\n",
    "def train(features):\n",
    "    model = collections.defaultdict(lambda: 1)\n",
    "    for f in features:\n",
    "        model[f] += 1\n",
    "    return model\n",
    "    \n",
    "with codecs.open('words.txt', 'r', encoding = 'windows 1251') as file:\n",
    "    TEXT = file.read().replace('\\n', ' ')\n",
    "\n",
    "NWORDS = train(words(TEXT))\n",
    "alphabet = 'абвгдеёжзийклмнопрстуфхцчшщъыьэюя'\n",
    "def edits1(word):\n",
    "    n = len(word)\n",
    "    return set( [word[0:i]+word[i+1:] for i in range(n)] +                      # deletion\n",
    "                [word[0:i]+word[i+1]+word[i]+word[i+2:] for i in range(n-1)] +   # transposition\n",
    "                [word[0:i]+c+word[i+1:] for i in range(n) for c in alphabet] +    # alteration\n",
    "                [word[0:i]+c+word[i:] for i in range(n+1) for c in alphabet])    # insertion\n",
    "def known_edits2(word):\n",
    "    return set(e2 for e1 in edits1(word) for e2 in edits1(e1) if e2 in NWORDS)\n",
    "def known(words): return set(w for w in words if w in NWORDS)\n",
    "def correct(word):\n",
    "    candidates = known([word]) or known(edits1(word)) or known_edits2(word) or [word]\n",
    "    return max(candidates, key=lambda w: NWORDS[w])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Использовать этот код нужно следующим образом –\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_correct = tokens('Саничка не хочит в шшклу')\n",
    "\n",
    "for word in to_correct:\n",
    "    print(correct(word), end = ' ') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Пусть дано слово, будем пытаться отыскать слово, в котором с наибольшей вероятностью исправлены допущенные ошибки (если ошибок нет, то таким словом будет данное). Разумеется, мы не сможем гарантировать 100% исправления всех ошибок. (Например, если нам дано слово «пак», то правильным будет слово «паз» или «парк» ?), именно поэтому мы используем вероятностный (или другими словами стохастический) подход. \n",
    "Будем говорить, что мы пытаемся выбрать такое слово c из всех возможных слов-исправлений, что вероятность появления именно слова c при данном слове w будет максимальна:\n",
    "\n",
    "`argmaxc P(c|w)`\n",
    "\n",
    "\n",
    "Согласно теореме Байеса - выражение, записанное выше, эквивалентно следующему выражению:\n",
    "\n",
    "`argmaxc P(w|c) P(c) / P(w)`\n",
    "\n",
    "Поскольку P(w) одинакова для всех c мы можем отбросить P(w), что даст нам:\n",
    "\n",
    "`argmaxc P(w|c) P(c)`\n",
    "\n",
    "--------------------------------------------------------------------------------------\n",
    "В этом выражении присутствуют три части.  \n",
    "Справа налево:\n",
    "\n",
    "***P(c)*** – вероятность появления слова c (частотность употребления c). Эта вероятность обусловлена самим языком (точнее моделью языка). Иначе говоря, P(c) определяет как часто c встречается в текстах на русском языке. P(«превед») будет достаточно высока, тогда как P(«благоденствовать») будет меньше, а P(«ыгввыцшы») будет около нуля.\n",
    "\n",
    "***P(w|c)*** – вероятность того, что автор опечатался и написал w, хотя имел в виду c. По сути дела эта вероятность обусловлена частотностью тех или иных ошибок в языке (и называется моделью ошибок языка).\n",
    "\n",
    "***argmaxc*** – оператор, перебирающий все возможные c в поиске наиболее (вероятнее всего) подходящего из них (т.е. данный оператор ищет такое допустимое c, которе максимизирует условную вероятность появления w при данном c).\n",
    "\n",
    "Может возникнуть очевидный вопрос – зачем мы преобразовали простое выражение «argmaxc P(c|w)» с помощью какого-то Байеса в более сложное выражение, в котором используются аж две языковые модели, вместо одной? Дело в том, что P(c|w) учитывает в себе сразу обе языковых модели, поэтому очевидно, что проще выделить эти модели и работать с ними по отдельности. Предположим у нас есть слово с опечаткой – «езать», это может быть как «ехать», так и «резать». Для какого из исправлений P(c|w) будет максимально ? Оба исправления имеют примерно одинаковую частотность в русском языке. Хорошо допустим «х» и «з» близко расположены в русской раскладке клавиатуры и это повышает вероятность варианта «ехать», но это не повод, чтоб отбрасывать «резать», ведь «е» и «р» тоже близки. Поэтому лучше не рассматривать P(c|w) как единую величину, ибо нам приходится учитывать и частность исправления c и вероятность исправления c для данной опечатки в w. Удобнее работать с этими двумя вероятностями по отдельности.\n",
    "\n",
    "----------------------------------------------------------------------------------------------\n",
    "\n",
    "нужно продолжить описание "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "87852089012c4b81b9af9dc676d2b889d7d3b7bd761d748065844c07d5d6aa6d"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
