{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Автокорректор ошибок на Python**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "%pylab is deprecated, use %matplotlib inline and import the required libraries.\n",
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "# Импортируй и властвуй\n",
    "%pylab inline\n",
    "import re\n",
    "import math\n",
    "import string\n",
    "from collections import Counter\n",
    "import requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для начала немного теории. \n",
    "Пусть дано слово, будем пытаться отыскать слово, в котором с наибольшей вероятностью исправлены допущенные ошибки (если ошибок нет, то таким словом будет данное). Разумеется, мы не сможем гарантировать 100% исправления всех ошибок. (Например, если нам дано слово «пак», то правильным будет слово «паз» или «парк» ?), именно поэтому мы используем вероятностный (или другими словами стохастический) подход. \n",
    "Будем говорить, что мы пытаемся выбрать такое слово c из всех возможных слов-исправлений, что вероятность появления именно слова c при данном слове w будет максимальна:\n",
    "\n",
    "argmaxc P(c|w)\n",
    "\n",
    "\n",
    "Согласно теореме Байеса - выражение, записанное выше, эквивалентно следующему выражению:\n",
    "\n",
    "***argmaxc P(w|c) P(c) / P(w)***\n",
    "\n",
    "Поскольку P(w) одинакова для всех c мы можем отбросить P(w), что даст нам:\n",
    "\n",
    "***argmaxc P(w|c) P(c)***\n",
    "\n",
    "\n",
    "В этом выражении присутствуют три части.  \n",
    "Справа налево:\n",
    "\n",
    "***P(c)*** – вероятность появления слова c (частотность употребления c). Эта вероятность обусловлена самим языком (точнее моделью языка). Иначе говоря, P(c) определяет как часто c встречается в текстах на русском языке. P(«превед») будет достаточно высока, тогда как P(«благоденствовать») будет меньше, а P(«ыгввыцшы») будет около нуля.\n",
    "\n",
    "***P(w|c)*** – вероятность того, что автор опечатался и написал w, хотя имел в виду c. По сути дела эта вероятность обусловлена частотностью тех или иных ошибок в языке (и называется моделью ошибок языка).\n",
    "\n",
    "***argmaxc*** – оператор, перебирающий все возможные c в поиске наиболее (вероятнее всего) подходящего из них (т.е. данный оператор ищет такое допустимое c, которе максимизирует условную вероятность появления w при данном c).\n",
    "\n",
    "Может возникнуть очевидный вопрос – зачем мы преобразовали простое выражение «argmaxc P(c|w)» с помощью какого-то Байеса в более сложное выражение, в котором используются аж две языковые модели, вместо одной? Дело в том, что P(c|w) учитывает в себе сразу обе языковых модели, поэтому очевидно, что проще выделить эти модели и работать с ними по отдельности. Предположим у нас есть слово с опечаткой – «езать», это может быть как «ехать», так и «резать». Для какого из исправлений P(c|w) будет максимально ? Оба исправления имеют примерно одинаковую частотность в русском языке. Хорошо допустим «х» и «з» близко расположены в русской раскладке клавиатуры и это повышает вероятность варианта «ехать», но это не повод, чтоб отбрасывать «резать», ведь «е» и «р» тоже близки. Поэтому лучше не рассматривать P(c|w) как единую величину, ибо нам приходится учитывать и частность исправления c и вероятность исправления c для данной опечатки в w. Удобнее работать с этими двумя вероятностями по отдельности."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Начнем с P(c)**. \n",
    "\n",
    "Мы читаем большой текстовый файл, words.txt, в котором записано много слов из таких произведений как: \"Тихий Дон\", \"Преступление и наказание\", \"Котлован\", \"Каштанка\", \"Капитанская дочка\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "После этого мы извлекаем отдельные слова из файла \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [],
   "source": [
    "import codecs\n",
    "with codecs.open('text.txt', 'r', encoding = 'utf-8') as file:\n",
    "    TEXT = file.read().replace('\\n', ' ') # для текста, в котором слова разделены '\\n'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [],
   "source": [
    "with codecs.open('russian.txt', 'r', encoding = 'windows 1251') as file:\n",
    "    TEXT2 = file.read().replace('\\n', ' ') # для текста, в котором слова разделены '\\n'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokens(text):\n",
    "    \"\"\"Возвращает список токенов (подряд идущих буквенных последовательностей) в тексте. \n",
    "       Текст при этом приводится к нижнему регистру.\"\"\"\n",
    "    return re.findall(r'[а-ё]+', text.lower()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "232251\n",
      "45051\n",
      "1536030\n"
     ]
    }
   ],
   "source": [
    "WORDS = tokens(TEXT)\n",
    "print(len(WORDS))\n",
    "print(len(set(WORDS)))\n",
    "\n",
    "WORDS2 = tokens(TEXT2)\n",
    "print(len(WORDS2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Хочу заметить, что сейчас слова появляются в нашем списке в том порядке, как они располагались в файле"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Модель: Мешок слов (aka Bag of Words)***\n",
    "\n",
    "Мы создали список *WORDS* - список слов в том порядке, как они следуют в *TEXT*. Мы можем использовать этот список в качестве порождающей модели (generative model) текста. Язык - очень сложная штука и мы создаем крайне упрощенную модель языка, которая может ухватить часть этой сложной структуры. \n",
    "\n",
    "В модели мешка слов , мы полностью игнорируем порядок слов, зато соблюдаем их частоту. Представить это можно себе так: вы берете все слова текста и забрасываете их в мешок. Теперь, если вы хотите сгенерировать предложение с помощью этого мешка, вы просто трясете его(слова там перемешиваются) и достаете указанное количество слов по одному (мешок непрозрачный, так что слоа вы достаете наугад). Почти наверное полученное предложение будет грамматически некорректным, но слова в этом предложении будут в +- правильной пропорции (более частые будут встречаться чаще, более редкие - реже). Вот функция, которая сэмплирует(от англ. sample) предложение из n слов с помощью нашего мешка:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "def sample(bag, n=10):\n",
    "    \"Выборка случайного предложения из n слов из модели, описанной мешком слов.\"\n",
    "    return ' '.join(random.choice(bag) for _ in range(n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'обстреляна овечьих экспериментировавшею палашка навинчиваются цветков фаршировалась кольцевавшей трубило пашковцы'"
      ]
     },
     "execution_count": 282,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample(WORDS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Другое представление мешка слов - Counter. Это словарь, состоящий из пар {'слово': кол-во вхождений слова в текст}. Например,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'между': 1,\n",
       "         'нами': 1,\n",
       "         'провода': 1,\n",
       "         'города': 1,\n",
       "         'да': 6,\n",
       "         'я': 1,\n",
       "         'сказал': 1,\n",
       "         'иди': 1,\n",
       "         'сюда': 1,\n",
       "         'и': 1,\n",
       "         'ты': 1,\n",
       "         'сказала': 1})"
      ]
     },
     "execution_count": 283,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(tokens('Между нами провода, Города да да да. Я сказал иди сюда, И ты сказала: «Да, да, да..»'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Counter очень похож на словарь из Python - тип dict , но у него есть ряд дополнительных методов. Давайте завернем в Counter наш список слов WORDS и посмотрим, что получится:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('и', 10020), ('в', 5938), ('не', 4061), ('на', 3696), ('что', 3243), ('с', 2950), ('он', 2632), ('а', 2303), ('я', 2129), ('то', 1895)]\n"
     ]
    }
   ],
   "source": [
    "COUNTS = Counter(WORDS)\n",
    "\n",
    "print(COUNTS.most_common(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14 самые\n",
      "10 редкие\n",
      "121 слова\n",
      "0 крыжить\n",
      "1 михрютка\n",
      "3 драдедамовый\n"
     ]
    }
   ],
   "source": [
    "for w in tokens('самые редкие слова: Крыжить, Михрютка, Драдедамовый '):\n",
    "    print(COUNTS[w], w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В 1935, лингвист Джордж Ципф отметил, что в любом большом тексте n-тое наиболее часто встречающееся слово появляется с частотой ~ 1/n от частоты наиболее часто встречающегося слова. Это наблюдение получило название Закона Ципфа, несмотря на то, что Феликс Ауэрбах заметил это еще в 1913 году. Если нарисовать частоты слов, начиная от самого часто встречающегося, на log-log-графике, они должны приблизительно следовать прямой линии, если закон Ципфа верен. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1a9655227a0>]"
      ]
     },
     "execution_count": 286,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXwAAAEMCAYAAADHxQ0LAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8qNh9FAAAACXBIWXMAAAsTAAALEwEAmpwYAAA3d0lEQVR4nO3dd3hUVfrA8e+bRiBA6DX0Jk1pUkQQBRSQZhcRG4IoqOvqKv5014ptdW3AIijLqggqKh1FpIpIE6RICyCE3kMn7fz+ODc6ZFMmyZQ7yft5njxw+3tm7rxz5txzzxVjDEoppQq+sGAHoJRSKjA04SulVCGhCV8ppQoJTfhKKVVIaMJXSqlCQhO+UkoVEprwlSuJSJiI6PmplA/pB0q5hojcJCKLRWQPkAi0DXZMShUkrk/4ImJEpK7HdF0R0bvFChgR6Qf8C3gaqGaMKWGM+SnIYSnlFyLSRES+E5EjWeUzEaniVH58xvUJXxUarwC3GmOWGr39WxV8ycAXwMBs1ukBfOvToxpjXP0HnAOaeEzXtWH/MX0vsAk4BewAHsiwfR9gLXAS2A50A54ETjt/ac4xTgMbnW1igY+Bw8Au4Fnsl2MVj+2SsG9a+nQHoDQw09nuuPP/uGzK9jvwBLAO24TxORCdxbojPY5lgDPO/+c4y6sA04FjQDwwyGPb5zPEehpo6izrDWwETgALgYbZxGuAuh7TLwMTPKa/BA44ZVkMNPZYNgF42WN6trO/CKCCU56PgSOer7mzbpgzvQs45KwXm0ls6a9JcoZj3eecI8eB74AaHssuAb53Xrct2C8db8v/xzRwPbAGe54lAM9n2PZK4CfndU4A7gFu83g/UoHz6dPONkWAd4B9zt87QBFnWSfsuev5nj7kLGvovJcnnPe2dzZlKgP8x9n/cWCqx7KaThk9Y7w/l7GdAlZw8Wc4y/Mkk/gWehwzDFgP7MkhZ/zOn5/pJOBTj7j2eKx3q1O+9P3fA/yYYV97gE4en6NPPZaNznAOVALmOq97+nn4fA6xXpTPMiz7Grgxt7ki2+PldoNA/2E/JK8B4Zm9QNgPWh1AgKuAs0ALZ1lr58Xp6pwsVYFLMjk5umSY9zEwDSjhnPRbgYEZ1rnozXfmlQVuAoo5236JxwcoixNzBTZZl8EmpSFevCYXJR5n3mLnBIwGmmG/dK7JKlZnfn1skuwKRGK/COOBKG+Oy/8m/Puccqcng7UeyybgJGHgaueDlJ7wazr/z/Q1d/YbD9QGijsfhE889h3mbF8nk2P1cbZt6BzrWeAnZ1kMNvne6yxrjv3CaZRF+dOA+pm9Hthk0tSJ5VLgINDXWVYDm/j6Oa9zWaBZhn0vxEk8HvNeBH7GfiGWx34WXvI43v8kPmf/8cD/AVHANc6xG2RRplnY5FHa2fYqj2W1nTKGZ4zR29iAcGAcMMWb8yST+DyPea9z3uSU8HcDnTOe+xniisR+we8jDwkf+9nZmeEceA2YAxR1pj8ljwnfie8IUCI/uSLjXyg06QwGOgJHReQE8IvnQmPMLGPMdmMtwn7DdnAWDwTGG2O+N8akGWP2GmM2Z3cwEQkHbgeeNsacMsb8DrwFDMgpUGPMUWPMV8aYs8aYU8AI7JdQdt4zxuwzxhwDZmCTda6ISDWgPfCUMea8MWYt8CFwVw6b3gbMcl6fZOBNoChwRW5jADDGjHdeswvYD8dlIhKbIVYB3gD+kckusnrN+wP/MsbsMMacxrbz3y4iEc7yKOffpEz2OQR41RizyRiTgm06aiYiNYCewO/GmP8YY1KMMWuAr4BbsijibuyXY2ZlX2iMWe+cZ+uASfz53t8BzDPGTDLGJDvnydosjuGpP/CiMeaQMeYw8AI5n4dtsV+Krxljkowx87G/NPtlXFFEKgPdsYnjuBPbIo9VooA0Y0xqPmILwyb9o+kzvDlPMok1GnvOvJTdeh5xZ3YueHoAWI6tWOTFK1nEEoZvmso7Ar86eSRdvnOF6xO+MWaDMeYKY0wpY0wpoIXnchHpLiI/i8gx5wuhB1DOWVwN24yTG+Ww3667PObtwv46yJaIFBORD0Rkl4icxNa6SzlfIlk54PH/s9gPKyIyR0ROO3/9czh0FeBYhpPDm5ir4FFOY0watsab3Xa/iMgJ57V+In2miISLyGsist0p++/OonIZtr8VW3OZ7zHvgkfMmcVfJZNlEUBFZ7qM8+/xTOKtAbzrEfMx7K/Bqs6yNunLnOX9sT/NMzMMeFxEEp11/yAibURkgYgcFpFE7BdNfs5DyLzcVbzYJsF5Lz23y+w9rYY9bzJ73cC+rlktyym2Ks5rdAr7pfI+5Oo8yehRbHv2luxWcioUpbKJGxEpgf01+/dMFrfNcD78z+stIm2BBsB/Myx6C/sZPuVse2t2seagB7bZ01OmuSI3XJ/wsyMiRbA1sjeBis4XwmzsBxps8qqTy90ewba91fCYVx3Y68W2j2NPhDbGmJLYb2k84vGaMaa7Maa48zcxh9X3AWWcEzk3Me/Do5zOh6VaDtu18PjyfdNj/h3Y5pMu2GsgNdN367FOJLZW9FSGfR7E1siyes33ZbIsxdkO7M/r/U7tP6ME7HWdUh5/RY3tAZQALMqwrLgx5sHMCm6MmWmMqW2MiXXK7+kz7DWUasaYWGAM+TsPIfNy7/Nim2oZ7mHI6lxIwJ43pbLYV32yrgHnFNs+5zUqCgzHfk7Bu/MkozLYL9sXslknXQ1sZWBHNuv8DfjCGLMrk2U/e54PZP56v4H9NXrRLx/nl84S7HW1UtiLsnmVWcLPt5BO+NifbkWw7dUpItIduNZj+UfAvSLS2bmRp6qIXJLdDp038QtghIiUcH76/xXbHpeTEtiLRSdEpAzwXO6LlHvGmARsG+qrIhItIpdim7NyivkL4Hrn9YnEfmFdcPaVWyWcbY9ir2G8ksk6A7Dt5+syxJ+GbUfO6jWfBDwmIrVEpLiz78+NMSkiUg6bUKZmEdcY4GkRaQwgIrEikt5kMxOoLyIDRCTS+btcRBrmsfzHjDHnRaQ1NrGlmwh0EZFbRSRCRMqKSDMv9jkJeFZEyjvl/Ac5v6fLsbW/J53ydAJ6AZMzrmiM2Y9tcx4tIqWd9TvCH82Ej5L16+pVbMY2QKfyZw3em/Mko78AHxljDmS3klPheQ6Ya4w5m8VqJbDXAkZ4cdzMXINt5pqZyfFrYiszD+W0E7GicZojnc9tEef/tbAXwDflMcYshXTCd5owHsEmruPYD9l0j+UrsG/u29iLt4u4uFaSlYexFzN3AD9ia2/jvdjuHWyN5gj2gpZvu1Rlrx+2trQP+AZ4zhgzL7sNjDFbgDuxP7ePYBNDL2NMTu2fmfkY+7N+L/AbtvwZlSbzn9Fgk8tZ7IWwJVz8mo8HPsE2ke3E9mZ52Fk2GVvTH57ZTo0x3wCvA5OdJoQN2CaG9PPnWuw1m33Yn8yvYysRufUQ8KKInMImvz9qd8aY3dga2+PYJqW1wGVe7PNlYBW2Z8Z67PWrl7PbwHnvemHLeAR7If+ubK5dDcD+ot2M7QH1F2f+d9gLpm/nMbYqTnPkKeAZ7IVa8O48ySici39NZuV97K+B+7NZpyS2LTzLJp8cVMY2B2XmA+y1k8x+OWRUA1s53OhMn+PP5qrryUXtXkSqO6919RzXtV/ASiml3EBEZgMjjTHapKOUUgXcQmCBP3asNXyllCoktIavlFKFhCZ8pZQqJCJyXiXwRKQX0KtEiRKD6tevH+xwlFIqZKxevfqIMaZ8Zstc3YbfqlUrs2rVqmCHoZRSIUNEVhtjWmW2TJt0lFKqkNCEr5RShYQmfKWUKiQ04SulVCER0IQvIjEiskpEegbyuEoppfKZ8EVkvIgcEpENGeZ3E5EtIhIvIp6DWj1F/oYMVUoplUf5reFPwD4j9g/Owz5GYUfrawT0E5FGItIVOzreoXweUymlVB7k68YrY8xiZwxoT62BeGPMDgARmYx94EFx7DNEGwHnRGR2hqfy4Kw/GPtYQ6pXz3G0T6WUUl7yx522VbFP0km3B/sEqGEAInIPcCSzZA9gjBkLjAV745Uf4lNKqUIp4EMrGGMm5LRO+tAKdevW9X9ASilVSPijl85e7HNR08Xh3fNg/2CMmWGMGRwbm+2D7JVSSuWCPxL+SqCe8/zRKOzj46bnsM1FRKSXiIxNTEz0Q3hKKVU45bdb5iRgGdBARPaIyEBjTAr2CfPfAZuwT4ffmN1+lFJK+Z+rR8ts0bKlWfLT8jxtWywqHBHxcURKKeVu2Y2W6crx8NNt3HeSxs99l6dtq5YqSu9mVejbrCoNKpXwcWRKKRV6XFnDT++lU6F63UEvffJtrrdPTYPlO4+yZNsRUtMMl1QqwQ3Nq9K7WRUqxxb1fcBKKeUS2dXwXZnw07WqXdasGnFd3jau2Ihj9W5i+u5opq7dx9qEE4hAm1pl6NusKt2bVia2aKRvA1ZKqSALuYSfXsNvVjV60JpnWuZ+B2kpcGQLmDSo0R6a9WdXxa5M/S2RaWv3suPIGaLCw7j6kvLc0LwqnRpUIDoy3OflUEqpQAu5hJ8uX484PLkPfp0EaybCse0QGQONb8A07886acjUX/cx49f9HDl9gRLREfRoUpk+zavQtlZZwsL0Yq9SKjQVzoSfzhjY/TOs/RQ2ToWk01CmDjS7g5Smt/HT4Wimrt3LdxsOcCYplUolo2lZozS1y8dQq1wMtcsXp3b5GEpGa/OPUsr9Qi7hewytMGjbtm2+2/GF07BpOqz5FHYtBQmDOtdAs/6cq92NedtOMGvdfjYfOEnC8XOkpv352pQrHkXtcsWpVS6GehWL075uOS6pVEK7fiqlXCXkEn46n9Tws3JsB6z9zP6d3AtFS0PTW6BZf6h8GUmpht3HzrLj8Gl2HDnDzsNn2HHkNDsOn+HomSQAKpWMplOD8nRqUIH2dctSQn8FKKWCTBN+dtJSYcdCWDsRNs2E1AsQ1xraPwoNekDY/96MfCDxPIu2HmLhlsMs2XaE0xdSiAgTWtUszdUNKtCpQQXqVyyutX+lVMBpwvfWueOw7gtYNgpO7IKydeGKR+DS2yAyOtNNklPTWL3rOAu2HGLRlsNsPnAKgCqx0VzVoAKdL6lApwbliQjXxwcrpfwv5BK+39rwvZWaApumwdJ3Yf+vEFMB2g6BVvfZpp9s7E88x8Ith1m45RBL449y+kIKFUsW4bbLq3P75dWoUkpv/FJK+U/IJfx0Aa/hZ2QM7FxsE//2HyCqOLS8B9o+CLFxOW6elJLGwi2H+GzFbhZtPYwA1zWuxBPXNaBO+eJ+D18pVfhowveFA+th6Xuw4SsQgeZ3QocnoFS1nLcFEo6d5bMVu/lk2S7OJ6dyZ9saPNq5HqVjovwcuFKqMNGE70sndtsa/y8f218ALQZAh8e9qvEDHDl9gbe/38qkFbuJKRLBTS3iuLVVNRpVKennwJVShYEmfH9I3ANL3oJfPrE1/hZ3wZV/hdiqXm2+9eAp3p8fz3cbDpCUmsZlcbH8o1djWtbI/hqBUkplRxO+P53YbRP/mk9BwqH1IFvjL1bGu83PJjH9132MWbid/SfPc3e7mgxoV4NSRSMpXSxKh3lQSuVKyCX8oPfSyYvju2DRG/DrZ/bi7pV/gTYPQlQxrzY/fSGFN7/bwn+X/U76W3JpXCxfDmlHkQgd2E0p5Z2QS/jpQqKGn9GhTfDDi7BlNpSoDJ2GQ7M7Idy7Z81sPnCSzftPsfPIGd79YRuPd63Pw53r+TlopVRBkV3C17uBfK1CQ+g3Ce79FkpVhxmPwui2sGkGePHlekmlkvRtXpXHutbn+qaVGbkgnt1HzwYgcKVUQac1fH8yxtb05z0PR7ZC3OXQ9UWocYVXmx9IPE/ntxZSuVRRmlUrRcnoSMrERHJ76+qUK17Ev7ErpUKSNukEW2qKbdtf8Aqc2g+N+tjEX7pmjpvO+HUfoxbEc/JcMonnkjmTlErH+uX5+L7W/o9bKRVyNOG7RdJZWDYSfnzbPpWr3VDbo6eI9w9Z/+jHnbw08zcm3Hs5nRpU8GOwSqlQ5Io2fBFpKCJjRGSKiDwYqOO6SlQxuOpJeHg1NLnJJv73Wti+/GmpXu1iQNsa1CxbjBdm/MbirYdJS3PvF7ZSyl3ylfBFZLyIHBKRDRnmdxORLSISLyLDAYwxm4wxQ4Bbgfb5OW7IK1kFbhgD98+3zTrTh8HYTvD70hw3jYoIY8QNTTl6+gJ3jV9Bv3E/c/jUBb+HrJQKffmt4U8AunnOEJFwYBTQHWgE9BORRs6y3sAsYHY+j1swxLWEgXPhpo/g7DGY0AM+HwDHf892s/Z1y7HimS6MuKEJaxNOcP17S1j1+7HAxKyUCln5bsMXkZrATGNME2e6HfC8MeY6Z/ppAGPMqx7bzDLGXJ/F/gYDgwGqV6/ecteuXfmKL2Qkn4OfRsKP/wKTBlc+Zh/CEpn9cMq/7TvJgxNXs+voWWKiwmlTuyy9LqtMl4YV9QlcShVCfr1om0nCvxnoZoy535keALQBpgA3AkWAdcaYUTntu8BdtPVG4l6Y+yxs/BpK1YDur0OD7tlvci6ZL1YmsPvYWX7YdJB9ieeJigjjusaVGHp1HS6ppAOzKVVYuCLhG2OG5WKfoTe0gq/tWARznoTDm6HeddD9NShTO8fN0tIMaxKOM33tPr5esxeAyYPb0rhKrL8jVkq5QKB76ewFPAeJj3Pmec0YM8MYMzg2thAnqdpXwZAf4dqXYddSGNUW5o+wXTuzERYmtKxRhhf6NGHOox0oFhVOn5FLuWH0UhZvPRyg4JVSbuSPhL8SqCcitUQkCrgdmJ6bHYhILxEZm5iY6IfwQkh4JFzxMAxbBY16w+I3YFQb+7B1L36ZxZUuxoyHr+SeK2py7EwSD038hW0HTwUgcKWUG+W3W+YkYBnQQET2iMhAY0wKMAz4DtgEfGGM2Zj/UAuxkpXhpg/hnllQpDh83h8m3gxHt+e4aYUS0TzbsxGTB7clOjKc+z9exYmzSQEIWinlNnqnbahJTYYV42Dhq5By3vbk6fB4jr15AFbvOk6/sT9Tq1wMvZtVoUvDijSo5P1dvkop99OhFQqiUwfh+7/Dus+hdC24/i2o2znHzb7dsJ93f4hn0/6TANzUIo4nuzWgYslof0eslAqAkEv42ksnF3Ysgll/haPx0PhG6PYqlKiU42aHT11g1IJ4Pluxm5LRkdzaKo4B7WpQOTbnXwpKKfcKuYSfTmv4Xkq5AD++Yx+1GFEEOv8DWt0HYTk/KeuX3cd5a+4Wlm0/SmzRSL568Apqly/u/5iVUn4Rcglfa/h5dHS7re3vWAhVWkCvd6DyZV5tGn/oFDePWUa4CKP6t6Bt7bJ+DVUp5R+uGC0zN7Qffh6VrQMDpsKNH0Jigh2Q7dun4ULOXTHrVijBv269jFRj6P/hcv71/VbOJ3s3gqdSKjS4soafTpt08uHccfts3VX/sc/W7f46NOwFItludvjUBR6ZtIZlO44SExXONQ0r8njX+tQsFxOgwJVS+aFNOoVZwkqY+RgcXA/1u0H3N6B0jWw3uZCSyvxNh1i87Qgz1+0jMjyMV25oynWNKyI5fGEopYIr5BJ+Oq3h+0hqCiwfYx+xiIGrnrJP2wrPeTTNnUfOcPf4Few+dpY65WMY3LE2t11e3f8xK6XyRBO+sk4kwLfDYfNMqNAYer8HcZmeFxc5n5zK5BW7mbQigS0HT9G9SSUe61qf+hX1pi2l3EYTvrrY5lkw6wn7QPXWg6Hz3716ru755FSe+WYDX6/ZgzHQqkZp7rqiJtc3rUx4mDb1KOUGIZfwtQ0/AM6fhPkv2WEaSlaxd+rmMO5+umNnkpiyOoFJKxLYeeQMtcvHcEfr6nRvWpkqsdHazq9UEIVcwk+nNfwASFgJMx6BQ79Bo762N48Xd+qCHXt/9ob9jFuyk18TTgBwW6tqvHpjU8K0xq9UUGjCV9lLSYKf3oNFb0BENFz7IjS/C8K8v01j0/6TfLxsF5NW7KZ1rTK8dctlVCtTzI9BK6UyowlfeedIPMz8C/y+BGq0h57vQPn6Xm9ujOGjH3fy5twtnE9Oo33dsoy6owWlikX5LWSl1MVC7k5bFSTl6sLdM6D3SDi4Eca0t7X+FO/GzxcR7u9Qm28f7Uj3JpVYGn+U4V+tJyU1zc+BK6W8oQlfXUwEWgyAYSvhkp6wYAR80AF2L/d6FzXLxTC6fwsGdajFtxsPcO+ElSzaeliHalAqyFzZpKO9dFxk63cw63FI3GNH4OzyHER7P8bRxOW7eG7aRlLSDCWKRDDixqb0urSy9uRRyk+0DV/lz4XTtqa/fAwUrwg9/mnH5fHSibNJ/LT9KK/O2UTCsXNUL1OMcXe10qdtKeUHmvCVb+xdDdMfgYMbbHNPj3/aPvxeSkpJ45Ofd/HK7E2kGcPgjrV54toGRIZry6JSvqIXbZVvVG0JgxdCl+chfh6MagMrP4Q07y7KRkWEMfDKWnz7aAc6X1KRDxbt4M4Pl5Oa5t5Kh1IFiSZ8lTvhkXDlY/DgT1CluW3f/083OLzF613Uq1iCD+9uxeNd67N85zGe+WY9ieeS/Ri0Ugo04au8KlsH7poGff9tk/2YK2HRP73uwgkw9Oq6tKlVhskrE7j85Xm898M23NzEqFSoC1jCF5G+IjJORD4XkWsDdVzlRyLQ7A6nC+f1sOBl+5Stvau92jwsTJg8uC1fDmlHXJmi/Ov7rdw+9mftt6+Un+Qr4YvIeBE5JCIbMszvJiJbRCReRIYDGGOmGmMGAUOA2/JzXOUyxSvALRPg9klw7hh82AW+ewaSzuS4qYhwec0yzHm0A7e0jGP5zmN0fGMB6/ac8HvYShU2+a3hTwC6ec4QkXBgFNAdaAT0E5FGHqs86yxXBc0lPWDocmhxNywbCaPbwfYFXm1aJCKcV29syst9m3DqfAo3jP6Jz5bv5lyS3qyllK/kK+EbYxYDxzLMbg3EG2N2GGOSgMlAH7FeB+YYY37Jap8iMlhEVonIqsOHD+cnPBUM0bHQ6x24ZxaERcAnfWHqUPuM3RxEhIdxZ9saTB3Wnmqli/J/36yn5cvf8+68bZw6rxd1lcovf7ThVwUSPKb3OPMeBroAN4vIkKw2NsaMNca0Msa0Kl++vB/CUwFR80p4cKnt0fPrJBjZGjZOBS8uytYpX5x5f72KkXc0p1a5GN6et5Ve7//I5gMn/R+3UgVYwC7aGmPeM8a0NMYMMcaMyW5dEeklImMTExMDFZ7yh8iits/+4AV2jP0v74bP74ST+3PcNCI8jJ6XVmHGsCt5/aam7Dp2lh7vLmHa2r3+j1upAsofCX8vUM1jOs6ZpwqrypfBoAXQ5YU/b9haPcGr2n5YmHDb5dVZ8Hgn4koX49HJa+kzaimTV+zW3jxK5VK+h1YQkZrATGNME2c6AtgKdMYm+pXAHcaYjbndtw6tUAAd3Q4zHrVj7tfsAL3etX36vXDqfDJvzd3KZ8t3k5SaRrGocK5vWpmnezSkTIyOua8U+HEsHRGZBHQCygEHgeeMMR+JSA/gHSAcGG+MGZHL/epomQVZWhqs+Rjm/h1Sk6DT09BuGIRHeLV5aprhs+W7GL/0d3YesV0/n+p2CYM71taHqatCTwdPU+50cj/MfgI2z7TNPr1HQuVLc7WL2ev38/TXdmiG8DBhyFW1ebxrA32mriq0Qi7haw2/EDEGfptmE//ZY9D+UbjqKYiM9noXqWmGcUt28NqczQBUK1OUCfe2pk754v6KWinXCrmEn05r+IXI2WMw91lYOxHK1oVe70HN9rnaRWqaYeT8eN6etxWAfq2rMfTqusSV1oepq8JDE74KHdvn24u6J3Y7T9h6AaJL5moXG/Ym8rcp69i03/bbH9yxNv/Xo6E/olXKdUIu4WuTTiGXdAbmj4Dl/4bilaDnv6BB91zv5qf4I9zxoX0Wb5XYaMYMaMmlcaV8HKxS7hJyCT+d1vALuT2rYfowOPQbNL4Rur8BxXN39/WZCyn0G/cz6/bYm/gaVi7J5EFtiS0W6Y+IlQo6feKVCk1xLWHwIrj6GduTZ9TlsHaSVzdspYspEsH0YVfy2f1tKF0skk37T3LZi3P5cMkOPwaulDu5soavTTrqfxzaDDMegYTlUOca6PkOlK6Rq12kpRnenreV9+fHA1AkIowXejfm1lbVtBunKjC0SUcVDGlp9hm6P7xga/md/w6tB0NYeK52c+jUeV6Y/huz1tsxfUoUiWDS4LY0qRrrj6iVCihN+KpgOZEAMx+D+O8h7nLo/T5UyH0vnH0nznHnh8vZ4dyte0mlEvz3vtZULOn9PQBKuY224auCpVQ16P8l3DjOjs0zpgMsfD1Xz9MFqFKqKPOf6MRng9pQrngRNh84RZtXfmD8jztJS3NvRUipvHJlDV/b8JXXzhyBOU/BhilQoTH0eR+qtszTrkYvjOeNb7cAUK54Ed67vRlX1C3ny2iV8jtt0lEF35Y5MPOvcPoAtBsKnf4PonJ/h+3Bk+cZ+N+VbNhrb9pqXbMML/VtQoNKJXwdsVJ+oQlfFQ7nE+H7f9ix9kvXsm37tTrkaVeLth7mkUlrSDxnH614WbVSjBvQkgravq9cThO+Klx2Lobpj8DxndDyHuj6on3Wbh78sOkgwz5bw7lk+zD1G1tUZUTfphSNyl3PIKUCRRO+KnySzsLCV2DZKCheEXq+nafhGdJ9vnI3T321HoDoyDBevbEpfZtVRUT77yt30YSvCq+9q2Haw3BoIzS5Gbq/DjF5uxCbeC6ZhyetYfHWwwAUiwpnypAraFQld4O7KeVPIZfwtZeO8qmUJPjxbVj8TyhSwo7J0/RmyGPtfPOBkzw08Rd2HLb99++/shYPX1NPx+dRrhByCT+d1vCVTx3aBNOGwd5VUO86OwpnbFyedpWaZvj05108N/3PRzW/1LcJA9rmbrgHpXxNE75S6dJSYfkHMP8lkHDo+gK0vBfC8nYPYlqa4cmv1jFl9R4AqpcpxucPtKVSyWht31dBoQlfqYyO7bQPWtm5CGq0t104y9bJ8+5W7zrOy7N+Y83uEwBULVWU2Y92oGhkOFERekO7ChxN+EplxhhY8yl89wykXoBOT0O7YRAekafdpaUZPl2+i3FLdpBw7BwAlUpGs+CJTkRHhmmNXwWEJnylsnNyv32I+uaZULkZ9BkJlZrmeXeJ55KZumYvq3cdZ/qv+wC4uWUcb95ymY8CViprrkj4IlIbeAaINcbc7M02mvBVwBgDv02F2X+Dc8fhyseg498gokied5l4LpkvVibw1S972HboNLFFI2lWrRTj77ncd3ErlYHfRssUkfEickhENmSY301EtohIvIgMBzDG7DDGDMzP8ZTyGxFofAMMXQFNb7FdOMd0gIQVed5lbNFIBnWszSs3NuWO1tWpVqYYC7Yc4p7/rODBT1dz+NQFHxZAqZzl92rSBKCb5wwRCQdGAd2BRkA/EWmUz+MoFRjFysANY6D/V5B8Fj66FuYMhwun87zLFtVL81LfJjzXqxHNq5Vi/4nzzNlwgPfnb+PzlbvZfOCkDwugVNby3aQjIjWBmcaYJs50O+B5Y8x1zvTTAMaYV53pKdk16YjIYGAwQPXq1Vvu2rUrX/EplWcXTsEPL8KKsVCqOvR61z5eMZ8SzyXT5pV5nE9OA+DSuFimD7sy3/tVCgL/AJSqQILH9B6gqoiUFZExQPP0L4HMGGPGGmNaGWNalS9f3g/hKeWlIiWgxz/h3m8hvAh8cgNMHWrb+PMhtmgkK57pwk/Dr6HnpZVZvzeR5i/OpfmLcxmzaLuPglfqf+Wt/1keGGOOAkO8WddjaAX/BqWUN2q0gyE/wqLXYem79tGKPd6ERr3zvMuS0ZGUjI5kyFV1KBsThQFmrz/A7PX7aVDRjr1frUxR6lbQcfiV7/gj4e8FqnlMxznzlApdkdHQ5Tlo3BemDYUvBkDD3jbxl6iY5902qRr7x8PTD528wLcbD3DvhJUAlC4WyZp/XOuL6JUC/NOGHwFsBTpjE/1K4A5jzMYsd5IF7ZapXCk1GX56Hxa+BpFF4bpXoNkdeR6MLd2p88lsdwZk+2JVAp8t383fezYifa91KxSnY31t5lTZ81s/fBGZBHQCygEHgeeMMR+JSA/gHSAcGG+MGZHL/epomcr9jmyD6Q/D7mX2Ym7Pd6C0bwZPm7VuP0M/++WiecWLRLDhhet8sn9VcLnixqu80Bq+cr20NFj1Ecx73t681eU5uHxQngdj83TqfDJptiMPHyzezuiF2/nwrlYX7bp5tdKUjonK97FUwZFdwg/YRdvc0Iu2KmSEhUHrQVD/Opj5GMx5EjZ8ZQdjK98gX7suEf3n+Po1y8YAcP/HF1eAbmkZxz91yAblJa3hK+UrxsC6z+Hb4ZB0Bq56Etr/BcLz/2CU1DTD5gMnSUn98/P62BdrqV0uhg/v1qEa1J9CroavVEgSgctut+35c56E+S/DxmnQ532o0jxfuw4PExpXufhB7GVjoli+8xg3//uni+ZHhofxQp/G1K+oXTrVxVw5ULeI9BKRsYmJicEORancK14BbpkAt02EM4dhXGf4/jlIPufTw9zSshqXxsVSJDLsj78wEZbtOMryHUd9eixVMGiTjlL+dO4EzH0W1nwCZerYtv2a7f12uFPnk2n6/Fye6dGQQR1r++04yr20SUepYClayo6v3/RmmP4ITOgBrQZCl+chuqTPDxcdGQ7Al6sTWJOQ+RAQl1QqySOd6/n82Mr9XFnD1374qkBKOgPzR8DPo6FkFdtvv77v76R9aOJqth3MfHTPo2eSSDyXzPZXevj8uModtB++Um6SsBKmD4PDm6HprdDtNYgpG5BDvztvG2/P28r2V3oQHqaPXCyIAj1aplIqO9UuhwcWw1XDYePXMKq17bsfgMpX+gPVk1LS/H4s5T7ahq9UMEQUgauftiNuThsGU+6D9VPg+rdsc4+fFHES/sOT1hAZnnMNv37FEjzWtb7f4lGB5comHW3DV4VKWqpt158/wt6kde1L0OLufA/Glpl1e07w9NfrSU7NuYZ/9HQSx84msfPV630eh/IfbcNXKhQc3Q4zHoXfl0DNDtD7PSgTvK6V2t4fmrQNX6lQULYO3D3DPkpx/68w+go7DHNaalDCiYywSd6bXwMqNGjCV8pNRKDlPTB0OdTuZG/a+rALHPwt4KFEOsNyasIvOPSirVJuVLIK9Jtke/HMfhI+6AgdHocOf7UXfAMgwrmoe+p8is+adMLDhCIR4T7Zl8o9TfhKuZUINLkJanWyI3Aueg1+m2bv3I3LtInWp9Lv2r3itfk+22dEmPDNQ+1pGheb88rK51yZ8HU8fKU8xJSFm8bZ4RlmPmabeNo+BNc8A1ExfjtsjyaVOZeU6rMmnYMnLzB+6U72njinCT9ItJeOUqHk/En7dK1VH0HpmtDrPah9VbCj8srmAyfp9s4SRvdvQY+mlYMdToGlvXSUKiiiS0LPf8E9s0HC4ePe9rm6504EO7IchTv3FaSmubeSWdBpwlcqFNVsDw8utU/UWjMRRrWBzbOCHVW2wpwLv2kublUo6DThKxWqIotC1xdg0A8QUx4m3wFf3gOnDwU7skxpDT/4NOErFeqqNIfBC+CaZ20tf1Rr+HVyQAZjy430rp2a8IMnYAlfRGJE5L8iMk5E+gfquEoVCuGR0PFvMORHKFsPvnkAJt4CJxKCHdkftEkn+PKV8EVkvIgcEpENGeZ3E5EtIhIvIsOd2TcCU4wxg4De+TmuUioL5RvAfd9C9zdg108wui2sGAdpwb9b9s8mnSAHUojltx/+BGAk8HH6DBEJB0YBXYE9wEoRmQ7EAeud1YIzOIhShUFYOLR5AOp3s4OxzX7Cjrff+30oF7xHGzojNbB0+xEdrsELd7at4fNB6/KV8I0xi0WkZobZrYF4Y8wOABGZDPTBJv84YC3Z/LIQkcHAYIDq1avnJzylCrfSNWDAN/DrJPj2afh3e+g0HK542DYBBVjJ6EhKRkcwa91+Zq3bH/Djh5p+rav7POHn+8YrJ+HPNMY0caZvBroZY+53pgcAbYCnsL8GzgM/GmMm5rRvvfFKKR85ddDW9DdNh0qX2uEZKl8W8DDOJ6dyNkl/4HujdLFIJA/PRMjuxquADa1gjDkD3OvNujq0glI+VqIi3PaJHYtn1hMw9mq48i/Q8UmIjA5YGNGR4X+M0aMCzx+9dPYC1Tym45x5Sqlga9QHhq2Ay/rBkrdgzJWw++dgR6UCxB8JfyVQT0RqiUgUcDswPTc7MMbMMMYMjo3VAZaU8rmipaHvKLjza0i9AOO7wey/wYVTwY5M+Vl+u2VOApYBDURkj4gMNMakAMOA74BNwBfGmI253G8vERmbmJiYn/CUUtmp2xkeXGZ79KwYB6PbwbZ5wY5K+ZGOlqmUgt3LYfowOLLVNvdc9woUKxPsqFQehNxomVrDVyrAqrexd+l2eALWf2mHZ9g41XXDM6j8cWXC1zZ8pYIgogh0/jsMXgglq8KXd8Pnd8KpA8GOTPmIKxO+UiqIKjWF+3+ALi9A/Dxb21/zqdb2CwBXJnxt0lEqyMIjbD/9IUuhQmOYNhQ+6QvHfw9yYCo/XJnwtUlHKZcoVxfumQXXvwV7VtuePD+PgTS9WzYUuTLhK6VcJCwMLr8fhv4MNdrDt0/ZvvuHNgc7MpVLrkz42qSjlAvFxkH/L+HGcXA0Hj7oAIv+CanJwY5MecmVCV+bdJRyKRG49FYYugIu6QkLXoaxnWDfmmBHprzgyoSvlHK54uXhlv/A7Z/BmSMw7hr4/h+QfC7YkalsaMJXSuXdJdfD0OXQ/E5Y+q4dc//3pcGOSmXBlQlf2/CVCiFFS9mnad01DUwqTOgBM/8K508GOzKVgSsTvrbhKxWCaneCB3+CdsNg9X/s83S3zg12VMqDKxO+UipERcXAdSNg4PdQpAR8dgt8NQjOHA12ZApN+Eopf4hrBQ8shquGw8av7fAMG77S4RmCTBO+Uso/IorA1U/bxF+qGky5DybfASf1AebBoglfKeVfFRvDwHlw7cuwfQGMagOr/6u1/SBwZcLXXjpKFTDhEXDFw/DgUqh8Kcx4BP7bC47tCHZkhYorE7720lGqgCpbB+6aDj3fgf2/wugr4KeROhhbgLgy4SulCrCwMGh1Lzz0M9S+CuY+Ax91hYO/BTuyAk8TvlIqOGKrQr/JcNNHdpz9DzrCwtcgJSnYkRVYmvCVUsEjAk1vtoOxNe4LC1+FsVfZsfeVz2nCV0oFX0w5uOlD6Pc5nDsBH3WB756BpLPBjqxACVjCF5HaIvKRiEwJ1DGVUiGmQTf7oJUWd8OykfDvdrBzcbCjKjC8SvgiMl5EDonIhgzzu4nIFhGJF5Hh2e3DGLPDGDMwP8EqpQqB6Fjo9Q7cPRMQ231zxqNwXrtp55e3NfwJQDfPGSISDowCugONgH4i0khEmorIzAx/FXwatVKq4KvVwQ7GdsXD8MvH9oatLXOCHVVI8yrhG2MWA8cyzG4NxDs19yRgMtDHGLPeGNMzw98hbwMSkcEiskpEVh0+fNjrgiilCqCoYvYO3fvnQdEyMOl2mDLQPnRF5Vp+2vCrAgke03uceZkSkbIiMgZoLiJPZ7WeMWasMaaVMaZV+fLl8xGeUqrAqNoSBi+Eq5+B36bByMth3Zc6PEMuBeyirTHmqDFmiDGmjjHm1ezW1aEVlFL/IyIKrnoShiyBMrXh6/vhs9sgcU+wIwsZ+Un4e4FqHtNxzjyllPKfCg1h4Fy47lX4fQmMagurxkNaWrAjc738JPyVQD0RqSUiUcDtwHRfBKVj6SilshUWDu0eshd1q7aAmY/Z3jxHtwc7MlfztlvmJGAZ0EBE9ojIQGNMCjAM+A7YBHxhjNnoi6C0SUcp5ZUyteyzdHu/DwfWw7+vsA9TT00JdmSuJMbFFz1atWplVq1aFewwlFKh4OR+mPU4bJkFVZpD75FQqUmwowo4EVltjGmV2TJXDq2gNXylVK6VrAy3T4RbJtgLuWOvgvkjIOVCsCNzDVcmfG3DV0rliQg0vsEOxtbkZlj8hh2FM2FlsCNzBVcmfKWUypdiZeDGD6D/FLhw2o63/+3TkHQm2JEFlSsTvjbpKKV8ol5XeGgZXD4Qfh4No9vZ5+oWUq5M+Nqko5TymeiScP1bcM9sCIuAT/rCtKF2GOZCxpUJXymlfK5me/sQ9fZ/gbWT7GBsm2YGO6qAcmXC1yYdpZRfRBaFri/AoB8gpjx83h++uBtOez2+Y0hzZcLXJh2llF9VaQ6DF8A1f4cts2FUa/h1coEfjM2VCV8ppfwuPBI6PgFDfoSy9eCbB2DizXAiIedtQ5QmfKVU4Va+Adz3LXR/A3Ytg9FtYcW4AjkYmysTvrbhK6UCKiwc2jxgu3DGXQ6zn4AJPeDItmBH5lOuTPjahq+UCorSNWDAN9BnNBz6Df7dHpb8q8AMxubKhK+UUkEjAs37w9CVUP9a+OEF+PAa2L8u2JHlmyZ8pZTKTImKcNuncOvHdiTOsZ3ghxch+XywI8szTfhKKZWdRn1g6HK49DZY8haMuRJ2/xzsqPJEE75SSuWkWBm44d9w51eQch7Gd4PZT9qB2UKIKxO+9tJRSrlS3S62J0/rwbBirB2MLf6HYEflNVcmfO2lo5RyrSIloMcbtu9+RBH49EaY+hCcPRbsyHLkyoSvlFKuV72tvUu3w+N2WIZRbeC3acGOKlua8JVSKq8io6HzP2DwQihRCb64Cz6/E04dCHZkmdKEr5RS+VX5Uhg0Hzo/B1vn2sHY1kx03WBsmvCVUsoXwiOhw1/tmPsVGsG0h+CTG+D4rmBH9oeAJXwR6Ssi40TkcxG5NlDHVUqpgCpXzz5dq8ebsGel7cmz/ANXDMbmVcIXkfEickhENmSY301EtohIvIgMz24fxpipxphBwBDgtryHrJRSLhcWBq0H2S6cNdrBnCfhP93g8JbghuXlehOAbp4zRCQcGAV0BxoB/USkkYg0FZGZGf4qeGz6rLOdUkoVbKWqQ/8pcMMHcGSrvUt38T8hNTko4UR4s5IxZrGI1MwwuzUQb4zZASAik4E+xphXgZ4Z9yEiArwGzDHG/JLVsURkMDAYoHr16t6Ep5RS7iUCl90Oda6B2X+D+S/DxmnQZyRUaRbQUPLThl8V8Hw0zB5nXlYeBroAN4vIkKxWMsaMNca0Msa0Kl++fD7CU0opFyleAW79rx2Q7cwhGHcNfP8cJJ8LWAgBu2hrjHnPGNPSGDPEGDMmu3V1aAWlVIHVsJcdjK3ZHbD0HdvMs+ungBw6Pwl/L1DNYzrOmaeUUio7RUvbJp0BUyE1Cf7THWY9DudP+vWw+Un4K4F6IlJLRKKA24HpvghKx9JRShUKda6Gh36Gtg/Byo9sF85t3/vtcN52y5wELAMaiMgeERlojEkBhgHfAZuAL4wxG30RlDbpKKUKjagY6PYqDJxr/z/xZvj6ATh3wueHEuOyW389tWrVyqxatSrYYSilVGCkXIDFb8KGr+CBxVCkeK53ISKrjTGtMl3mxoQvIr2AXnXr1h20bVvBemq8UkrlKOWCHXo5D7JL+K4cS0fb8JVShVoek31OXJnwlVJK+Z4rE75etFVKKd9zZcLXJh2llPI9VyZ8pZRSvufKhK9NOkop5XuuTPjapKOUUr7nyoSvlFLK91x541U6EUkEPO+8igUSvfx/OeBIHg/tub/cLs+4zNvpQJchu3Uym5/V653VMreXIeN0dmWAvJcj2GXw/H8wPhMFoQyZLXPz57qeMSbz5hFjjGv/gLFZTef0f2CVr46bm+XZxZzddKDLkN06mc3P6vXOJnZXl8HLc8hzXp7KEewyBOq9KMhlyCnm7Kbd9Lk2xri+SWdGNtPe/N9Xx83N8uxizm460GXIbp3M5mcXX2bL3F6GjNMFtQzexpCTvH4mCkIZMlsWip9rdzfp5IeIrDJZjCcRKrQM7lEQyqFlcIdglsHtNfz8GBvsAHxAy+AeBaEcWgZ3CFoZCmwNXyml1MUKcg1fKaWUB034SilVSGjCV0qpQqJQJHwRiRGR/4rIOBHpH+x48kpEaovIRyIyJdix5JWI9HXeh89F5Npgx5MXItJQRMaIyBQReTDY8eSV87lYJSI9gx1LXolIJxFZ4rwfnYIdT16ISJiIjBCR90Xkbn8eK2QTvoiMF5FDIrIhw/xuIrJFROJFZLgz+0ZgijFmENA74MFmIzflMMbsMMYMDE6kWctlGaY678MQ4LZgxJuZXJZhkzFmCHAr0D4Y8WYml58JgKeALwIbZc5yWQ4DnAaigT2BjjUruSxDHyAOSMbfZcjrHV/B/gM6Ai2ADR7zwoHtQG0gCvgVaAQ8DTRz1vks2LHntRwey6cEO24flOEtoEWwY89rGbAVhznAHcGOPS9lALoCtwP3AD2DHXs+yhHmLK8ITAx27Hksw3DgAWcdv362Q7aGb4xZDBzLMLs1EG9sTTgJmIz99tyD/QYFl/2qyWU5XCk3ZRDrdWCOMeaXQMealdy+D8aY6caY7oBrmghzWYZOQFvgDmCQiLjmc5Gbchhj0pzlxwH/PAg2D/KQn44766T6M64If+48CKoCCR7Te4A2wHvASBG5Ht/c3uxvmZZDRMoCI4DmIvK0MebVoETnnazei4eBLkCsiNQ1xowJRnBeyup96IRtJiwCzA58WLmSaRmMMcMAROQe4IhH4nSrrN6LG4HrgFLAyCDElRtZfSbeBd4XkQ7AYn8GUNASfqaMMWeAe4MdR34ZY45i275DljHmPewXcMgyxiwEFgY5DJ8wxkwIdgz5YYz5Gvg62HHkhzHmLBCQa3Ou+RnnI3uBah7Tcc68UFMQyqFlcIeCUAYoGOUIehkKWsJfCdQTkVoiEoW9KDU9yDHlRUEoh5bBHQpCGaBglCP4ZQj21ex8XAWfBOznz65MA535PYCt2KvhzwQ7zsJQDi2DO/4KQhkKSjncWgYdPE0ppQqJgtako5RSKgua8JVSqpDQhK+UUoWEJnyllCokNOErpVQhoQlfKaUKCU34SilVSGjCV0qpQkITvlJKFRL/DwT+KGvxnPbbAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "M = COUNTS['и'] # вместо \"и\" самое часто встречающееся слово\n",
    "yscale('log'); \n",
    "xscale('log'); \n",
    "title('Частота n-того наиболее частого слова и линия 1/n.')\n",
    "plot([c for (w, c) in COUNTS.most_common()])\n",
    "plot([M/i for i in range(1, len(COUNTS)+1)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------------------------------------------------------------------\n",
    "Но чем плох Bag of words? Тем, что для него кошк|у| != кошк|а| - два разных слова. Как это побороть? А что если мы вырежем все окончания и суффиксы, одним словом сделаем **стемминг Портера**. Тем самым мы будем считать что слово одно, а его форм много, но они уже не считаются за разные слова."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [],
   "source": [
    "#что-то реализую\n",
    "WORDS += WORDS2\n",
    "#WORDS = [w for w in WORDS if len(w) > 2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Но тогда возникает проблема, существуют слова, для которых стемминг не приведет к одному и тому же слову, например \"шел и идти\". Тогда на помощь приходят морфологические анализаторы, которые приводят слово к начальной форме (происходит лемматизация)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "красивый мама красиво мыть рама\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pymystem3 import Mystem\n",
    "text = \"Красивая мама красиво мыла раму\" \n",
    "m = Mystem()\n",
    "lemmas = m.lemmatize(text)\n",
    "print(''.join(lemmas))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'analysis': [{'lex': 'идти', 'wt': 1, 'gr': 'V,несов,нп=прош,ед,изъяв,муж'}],\n",
       "  'text': 'шел'},\n",
       " {'text': '\\n'}]"
      ]
     },
     "execution_count": 288,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pymystem3\n",
    "m1=pymystem3.Mystem()\n",
    "\n",
    "m1.analyze('шел')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Но в таком случае возникает проблема, что делать с \"стекло\" или \"эти типы стали есть в прокатном цехе\".\n",
    "Морфологические анализаторы иногда делают ошибки и без контекста им не разобраться.\n",
    "Но pymystem вроде как +- справляются"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Задача: Проверка Правописания***\n",
    "\n",
    "Для данного слова w нужно найти наиболее вероятную правку *c = correct(w).*\n",
    "\n",
    "**Подход:** Найти все кандидаты c, достаточно близкие к w. Выбрать наиболее вероятный из них.\n",
    "\n",
    "Осталось понять, что такое близкие и наиболее вероятный.\n",
    "\n",
    "Применим наивный подход: всегда будем брать более близкое слово, если проверки на близость недостаточно, берем слово с максимальной частотой из WORDS. \n",
    "Сейчас мы будем измерять близость с помощью расстояния Левенштейна: минимального необходимого количества удалений, перестановок, вставок, и замен символов, необходимых чтобы одно слово превратить в другое. Конечно же это не единственный возможный подход. Тогда остается определить функцию *correct(w):*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correct(word):\n",
    "    \"Поиск лучшего исправления ошибки для данного слова.\"\n",
    "    # предрассчитать edit_distance==0, затем 1, затем 2; в противном случае оставить слово \"как есть\".\n",
    "    candidates = (known(edits0(word)) or \n",
    "                  known(edits1(word)) or \n",
    "                  known(edits2(word)) or \n",
    "                  [word])\n",
    "    return max(candidates, key=COUNTS.get)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Функции known и edits0 простые; функция edits2 iлегко получается из функции edits1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [],
   "source": [
    "def known(words):\n",
    "    \"Вернуть подмножество слов, которое есть в нашем словаре.\"\n",
    "    return {w for w in words if w in COUNTS}\n",
    "\n",
    "def edits0(word): \n",
    "    \"Вернуть все строки, которые находятся на edit_distance == 0 от word (т.е., просто само слово).\"\n",
    "    return {word}\n",
    "\n",
    "def edits2(word):\n",
    "    \"Вернуть все строки, которые находятся на edit_distance == 2 от word.\"\n",
    "    return {e2 for e1 in edits1(word) for e2 in edits1(e1)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Расстояние Левенштейна:\n",
    "\n",
    "Исходное слово --> поиск\n",
    "\n",
    "Вставка --> происк\n",
    "\n",
    "Удаление --> п_иск\n",
    "\n",
    "Замена --> поеск\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [],
   "source": [
    "def edits1(word):\n",
    "    \"Возвращает список всех строк на расстоянии edit_distance == 1 от word.\"\n",
    "    pairs      = splits(word)\n",
    "    deletes    = [a+b[1:]           for (a, b) in pairs if b]\n",
    "    transposes = [a+b[1]+b[0]+b[2:] for (a, b) in pairs if len(b) > 1]\n",
    "    replaces   = [a+c+b[1:]         for (a, b) in pairs for c in alphabet if b]\n",
    "    inserts    = [a+c+b             for (a, b) in pairs for c in alphabet]\n",
    "    return set(deletes + transposes + replaces + inserts)\n",
    "\n",
    "def splits(word):\n",
    "    \"Возвращает список всех возможных разбиений слова на пару (a, b).\"\n",
    "    return [(word[:i], word[i:]) \n",
    "            for i in range(len(word)+1)]\n",
    "\n",
    "alphabet = 'абвгдеёжзийклмнопрстуфхцчшщъыьяюя'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "33"
      ]
     },
     "execution_count": 293,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(alphabet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'атец'}\n",
      "{'атрец', 'атшц', 'аткец', 'ятец', 'атем', 'атфец', 'атмец', 'атебц', 'атех', 'атеец', 'ыатец', 'атеце', 'отец', 'атпц', 'цтец', 'ачтец', 'ацец', 'атецл', 'азтец', 'атег', 'оатец', 'атце', 'атёц', 'атет', 'атъец', 'дтец', 'атдц', 'аатец', 'афец', 'алтец', 'атнец', 'аутец', 'атеж', 'атецё', 'асец', 'аъец', 'ажтец', 'атецм', 'ътец', 'атеь', 'атецж', 'атез', 'атезц', 'аыец', 'атечц', 'атецт', 'атедц', 'артец', 'ъатец', 'атецг', 'бтец', 'атаец', 'аткц', 'атецы', 'атеяц', 'цатец', 'атецв', 'атеы', 'ёатец', 'атоец', 'аеец', 'ытец', 'азец', 'мтец', 'нтец', 'аытец', 'атещ', 'атев', 'аетец', 'атець', 'иатец', 'атпец', 'атюец', 'атц', 'атед', 'атецъ', 'атежц', 'атею', 'ажец', 'адтец', 'атбец', 'атецб', 'аюец', 'ателц', 'анец', 'атгец', 'ааец', 'амец', 'атес', 'аятец', 'атеа', 'антец', 'ьтец', 'уатец', 'ахец', 'атфц', 'апец', 'атеё', 'атецу', 'атехц', 'итец', 'атек', 'атеоц', 'атецю', 'атзец', 'атея', 'атеиц', 'атсец', 'атецп', 'агец', 'атнц', 'катец', 'аштец', 'атецх', 'атьц', 'астец', 'арец', 'атевц', 'атей', 'атер', 'атецщ', 'гтец', 'зтец', 'хтец', 'атыец', 'авец', 'аяец', 'адец', 'жтец', 'затец', 'ашец', 'аптец', 'атео', 'аьец', 'чтец', 'сатец', 'ател', 'ащтец', 'еатец', 'ацтец', 'атещц', 'амтец', 'ачец', 'атчц', 'шатец', 'атецк', 'атец', 'аитец', 'атеёц', 'атиц', 'атецс', 'атекц', 'аиец', 'атмц', 'атлец', 'ттец', 'атеу', 'татец', 'хатец', 'атжец', 'атлц', 'атее', 'атшец', 'аетц', 'батец', 'атеъ', 'натец', 'юатец', 'датец', 'ютец', 'атеац', 'атеьц', 'йтец', 'атюц', 'атдец', 'алец', 'атёец', 'атеб', 'атегц', 'атецз', 'атбц', 'аотец', 'афтец', 'атепц', 'атрц', 'лтец', 'аьтец', 'атеци', 'птец', 'атецш', 'айец', 'атеуц', 'атецч', 'аёец', 'автец', 'атеф', 'атщец', 'атетц', 'атецд', 'атяц', 'аттец', 'атеъц', 'атац', 'матец', 'ктец', 'атжц', 'латец', 'аттц', 'атгц', 'атьец', 'ауец', 'атен', 'ащец', 'атесц', 'атефц', 'атеч', 'стец', 'атуец', 'аоец', 'атеш', 'тец', 'атецф', 'аец', 'атъц', 'атеп', 'аютец', 'патец', 'айтец', 'атыц', 'атецн', 'йатец', 'ватец', 'атйц', 'ратец', 'щатец', 'атцц', 'атеи', 'аётец', 'атуц', 'агтец', 'жатец', 'ахтец', 'атцец', 'атеця', 'атецц', 'атецо', 'ртец', 'атемц', 'утец', 'атвец', 'атеца', 'атеюц', 'штец', 'етец', 'актец', 'атеыц', 'атешц', 'атецр', 'атйец', 'атщц', 'абтец', 'щтец', 'атсц', 'атхц', 'таец', 'атоц', 'фатец', 'аътец', 'яатец', 'атчец', 'атенц', 'втец', 'атвц', 'чатец', 'ате', 'атиец', 'фтец', 'атерц', 'атяец', 'абец', 'атзц', 'атецй', 'атхец', 'ьатец', 'акец', 'ётец', 'атейц', 'гатец'}\n",
      "36956\n"
     ]
    }
   ],
   "source": [
    "print(edits0('атец'))\n",
    "print(edits1('атец'))\n",
    "print(len(edits2('атец')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ана', 'ни', 'зочит', 'в', 'шшколу']"
      ]
     },
     "execution_count": 295,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens('Ана ни зочит в шшколу.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['она', 'ре', 'хочет', 'в', 'школу']"
      ]
     },
     "execution_count": 300,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(map(correct, tokens('Ога  хочит в шшколу.')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Теория: От счетчика слов к вероятностям последовательностей слов***\n",
    "\n",
    "Нам нужно научиться подсчитывать вероятности слов, P(w). Делать мы это будем с помощью функции pdist, которая на вход принимает Counter (наш мешок слов) и возвращает функцию, выполняющую роль вероятностного распределения на множестве всех возможных слов. В вероятностном распределении вероятность каждого слова лежит между 0 и 1, и сложение вероятностей всех слов дает 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pdist(counter):\n",
    "    \"Превращает частоты из Counter в вероятностное распределение.\"\n",
    "    N = sum(list(counter.values()))\n",
    "    return lambda x: counter[x]/N\n",
    "\n",
    "P = pdist(COUNTS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0 то\n",
      "5.543719228711097e-05 мать\n",
      "1.16709878499181e-06 пирогов\n",
      "5.83549392495905e-07 напечет\n",
      "0.0 то\n",
      "3.5012963549754295e-06 бабушка\n",
      "0.0 с\n",
      "5.83549392495905e-07 булочками\n",
      "2.33419756998362e-06 приедет\n"
     ]
    }
   ],
   "source": [
    "for w in tokens('То мать пирогов напечет, то бабушка с булочками приедет'):\n",
    "    print(P(w), w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Итак, что же такое вероятность последовательности слов? Используем определение совместной вероятности:\n",
    "$P(w_1 ... w_n) = P(w_1)*P(w_2|w_1)*P(w_3|w_1w_2)...*...P(w_n|w_1...w_n-1)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Модель мешка слов подразумевает, что каждое слово из мешка достается независимо от других. Это дает нам упрощенную аппроксимацию:\n",
    "$P(w_1 ... w_n) = P(w_1)*P(w_2)*P(w_3)...*...P(w_n)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Известный статистик Джордж Бокс сказал Все модели неверны, но некоторые полезны."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как же нам посчитать $P(w_1 ... w_n)$? Мы будем использовать другое название, чтобы не обманывать себя, Pwords вместо P, и посчитаем ее как произведение индивидуальных вероятностей:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Pwords(words):\n",
    "    \"Вероятности слов, при условии, что они независимы.\"\n",
    "    return product(P(w) for w in words)\n",
    "\n",
    "def product(nums):\n",
    "    \"Перемножим числа.  (Это как `sum`, только с умножением.)\"\n",
    "    result = 1\n",
    "    for x in nums:\n",
    "        result *= x\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0 тест\n",
      "0.0003806772696482654 дом\n",
      "0.0 Крыжить\n"
     ]
    }
   ],
   "source": [
    "tests = ['тест', \n",
    "         'дом',\n",
    "         'Крыжить']\n",
    "\n",
    "for test in tests:\n",
    "    print(Pwords(tokens(test)), test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "кажется, присвоить последнюю вероятность 0, неправильно; Она просто должна быть маленькой. К этому вернемся попозже. Ну а другие вероятности кажутся +- адекватными."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Задача: Разбиение слов на сегменты***\n",
    "Ситуации, когда слова пишутся слитно (по ошибке или нет)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Подход 2: Делаем одно разбиение - на первое слово и все остальное. Если предположить, что слова независимы, можно максимизировать вероятность первого слова + лучшего разбиения оставшихся букв.\n",
    "\n",
    "<code>\n",
    "assert segment('фотоальбом') == ['фото', 'альбом']\n",
    "segment('choosespain') ==\n",
    "   max(Pwords(['ф'] + segment('отоальбом')),\n",
    "       Pwords(['фо'] + segment('тоальбом')),\n",
    "       Pwords(['фот'] + segment('оальбом')),\n",
    "       Pwords(['фото'] + segment('альбом')),\n",
    "       ...\n",
    "       Pwords(['фотоальбом'] + segment('')))\n",
    "</code>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Чтобы сделать это хоть сколько-нибудь эффективным, нужно избежать слишком большого числа пересчетов оставшейся части слова. Это можно сделать или с помощью динамического программирования или с помощью мемоизации aka кэширования. Кроме того, для первого слова не обязательно брать все возможные варианты разбиений - мы можем установить максимальную длину. Какой она должна быть? Чуть большей, чем длина самого длинного слова, которое мы видели."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "def memo(f):\n",
    "    \"Запомнить результаты исполнения функции f, чьи аргументы args должны быть хешируемыми.\"\n",
    "    cache = {}\n",
    "    def fmemo(*args):\n",
    "        if args not in cache:\n",
    "            cache[args] = f(*args)\n",
    "        return cache[args]\n",
    "    fmemo.cache = cache\n",
    "    return fmemo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(len(w) for w in COUNTS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "def splits(text, start=0, L=20):\n",
    "    \"Вернуть список всех пар (a, b); start <= len(a) <= L.\"\n",
    "    return [(text[:i], text[i:]) \n",
    "            for i in range(start, min(len(text), L)+1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('', 'слово'), ('с', 'лово'), ('сл', 'ово'), ('сло', 'во'), ('слов', 'о'), ('слово', '')]\n",
      "[('дли', 'нныйтекст'), ('длин', 'ныйтекст'), ('длинн', 'ыйтекст'), ('длинны', 'йтекст'), ('длинный', 'текст')]\n"
     ]
    }
   ],
   "source": [
    "print(splits('слово'))\n",
    "print(splits('длинныйтекст', 3, 7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [],
   "source": [
    "@memo\n",
    "def segment(text):\n",
    "    \"Вернуть список слов, который является наиболее вероятной сегментацией нашего текста.\"\n",
    "    if not text: \n",
    "        return []\n",
    "    else:\n",
    "        candidates = ([first] + segment(rest) \n",
    "                      for (first, rest) in splits(text, 1))\n",
    "        return max(candidates, key=Pwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "segment('сдырочкой')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "decl = ('предложениеэтосложнаясинтаксическаяконструкция')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['п', 'р', 'е', 'д', 'л', 'о', 'ж', 'е', 'н', 'и', 'е', 'э', 'т', 'о', 'с', 'л', 'о', 'ж', 'н', 'а', 'я', 'с', 'и', 'н', 'т', 'а', 'к', 'с', 'и', 'ч', 'е', 'с', 'к', 'а', 'я', 'к', 'о', 'н', 'с', 'т', 'р', 'у', 'к', 'ц', 'и', 'я']\n"
     ]
    }
   ],
   "source": [
    "print(segment(decl))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Насколько дорого превращать одно слово в другое?***\n",
    "\n",
    "Динамическое программирование позволяет разбить задачу на подзадачи, решив которые можно скомпоновать финальное решение. Мы будем пытаться превратить строку $source[0..i]$ в строку $target[0..j]$, мы сосчитаем все возможные комбинации подстрок $substrings[i, j]$ и рассчитаем их *edit_distance* до нашей исходной. Мы будем сохранять результаты в таблицу и переиспользовать их для расчета дальнейших изменений.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def min_edit_distance(source, target, ins_cost = 1, del_cost = 1, rep_cost = 2):\n",
    "    '''\n",
    "    Input: \n",
    "        source: строка-исходник\n",
    "        target: строка, в которую мы должны исходник превратить\n",
    "        ins_cost: цена вставки\n",
    "        del_cost: цена удаления\n",
    "        rep_cost: цена замены буквы\n",
    "    Output:\n",
    "        D: матрица размера len(source)+1 на len(target)+1 содержащая минимальные расстояния edit_distance\n",
    "        med: минимальное расстояние edit_distance (med), необходимое, \n",
    "        чтобы превратить строку source в строку target\n",
    "    '''\n",
    "    # стоимость удаления и вставки = 1\n",
    "    m = len(source)\n",
    "    n = len(target)\n",
    "\n",
    "    # Заткнем нашу матрицу нулями\n",
    "    D = np.zeros((m+1, n+1), dtype=int) \n",
    "    \n",
    "    # Заполним первую колонку\n",
    "    for row in range(1,m+1): \n",
    "        D[row,0] = D[row-1,0] + del_cost\n",
    "        \n",
    "    # Заполним первую строку\n",
    "    for col in range(1,n+1): \n",
    "        D[0,col] = D[0,col-1] + ins_cost\n",
    "        \n",
    "    # Теперь пойдем от 1 к m-той строке\n",
    "    for row in range(1,m+1): \n",
    "        \n",
    "        # итерируемся по колонкам от 1 до n\n",
    "        for col in range(1,n+1):\n",
    "            \n",
    "            # r_cost - стоимость замены\n",
    "            r_cost = rep_cost\n",
    "            \n",
    "            # Совпадает ли буква исходного слова из предыдущей строки\n",
    "            # с буквой целевого слова из предыдущей колонки, \n",
    "            if source[row-1] == target[col-1]:\n",
    "                # Если они не нужны, то замена не нужна -> стоимость = 0\n",
    "                r_cost = 0\n",
    "                \n",
    "            # Обновляем значение ячейки на базе предыдущих значений \n",
    "            # Считаем D[i,j] как минимум из трех возможных стоимостей (как в формуле выше)\n",
    "            D[row,col] = min([D[row-1,col]+del_cost, D[row,col-1]+ins_cost, D[row-1,col-1]+r_cost])\n",
    "          \n",
    "    # установить edit_distance в значение из правого нижнего угла\n",
    "    med = D[m,n]\n",
    "    \n",
    "\n",
    "    return D, med"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Расстояние:  3 \n",
      "\n",
      "   #  к  и  т  ы\n",
      "#  0  1  2  3  4\n",
      "к  1  0  1  2  3\n",
      "р  2  1  2  3  4\n",
      "о  3  2  3  4  5\n",
      "т  4  3  4  3  4\n",
      "ы  5  4  5  4  3\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "source =  'кроты'\n",
    "target = 'киты'\n",
    "matrix, min_edits = min_edit_distance(source, target)\n",
    "\n",
    "print(\"Расстояние: \",min_edits, \"\\n\")\n",
    "\n",
    "idx = list('#' + source)\n",
    "cols = list('#' + target)\n",
    "df = pd.DataFrame(matrix, index=idx, columns= cols)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Нам мало миллионов слов в \"обучающей выборке\" давайте перейдем к МИЛЛИАРДАМ слов. Получив такой огромный объем информации, можно перейти к анализу пар последоваительных слов, не ожидая, что вероятности слишком часто будут обнуляться (представьте себе, сколько в языке может быть грамматически корректных сочетаний из двух слов). Мы вновь позаимствуем уже собранные данные у мистера Норвига. Лежат они на его сайте в формате \"word \\t count\" для отдельных слов и в формате \"word1 word2 \\t count\" для биграмм. Считаем их и упакуем в наши словари с вероятностями:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_counts(text, sep='\\t'):\n",
    "    \"\"\"Возвращает Counter, полученный из пар ключ-значение,в каждой строке файла.\"\"\"\n",
    "    C = Counter()\n",
    "    for i in [l.split('\\t') for l in text.split('\\n')][:-1]:\n",
    "        key, count = i\n",
    "        C[key] = int(count)\n",
    "    return C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "COUNTS1 = load_counts(requests.get('https://www.norvig.com/ngrams/count_1w.txt').text) #список слов с частотой их встречания\n",
    "#COUNTS2 = список слов с частотой их встречания\n",
    "\n",
    "P1w = pdist(COUNTS1)\n",
    "#P2w = pdist(COUNTS2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(COUNTS1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(COUNTS1), sum(list(COUNTS1.values()))/1e9)\n",
    "print(len(COUNTS2), sum(list(COUNTS2.values()))/1e9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ужас! Сотни миллиардов. Но мы справились."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "COUNTS2.most_common(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Сегментация с помощью биграмм**\n",
    "Чуть менее неправильная аппроксимация:\n",
    "$P(w_1...w_n)=P(w_1)*P(w_2|w_1)*P(w_3|w_2)*...*...P(w_n|w_n-1)$\n",
    "Эта штука называется биграммной моделью. Представьте, что вы взяли текст, достали из него все возможные пары подряд идущих слов и положили каждую пару в мешок, промаркированный ПЕРВЫМ словом из пары. После этого, чтобы сгенерировать кусок текста, мы берем первое слово из исходного мешка слов , а каждое следующее слово вынимаем из соответствующего мешка биграмм.\n",
    "\n",
    "Начнем с определения вероятности текущего слова при условии данного предыдущего слова из Counter:\n",
    "\n",
    "Отмечу, что для английского языка биграммная модель будет выглядеть так:\n",
    "$P(w_1...w_n)=P(w_1)*P(w_2|w_1)*P(w_3|w_2)*...*...P(w_n|w_n-1)$ условная вероятность слова при условии предыдущего слова определяется так:\n",
    "$P(w_n|w_n-1)=P(w_n-1w_n)/P(w_n-1)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Pwords2(words, prev='<S>'):\n",
    "    \"Вероятность последовательности слов с помощью биграммной модели(при условии предыдущего слова).\"\n",
    "    return product(cPword(w, (prev if (i == 0) else words[i-1]) )\n",
    "                   for (i, w) in enumerate(words))\n",
    "\n",
    "# Перепишем Pwords на большой словарь P1w вместо Pword\n",
    "def Pwords(words):\n",
    "    \"Вероятности слов при условии их независимости.\"\n",
    "    return product(P1w(w) for w in words)\n",
    "\n",
    "def cPword(word, prev):\n",
    "    \"Условная вероятность слова при условии предыдущего.\"\n",
    "    bigram = prev + ' ' + word\n",
    "    if P2w(bigram) > 0 and P1w(prev) > 0:\n",
    "        return P2w(bigram) / P1w(prev)\n",
    "    else: # если что-то не встретилось, поставим среднее между P1w и 0\n",
    "        return P1w(word) / 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(Pwords(tokens('Он приехал в город')))\n",
    "print(Pwords2(tokens('Приехал он в город')))\n",
    "print(Pwords2(tokens('Город в он приехал')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Чтобы сделать segment2, скопируем segment, добавим в аргументы предыдущий токен, а вероятности будем считать с помощью Pwords2 вместо Pwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "@memo \n",
    "def segment2(text, prev='<S>'): \n",
    "    \"Возвращает наилучшее разбиение текста, используя статистику биграмм.\" \n",
    "    if not text: \n",
    "        return []\n",
    "    else:\n",
    "        candidates = ([first] + segment2(rest, first) \n",
    "                      for (first, rest) in splits(text, 1))\n",
    "        return max(candidates, key=lambda words: Pwords2(words, prev))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(segment2('фотоальбом'))\n",
    "print(segment2('поехалаонанет'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ну и что теперь? Биграммная модель вроде бы лучше, но не очень. Сотен миллиардов слов все равно может быть недостаточно. (Ну а почему бы не триллион слов?)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Валидация***\n",
    "\n",
    "До настоящего момента мы пытались интуитивно оценить результаты нашей работы. Тем не менее, никаких численных оценок качества мы пока не получили. Важно понимать, что без четких метрик слова \"плохо\"/\"хорошо\" не имеют никакого смысла. Более того - мы даже не можем четко ответить, было ли наше обновление модели в лучшую сторону или худшую. Обычно при построении неких прогностических моделей данные разбиваются на три части:\n",
    "\n",
    "**Обучающая выборка**: То, что мы использовали для создания модели исправления ошибок; У нас это был файл words.txt file.\n",
    "**Тестовая выборка**: Набор данных, который можно использовать для оценки качества вашей модели по ходу разработки.\n",
    "**Валидационная выборка**: Набор данных, который мы используем для оценки работы программы после того как программа готова. Тестовая выборка для этого быть использована не может—Стоит разработчику посмотреть на результаты на тестовой выборке, она уже \"испорчена\". В принципе, программист может изменить программу так, чтобы она \"подгонялась\" под тестовую выборку, а это будет \"переобучением\". Вот почему нам нужен отдельный набор тестов, который рассматривается только после завершения разработки..\n",
    "\n",
    "Для нашей программы обучающая выборка - словарь слов. Сделаем валидационную выборку."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_segmenter(segmenter, tests):\n",
    "    \"Оценка сегментатора на тестовых данных; вывести на печать ошибки; вернуть долю верно разбитого.\"\n",
    "    return sum([test_one_segment(segmenter, test) \n",
    "               for test in tests]), len(tests)\n",
    "\n",
    "def test_one_segment(segmenter, test):\n",
    "    words = tokens(test)\n",
    "    result = segmenter(''.join(words))\n",
    "    correct = (result == words)\n",
    "    if not correct:\n",
    "        print('expected', words)\n",
    "        print('got     ', result) \n",
    "    return correct\n",
    "\n",
    "proverbs = (\"\"\"Унас под крыльцом живут ежи. По вечерам вся смья выходит гулять.\"\"\".splitlines())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "expected ['унас', 'под', 'крыльцом', 'живут', 'ежи', 'по', 'вечерам', 'вся', 'смья', 'выходит', 'гулять']\n",
      "got      ['унасподкрыльцомживутежиповечерамвсясмьявыходитгулять']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0, 1)"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_segmenter(segment, proverbs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Место для анекдота про Лапласа**\n",
    "\n",
    "Однажды французского математика Лапласа спросили: \"Какова вероятность того, что Солнце завтра взойдет?\". Из данных, что оно из  ближайших дней взошло n раз следует оценка максимального правдоподобия n/n = 1. Но Лапласу хотелось чуть сбалансировать оценку на шанс того, что завтра Солнце может и не взойти, поэтому он дал оценку (n+1)/(n+2).\n",
    "\n",
    "То, что мы знаем, ограничено, а то, чего мы не знаем,-бесконечно\n",
    "— Пьер Симон Лаплас, 1749-1827"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pdist_additive_smoothed(counter, c=1):\n",
    "    \"\"\"Вероятность слова, при условии данных из Counter'a.\n",
    "    добавляем c к частоте каждого слова + слово 'unknown'.\"\"\"\n",
    "    N = sum(list(counter.values()))          # суммарное кол-во слов\n",
    "    Nplus = N + c * (len(counter) + 1) # кол-во слов + сглаживание\n",
    "    return lambda word: (counter[word] + c) / Nplus \n",
    "\n",
    "P1w = pdist_additive_smoothed(COUNTS1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь еще одна проблема ... у нас появились незнакомые слова с ненулевой вероятностью. А что если 10-12 - приемлемая вероятность для слов нашего текста: то есть, если я читаю новый текст, вероятность того, что следующее слово мне незнакомо, может быть порядка 10-12. Но если я случайно генерирую 20-буквенный последовательности, вероятность того, что одна из них будет реальным словом намного меньше чем 10-12.\n",
    "\n",
    "У нас две проблемы:\n",
    "\n",
    "Во-первых, у нас нет четкой модели для неизвестных слов. Мы говорим \"неизвестное слово\", но не различаем более вероятные неизвестные слова и менее вероятные неизвестные слова. Ну, например, вероятнее ли 8-буквенное неизвестное слово чем 20-буквенное неизвестное слово?\n",
    "\n",
    "Во-вторых, мы не берем в расчет информацию из частей неизвестных слов. Например, \"unglobulate\" явно должно быть более вероятным чем \"zxfkogultae\".\n",
    "\n",
    "Для нашего следующего подхода мы используем идеи метода Гуда - Тьюринга. Он оценивает вероятности слов, не встретившихся в нашем Counter'е, на основании вероятностей слов, встретившихся единожды (Можно туда же подключить вероятности для встретившихся 2 раза и т.д.).\n",
    "\n",
    "Итак, сколько слов встретилось 1 раз в COUNTS? (В COUNTS1 ни одного такого слова нет.) И какие длины у этих слов? Давайте посмотрим:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(8, 4418),\n",
       " (7, 4190),\n",
       " (9, 3967),\n",
       " (10, 3273),\n",
       " (6, 2820),\n",
       " (11, 2472),\n",
       " (12, 1615),\n",
       " (5, 1612),\n",
       " (13, 910),\n",
       " (14, 575),\n",
       " (4, 574),\n",
       " (15, 313),\n",
       " (3, 191),\n",
       " (16, 164),\n",
       " (17, 82),\n",
       " (2, 44),\n",
       " (18, 42),\n",
       " (19, 18),\n",
       " (20, 9),\n",
       " (21, 5),\n",
       " (1, 2),\n",
       " (23, 2),\n",
       " (22, 1)]"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "singletons = (w for w in COUNTS if COUNTS[w] == 1)\n",
    "lengths = list(map(len, singletons))\n",
    "Counter(lengths).most_common()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matplotlib.pyplot.hist(lengths, bins=len(set(lengths)));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Длины таких слов распределены похоже на нормальное распределение :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pdist_good_turing_hack(counter, onecounter, base=1/33., prior=1e-8):\n",
    "    \"\"\"Вероятность слова при условии данных из счетчика.\n",
    "    Для неизвестных слов, смотрим на слова, встретившиеся единожды из onecounter, \n",
    "    вероятность выбираем, основываясь на длине.\n",
    "    Воспользуемся идеей метода Гуда-Тьюринга(полностью мы его здесь не реализуем).\n",
    "    prior -добавочный фактор, который сделает неизвестные слова менее вероятными.\n",
    "    base -то, насколько мы уменьшаем вероятность за длину слова больше максимального.\"\"\"\n",
    "    N = sum(list(counter.values()))\n",
    "    N2 = sum(list(onecounter.values()))\n",
    "    lengths = list(map(len, [w for w in onecounter if onecounter[w] == 1]))\n",
    "    ones = Counter(lengths)\n",
    "    longest = max(ones)\n",
    "    return (lambda word: \n",
    "            counter[word] / N if (word in counter) \n",
    "            else prior * (ones[len(word)] / N2 or \n",
    "                          ones[longest] / N2 * base ** (len(word)-longest)))\n",
    "#Переопределим P1w\n",
    "P1w = pdist_good_turing_hack(COUNTS1, COUNTS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "segment.cache.clear()\n",
    "segment('какой-то сегмент')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Задача: Что если слово находится очень далеко по edit_distance, но звучит точно так же?***\n",
    "Часто можно встретить ошибки в текстах, вызванные неграмотным написанием слов. Особенно часто это происходит в случае иностранных фамилий или транслитерированной терминологии. Обычно в таких случаях в пример приводят написание фамилии"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для такого случая можно использовать следующую методологию. Давайте привлечем лингвистов и составим правила, которые одинаково звучащим словам будут ставить в соответствие один и тот же код. Допустим, с помощью лингвистов мы такой алгоритм придумали. Тогда дальнейшие наши действия таковы:\n",
    "\n",
    "1) Сделать словарь с вероятностями слов (как мы делали из мешка слов)\n",
    "\n",
    "2) Сделать словарь соответствий код слова -> слово (с помощью того самого алгоритма от лингвистов). \n",
    "    Если есть в списке есть слова с одинаковым кодом, выбирать будем наиболее частое слово.\n",
    "\n",
    "3) Сделаем аналогичный edit_distance алгоритм на множестве кодов слов\n",
    "\n",
    "4) Найдя соответствующую замену для слова в виде его кода, восстановим слово с помощью словаря из пункта 2\n",
    "\n",
    "Алгоритм, про который мы поговорим, называется Double Metaphone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import metaphone\n",
    "from metaphone import doublemetaphone"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Алгоритм возвращает кортеж из двух возможных фонетических кодов слова. Правило такое:\n",
    "\n",
    " (Primary Key = Primary Key) = Идеальное совпадение\n",
    " \n",
    " (Secondary Key = Primary Key) = Совпадение\n",
    " \n",
    " (Primary Key = Secondary Key) = Совпадение\n",
    " \n",
    " (Alternate Key = Alternate Key) = Совпадение +-\n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# THE END"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "У Норвига в статье код занимает 21 строчку: \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re, collections\n",
    "def words(text):\n",
    "    return re.findall('[а-ё]+', text.lower())\n",
    "def train(features):\n",
    "    model = collections.defaultdict(lambda: 1)\n",
    "    for f in features:\n",
    "        model[f] += 1\n",
    "    return model\n",
    "    \n",
    "with codecs.open('words.txt', 'r', encoding = 'windows 1251') as file:\n",
    "    TEXT = file.read().replace('\\n', ' ')\n",
    "\n",
    "NWORDS = train(words(TEXT))\n",
    "alphabet = 'абвгдеёжзийклмнопрстуфхцчшщъыьэюя'\n",
    "def edits1(word):\n",
    "    n = len(word)\n",
    "    return set( [word[0:i]+word[i+1:] for i in range(n)] +                      # deletion\n",
    "                [word[0:i]+word[i+1]+word[i]+word[i+2:] for i in range(n-1)] +   # transposition\n",
    "                [word[0:i]+c+word[i+1:] for i in range(n) for c in alphabet] +    # alteration\n",
    "                [word[0:i]+c+word[i:] for i in range(n+1) for c in alphabet])    # insertion\n",
    "def known_edits2(word):\n",
    "    return set(e2 for e1 in edits1(word) for e2 in edits1(e1) if e2 in NWORDS)\n",
    "def known(words): return set(w for w in words if w in NWORDS)\n",
    "def correct(word):\n",
    "    candidates = known([word]) or known(edits1(word)) or known_edits2(word) or [word]\n",
    "    return max(candidates, key=lambda w: NWORDS[w])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Использовать этот код нужно следующим образом –\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_correct = tokens('Саничка не хочит в шшклу')\n",
    "\n",
    "for word in to_correct:\n",
    "    print(correct(word), end = ' ') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "87852089012c4b81b9af9dc676d2b889d7d3b7bd761d748065844c07d5d6aa6d"
  },
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
